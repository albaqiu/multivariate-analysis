---
title: "Diamonds Data Frame Analysis"
author: "Laia Jané, Elisa Müller, Berta Torrents and Runxiao Qiu"
date: "December 2025"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  results = 'markup',
  message = FALSE,
  warning = FALSE,
  fig.width = 7,
  fig.height = 5
)
```

```{r}
set.seed(12345)

# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Libraries
library(dplyr)
library(FactoMineR)
library(MASS)
library(psych)
library(factoextra)
library(corrplot)
library(cluster)
library(ggplot2)
library(ggrepel)
library(klaR)
library(cluster)
library(kmed)
library(biotools)
```

## 1. Data preparation

The dataset has been imported using the \emph{read.csv()} function from the \emph{dplyr} library. The variable named X has been removed, as it corresponds to an index column rather than an explanatory variable. Additionally, the categorical variables cut, color, and clarity have been correctly converted into factors.

```{r}
diamonds <- read.csv('diamonds.csv')

diamonds <- diamonds[,-1]

diamonds$cut <- as.factor(diamonds$cut)
diamonds$color <- as.factor(diamonds$color)
diamonds$clarity <- as.factor(diamonds$clarity)

summary(diamonds)
```

Since the dataset contains a large number of observations $(53940)$, a random sample of $500$ observations has been selected to allow for a more manageable and focused analysis. It is worth noting that the means of the numerical variables and the proportions of observations in each category do not vary significantly.

```{r}
n <- nrow(diamonds)
indices <- sample(1:n, 500) # Take 500 random indices
diamonds <- diamonds[indices,]
rownames(diamonds) <- seq_len(nrow(diamonds))

summary(diamonds)
```

## Exploratory data analysis

For each variable, we have checked duplicates, existence of zeros, existence of outliers, existence of missing values and we have applied transformations if needed.

## Numerical variables

Helper function to evaluate missing values and zeros.

```{r}
check_na <- function(x) {
  list(na = sum(is.na(x)),
  zeros = sum(x == 0))
}
```

```{r}
results <- lapply(diamonds, check_na)
results
```

Observation: no numeric variable has NA or zeros.

### Carat variable

After analyzing the carat variable, no severe outliers are observed. The carat variable is right-skewed, with a long tail toward larger values, and therefore is not normally distributed. After applying the Box-Cox transformation, a logarithmic transformation has been identified as appropriate to improve symmetry and stabilize variance.

However, the log-transformed variable does not follow a normal distribution either. The histogram is not bell-shaped, and the Q-Q plot shows substantial deviations. As expected, both the Shapiro-Wilk test and the Kolmogorov-Smirnov test have rejected the hypothesis of normality.

```{r}
# Outliers
boxplot(diamonds$carat, horizontal = T)
varout <- summary(diamonds$carat)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$carat, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$carat >= usout) | (diamonds$carat <= lsout))
mild_out <- which((diamonds$carat >= umout) | (diamonds$carat <= lmout))
sev_out # 0
mild_out #22

# Distribution
hist(diamonds$carat,freq=F)
curve(dnorm(x),add=T, col="red") #It is obviously not normally distributed

# Transformation
b <- boxcox(lm(diamonds$carat~1)) # Box-Cox Transformation
# lambda <- b$x[which.max(b$y)]
# diamonds$new_carat <- (diamonds$carat^1-1)/lambda

diamonds$logCarat <- log(diamonds$carat) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logCarat, horizontal = T) # No outliers

# Is logCarat normally distributed?
hist(diamonds$logCarat, freq = F)
m = mean(diamonds$logCarat)
std = sd(diamonds$logCarat)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)

qqnorm(diamonds$logCarat, main = "logCarat Q-plot")
qqline(diamonds$logCarat)

shapiro.test(diamonds$logCarat)
ks.test(diamonds$logCarat, "pnorm", mean=mean(diamonds$logCarat), sd=sd(diamonds$logCarat))
```

### Depth variable

The depth variable shows two severe outliers, which have been removed to make the distribution closer to normality. The histogram indicates that the depth variable is approximately bell-shaped. In addition, the Q-Q plot is close to a straight line without major deviations, suggesting that this variable can be considered approximately normal.

Although the Shapiro-Wilk test and the Kolmogorov-Smirnov test yield low p-values, leading to the rejection of the normality hypothesis, the variable has been considered normally distributed based on the graphical assessment.

```{r}
# Outliers
boxplot(diamonds$depth, horizontal = T)
varout <- summary(diamonds$depth)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$depth, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$depth >= usout) | (diamonds$depth <= lsout))
mild_out <- which((diamonds$depth >= umout) | (diamonds$depth <= lmout))
sev_out #[1] 462 467
mild_out #21
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Is depth normally distributed?
hist(diamonds$depth,freq=F)
m = mean(diamonds$depth)
std = sd(diamonds$depth)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$depth, main = "depth Q-plot")
qqline(diamonds$depth)

shapiro.test(diamonds$depth)
ks.test(diamonds$depth, "pnorm", mean=mean(diamonds$depth), sd=sd(diamonds$depth))
```

### Table variable

The table variable contains one severe outlier, which has been removed. After its removal, the distribution becomes almost symmetric, with a mild right skewness. Since the shape is already close to normal and a transformation would not provide a meaningful improvement, no transformation is applied. The Q-Q plot shows points lying close to a straight line without major deviations, which further suggests approximate normality. Although the variable is discrete, its wide range of values and its bell-shaped histogram support the assumption that normality can reasonably be assumed.

Despite the low p-values obtained from the Shapiro-Wilk test and the Kolmogorov-Smirnov test, which lead to the rejection of the normality hypothesis, the variable has been considered normally distributed based on the graphical assessment.

```{r}
# Outliers
boxplot(diamonds$table, horizontal = TRUE)
varout <- summary(diamonds$table)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$table, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$table >= usout) | (diamonds$table <= lsout))
mild_out <- which((diamonds$table >= umout) | (diamonds$table <= lmout))
sev_out #[1] 206
mild_out #[1] 206 445
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Is table normally distributed?
hist(diamonds$table,freq=F)
m = mean(diamonds$table)
std = sd(diamonds$table)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$table, main = "table Q-plot")
qqline(diamonds$table)

shapiro.test(diamonds$table)
ks.test(diamonds$table, "pnorm", mean=mean(diamonds$table), sd=sd(diamonds$table))
```

### Price variable

The price variable does not show severe outliers, although a long right tail is observed in the boxplot, indicating the presence of potential mild outliers. The histogram shows a right-skewed distribution, which can be improved through a transformation to reduce skewness and stabilize variance. Based on the Box-Cox analysis, a logarithmic transformation has been identified as suitable. After applying the log transformation, the distribution becomes more symmetric and closer to a bell-shaped form, although it remains non-normally distributed. The Q-Q plot indicates that the log-transformed variable still deviates from normality, with several noticeable deviations from the straight line. In addition, both the Shapiro-Wilk test and the Kolmogorov-Smirnov test yield low p-values, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$price, horizontal = T)
varout <- summary(diamonds$price)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$price, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$price >= usout) | (diamonds$price <= lsout))
mild_out <- which((diamonds$price >= umout) | (diamonds$price <= lmout))
sev_out #0
mild_out #28

# Distribution
hist(diamonds$price, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$price)
std = sd(diamonds$price)
curve(dnorm(x,m,std),add=T, col="red")

b <- boxcox(lm(diamonds$price~1)) # Box-Cox Transformation
diamonds$logPrice <- log(diamonds$price) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logPrice, horizontal = T) # No outliers

# Is logPrice normally distributed?
hist(diamonds$logPrice, freq = F)
m = mean(diamonds$logPrice)
std = sd(diamonds$logPrice)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)

qqnorm(diamonds$logPrice, main = "logPrice Q-plot")
qqline(diamonds$logPrice)

shapiro.test(diamonds$logPrice)
ks.test(diamonds$logPrice, "pnorm", mean=mean(diamonds$logPrice), sd=sd(diamonds$logPrice))
```

### x variable

The x variable does not show mild or severe outliers. Nevertheless, the histogram indicates the presence of one observation with a very low x value compared with the rest of the data. An attempt has been made to remove this observation. However, even after applying a transformation, the variable has not been successfully normalized. For this reason, the variable has been analyzed in its original form, despite clearly not following a normal distribution.

```{r}
# Outliers
boxplot(diamonds$x, horizontal = T)
varout <- summary(diamonds$x)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$x, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$x >= usout) | (diamonds$x <= lsout))
mild_out <- which((diamonds$x >= umout) | (diamonds$x <= lmout))
sev_out #0
mild_out #0

# Distribution
hist(diamonds$x, freq = F)
m = mean(diamonds$x)
std = sd(diamonds$x)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$x, main = "x Q-plot")
qqline(diamonds$x)

shapiro.test(diamonds$x)
ks.test(diamonds$x, "pnorm", mean=mean(diamonds$x), sd=sd(diamonds$x))
```

### y variable

The y variable does not show mild or severe outliers. Nevertheless, the histogram indicates the presence of one observation with a very low y value compared with the rest of the data. An attempt has been made to remove this observation. However, even after applying a transformation, the variable has not been successfully normalized. For this reason, the variable has been analyzed in its original form, despite clearly not following a normal distribution.

```{r}
# Outliers
boxplot(diamonds$y, horizontal = T)
varout <- summary(diamonds$y)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$y, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$y >= usout) | (diamonds$y <= lsout))
mild_out <- which((diamonds$y >= umout) | (diamonds$y <= lmout))
sev_out #0
mild_out #0

# Distribution
hist(diamonds$y, freq = F)
m = mean(diamonds$y)
std = sd(diamonds$y)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$y, main = "y Q-plot")
qqline(diamonds$y)

shapiro.test(diamonds$y)
ks.test(diamonds$y, "pnorm", mean=mean(diamonds$y), sd=sd(diamonds$y))
```

### z variable

The z variable does not show mild or severe outliers. Nevertheless, the histogram indicates the presence of one observation with a very low z value compared with the rest of the data. An attempt has been made to remove this observation. However, even after applying a transformation, the variable has not been successfully normalized. For this reason, the variable has been analyzed in its original form, despite clearly not following a normal distribution.

```{r}
# Outliers
boxplot(diamonds$z, horizontal = T)
varout <- summary(diamonds$z)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$z, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$z >= usout) | (diamonds$z <= lsout))
mild_out <- which((diamonds$z >= umout) | (diamonds$z <= lmout))
sev_out #0
mild_out #0

# Distribution
hist(diamonds$z, freq = F)
m = mean(diamonds$z)
std = sd(diamonds$z)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$z, main = "z Q-plot")
qqline(diamonds$z)

shapiro.test(diamonds$z)
ks.test(diamonds$z, "pnorm", mean=mean(diamonds$z), sd=sd(diamonds$z))
```

## Categorical variables

### Cut variable

The cut variable shows no missing values. It is clearly unbalanced, with the Ideal level accounting for $37.4\%$ of the observations, while the Fair level represents only $1.8\%$ of the data. These sparse categories can be further grouped depending on the results of the multivariate analysis.

Since Fair and Good are consecutive levels on the standard cut-quality scale and represent similar performance in terms of light return and symmetry, they have been combined into a single category (Fair/Good). This grouping preserves the ordinal logic of the cut grading system while ensuring more balanced category frequencies, leading to more robust and interpretable results.

```{r}
# Distribution
summary(diamonds$cut)
barplot(summary(diamonds$cut), main="Diamond Cut Quality")

prop.table(table(diamonds$cut))

# We can group Fair and Good in one group because Fair has little observations and the variable is unbalanced

diamonds$cut <- ifelse(diamonds$cut %in% c("Fair", "Good"), "Fair/Good", as.character(diamonds$cut))

diamonds$cut <- factor(
 diamonds$cut,
 levels = c("Fair/Good", "Ideal", "Premium", "Very Good"))

barplot(table(diamonds$cut), main="Diamond Cut Quality")
```

### Color variable

The color variable shows no missing values and is relatively well balanced. Nevertheless, the most frequent grade is G, representing $12.6\%$ of the observations, while the least frequent grade is J, accounting for $3.4\%$, indicating that diamonds with a noticeable tint are less common in the dataset.

To avoid categories with very low frequencies and to improve stability in the multivariate analysis, the light-tinted grades I and J have been grouped together. These grades are consecutive on the GIA color scale and represent similar coloration.

```{r}
# Distribution
summary(diamonds$color)
barplot(summary(diamonds$color), main="Diamond Color")

prop.table(table(diamonds$color))

# We can group I and J in one group because J has little observations and the variable is unbalanced

diamonds$color <- ifelse(diamonds$color %in% c("I", "J"), "I/J", as.character(diamonds$color))
summary(diamonds$color)

diamonds$color <- factor(
  diamonds$color,
  levels = c("D", "E", "F", "G","H","I/J")
)

barplot(table(diamonds$color), main="Diamond Color")
```

### Clarity variable

The clarity variable shows no missing values and is clearly unbalanced. The distribution is dominated by the SI1 and VS2 grades, indicating that the sample is mainly composed of slightly to very slightly included diamonds.

Since some clarity levels, particularly I1 and IF, have very low frequencies, these sparse categories have been grouped to avoid levels with extremely few observations. The grouping has been performed by pairing each level with its closest neighboring grade on the GIA clarity scale: I1 has been grouped with SI2, and IF with VVS1. This transformation reduces sparsity, preserves the ordinal structure of the variable, and leads to more stable and interpretable results.

```{r}
# Distribution
summary(diamonds$clarity)
barplot(summary(diamonds$clarity), main="Diamond Clarity")

prop.table(table(diamonds$clarity))

# We can group I1 and IF in one group because I1 has little observations and the variable is unbalanced

diamonds$clarity <- ifelse(diamonds$clarity %in% c("IF", "VVS1"), "IF/VVS1", as.character(diamonds$clarity))
summary(diamonds$clarity)
diamonds$clarity <- ifelse(diamonds$clarity %in% c("I1", "SI2"), "I1/SI2", as.character(diamonds$clarity))
summary(diamonds$clarity)


diamonds$clarity <- factor(
  diamonds$clarity,
  levels = c("I1/SI2", "SI1", "VS1", "VS2", "VVS2", "IF/VVS1")
)


barplot(table(diamonds$clarity), main="Diamond Clarity")
```

In conclusion, three observations have been removed, and the carat and price variables have been transformed using a logarithmic transformation to improve interpretability. In addition, categorical variables have been grouped to reduce sparsity and ensure more balanced category frequencies while preserving their ordinal structure. Hence, the dataframe used for subsequent analyses is:

```{r}
diamondsOg <- diamonds[,c(1:10)]
diamonds <- diamonds[,c(2:6,8:12)]
```

## Correlations between numeric variables

The numeric variables are generally highly correlated. The x, y, z, logPrice, and logCarat variables exhibit nearly perfect positive correlations, and their scatter plots form almost straight lines. In fact, the logarithmic transformations of price and carat further increase these correlations.

In contrast, the depth and table variables show weak correlations with the remaining variables, with depth being the only variable negatively correlated with all others. Their scatter plots display diffuse clouds of points without a clear pattern, although table and depth appear to be slightly negatively correlated.

As expected, the Bartlett test has returned a very low p-value, leading to the rejection of the null hypothesis that the correlation matrix is equal to the identity matrix.

It is also noted that the correlation structures of the full population and the randomly generated sample are very similar. Therefore, no misleading conclusions are introduced by using the sample instead of the complete dataset.

```{r}
numeric_vars <- diamonds[ , sapply(diamonds, is.numeric)]

cor_matrix <- cor(numeric_vars)

corrplot(cor_matrix, method = "color", addCoef.col = "black", title = "Correlation Plot of Numeric Variables")

pairs(numeric_vars, main="Scatterplot Matrix of Numeric Variables")

cortest.bartlett(cor_matrix, nrow(diamonds))
```

Considering the KMO index, since it is higher than 0.5, the data are considered factorable. In other words, the variables are sufficiently correlated with each other to allow the extraction of meaningful factors.

```{r}
# (function written by Prof. Shigenobu Aok.)

kmo <- function(x)
{
  x <- subset(x, complete.cases(x))       # Omit missing values
  r <- cor(x)                             # Correlation matrix
  r2 <- r^2                               # Squared correlation coefficients
  i <- solve(r)                           # Inverse matrix of correlation matrix
  d <- diag(i)                            # Diagonal elements of inverse matrix
  p2 <- (-i/sqrt(outer(d, d)))^2          # Squared partial correlation coefficients
  diag(r2) <- diag(p2) <- 0               # Delete diagonal elements
  KMO <- sum(r2)/(sum(r2)+sum(p2))
  MSA <- colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

# KMO index
kmo(numeric_vars)
```

# Part 2: Multivariant analysis

## 2.1. Principal components analysis

The PCA analysis has been applied using the numeric variables of the diamonds dataset through the \emph{PCA()} function from the \emph{FactoMineR} library.

Given the strong correlations among the numerical variables, PCA has been considered an appropriate method to construct an index summarizing the numerical characteristics of the diamonds (logCarat, x, y, z, depth and table), with the aim of reducing the dimensionality of the dataset and generating an indicator that can be related to the diamond price. For this reason, the logPrice variable has been treated as a supplementary variable. Another objective has been to facilitate the interpretation of the MDS results. In addition, due to the high correlation among the variables, performing clustering on the principal components rather than on the original variables has been expected to produce more interpretable and robust clusters, by preventing distance-based clustering methods from being influenced by redundant information.

By treating price as a supplementary variable, it has been excluded from the computation of the principal components and only projected onto the factorial space afterward. This approach makes it possible to analyze how price aligns with the main axes without affecting their structure.

To determine the number of components to retain, and taking into account that the data are standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, larger eigenvalues indicate a greater proportion of explained variability. In this case, the first two components, with eigenvalues $4.06$ and $1.26$, satisfy the Kaiser criterion. Moreover, the cumulative percentage of explained variance shows that these two components account for $88.65%$ of the total variance, indicating that they capture most of the information in the dataset. Additionally, the scree plot exhibits a clear elbow after the first component, suggesting that retaining two components provides a good balance between variance explained and interpretability.

```{r}
diamonds.pca.sup <- PCA(numeric_vars, quanti.sup = 7, graph = T)

barplot(diamonds.pca.sup$eig[,1], names.arg = 1:6, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca.sup$eig[,1], type = "o", col = "red", pch = 19, add=T)

diamonds.pca.sup$eig
```

It is important to identify which variables contribute most to each relevant component in order to reduce the number of variables. By considering the coordinates of the variables, it is possible to determine which variables are associated with each component. In addition, examining the $cos^2$ values indicates which variables are more strongly related to a given component.

- \textbf{Dimension $1$: Overall physical magnitude and price association}

Dimension $1$ shows strong positive loadings for variables related to the physical magnitude of the diamond, such as x, y, z and logCarat. As shown in the figure, the supplementary variable logPrice is highly correlated with Dimension $1$. This indicates a clear relationship between the overall physical magnitude of a diamond and its price. As expected, larger and heavier diamonds tend to be more expensive. Therefore, this axis can be interpreted as an overall physical magnitude-price dimension. Diamonds with high coordinates on Dimension $1$ are generally larger, heavier, and more valuable, whereas diamonds with low coordinates tend to be smaller and less expensive.


- \textbf{Dimension $2$: Geometric balance}

Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Since depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. Specifically, Dimension $2$ is positively associated with taller, more pointed diamonds with a relatively narrower top surface, and negatively associated with shallower or more flattened diamonds.

It is important to note that these results are consistent with the correlation analysis. The physical dimensions and price variables are highly and positively correlated, whereas table and depth show a mild negative correlation.

```{r}
corrplot(diamonds.pca.sup$var$coord)
diamonds.pca.sup$var$cos2
diamonds.pca.sup$var$contrib

diamonds.pca.sup$var$coord

plot.PCA(diamonds.pca.sup, choix = "var", axes = c(1, 2))
```

Although the individual assessment of the dataset has not provided substantial additional information, it is important to note that some observations lie far from the main cloud of points. For instance, the diamond labeled $270$ is a clear outlier along dimension $2$, indicating that it corresponds to a shallow or flattened diamond, whereas the diamonds labeled $308$ and $358$ are clearly more pointed. Conversely, the diamond labeled $365$ is a clear outlier along dimension $1$, indicating that it is larger, heavier, and more valuable.

```{r}
plot.PCA(diamonds.pca.sup, choix = "ind", axes = c(1, 2))
```

In conclusion, PCA has effectively summarized the numeric variables of the diamonds dataset into a small number of interpretable and uncorrelated components. The first two dimensions capture most of the total variability and reflect meaningful characteristics of the diamonds. Dimension $1$ represents overall physical magnitude and is strongly associated with price, while Dimension $2$ captures geometric differences related to depth and table. This reduced representation retains most of the information in the data and provides a solid basis for further analyses.

## 2.2. Multidimensional scaling

Multidimensional scaling has been applied to provide an alternative representation of the diamonds dataset based directly on distances between observations, with the aim of assessing whether the patterns and structures identified through PCA are preserved. In addition, different distance measures are considered in order to evaluate the effect of including categorical variables on the resulting similarity structure.

The Euclidean distance matrix, based on the scaled numerical variables, has been computed using the \emph{dist()} function. Classical multidimensional scaling has then been applied through the \emph{cmdscale()} function.

The number of dimensions retained is $k=2$. According to the scree plot, a noticeable elbow appears after the first dimension, indicating that retaining two dimensions captures most of the information while keeping the solution interpretable. Moreover, the GOF obtained for the two-dimensional solution is $0.89$, indicating that the map preserves a substantial proportion of the original distance information and provides a reasonably good representation of the relationships among observations.

The MDS individual plot shows patterns similar to those observed in the PCA results. The diamonds labeled $270$, $308$, $358$ and $365$ stand clearly apart from the main cloud of points. Since these observations occupy positions comparable to those observed in the PCA representation, it can be concluded that the first and second MDS dimensions closely correspond to the first and second PCA components. One notable difference between the two methods is that MDS distributes the observations more evenly across the space, rather than concentrating them as in the PCA plot.

```{r}
ds_eu <- dist(scale(numeric_vars), method = "euclidean")

dimaonds.mds.eu <- cmdscale(ds_eu, eig = TRUE, k = 2)

dimaonds.mds.eu$GOF

plot(1:length(dimaonds.mds.eu$eig), dimaonds.mds.eu$eig, 
     type = "b",
     xlab = "Dimension", 
     ylab = "Eigenvalue",
     main = "Scree Plot")

x <- dimaonds.mds.eu$points[,1]
y <- dimaonds.mds.eu$points[,2]

plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
     main="Individual plot using the first two coordinates with Euclidian distance", type="n")

text(x, y, labels = row.names(diamonds), cex=.7)
```

On the other hand, MDS has also been performed using the Gower distance, computed with the \emph{daisy()} function from the \emph{cluster} library, which allows the inclusion of both numerical and categorical variables. This approach makes it possible to assess whether categorical information contributes additional structure beyond that captured by the numerical variables alone.

The scree plot again suggests that retaining two dimensions captures most of the information while keeping the solution interpretable. In the case of the Euclidean distance, both GOF values are identical. However, when using the Gower distance, the two GOF measures differ slightly. The first value $(0.26)$ represents the proportion of total variance explained by the two dimensions when both positive and negative eigenvalues are considered, whereas the second value $(0.36)$ accounts only for the positive eigenvalues. This difference arises because the Gower distance combines variables of mixed types, which affects the eigenvalue structure of the distance matrix.

Although the GOF values are lower when categorical variables are included, this result is expected, as incorporating mixed data types introduces additional heterogeneity into the distance computation. Nevertheless, the individual plot shows a clear separation of the diamonds into four distinct groups, suggesting that categorical variables contribute meaningful information for distinguishing similarity patterns among observations. Observation $365$ remains a clearly outstanding case in the MDS representation.

```{r}
ds_gw <- daisy(diamonds, metric = "gower")

diamonds.mds.gw <- cmdscale(ds_gw, eig = TRUE, k = 2)

diamonds.mds.gw$GOF

plot(1:length(diamonds.mds.gw$eig), diamonds.mds.gw$eig, 
     type = "b",
     xlab = "Dimension", 
     ylab = "Eigenvalue",
     main = "Scree Plot")

x <- diamonds.mds.gw$points[,1]
y <- diamonds.mds.gw$points[,2]


plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
     main="Individual plot using the first two coordinates with Gower distance", 
     type="n")

draw.ellipse <- function(x0, y0, a, b, col, lwd=2, alpha=0.3, angle=0) {
  theta <- seq(0, 2*pi, length.out=200)
  x_ellipse <- a*cos(theta)
  y_ellipse <- b*sin(theta)
  
  angle_rad <- angle * pi/180
  x_rot <- x_ellipse * cos(angle_rad) - y_ellipse * sin(angle_rad)
  y_rot <- x_ellipse * sin(angle_rad) + y_ellipse * cos(angle_rad)
  
  x_ellipse <- x0 + x_rot
  y_ellipse <- y0 + y_rot
  
  polygon(x_ellipse, y_ellipse, 
          col=adjustcolor(col, alpha.f=alpha), 
          border=NA)
  lines(x_ellipse, y_ellipse, col=col, lwd=lwd)
}

draw.ellipse(x0=-0.15, y0=0.18, a=0.2, b=0.05, angle=-20, col="red", lwd=2.5)
draw.ellipse(x0=-0.2, y0=-0.05, a=0.23, b=0.15, col="blue", lwd=2.5)
draw.ellipse(x0=0.2, y0=0.12, a=0.12, b=0.08, col="green", lwd=2.5)
draw.ellipse(x0=0.20, y0=-0.12, a=0.18, b=0.12, col="orange", lwd=2.5)

text(x, y, labels = row.names(diamonds), cex=.7)
```

In this context, MDS has proven to be a valuable complementary method to PCA, providing an alternative, distance-based perspective on the structure of the diamonds dataset. The strong consistency between the Euclidean MDS and PCA representations reinforces the robustness of the main dimensions identified using numerical variables. In contrast, the Gower-based MDS reveals additional structure driven by the inclusion of categorical variables, leading to a clearer separation of observations and highlighting grouping patterns that are not captured by numerical information alone. Overall, MDS enhances the interpretability of the data and supports the understanding and validation of subsequent clustering results when dealing with correlated variables and mixed data types.

## 2.3. Correspondence analysis

The CA analysis has been applied using the the original categorical variables (cut, color, clarity) by analyzing them pairwisely. We have applied the \emph{CA()} function from the \emph{FactoMineR} library.

The objective of the correspondence analysis is to explore pairwise associations between the categorical variables cut, color, and clarity in the diamonds dataset. These variables present a relatively large number of levels, for which CA is particularly useful as an exploratory technique. By analyzing contingency tables, this method allows the identification of category levels that tend to occur together and the visualization of their relationships in a reduced-dimensional space. This preliminary pairwise analysis facilitates the interpretation of the subsequent multiple correspondence analysis (MCA) by clarifying the main associations between variables. This analysis is therefore used as a supplementary exploratory step, as the following section incorporates both the categorical variables and the categorized numerical variables.

### Cut x color

There's 3 dimensions because dim = min(4−1, 6−1).

The first two dimensions explain 58.70% + 38.02% = 96.72% of the association between cut and color. This means that the two-dimensional map provides a really good summary of the relationship between the two variables.

The correspondence analysis shows distinct association patterns between cut and color when considering the representation on the first two dimensions, which together explain a substantial proportion of the total inertia.

The Premium cut is located in the left-hand side of the map and is positioned close to the color G, indicating a clear association between these categories. This relationship is mainly structured along Dimension 1, where both categories are well represented and contribute strongly to the inertia. The Fair/Good cut appears on the right-hand side of the map. However, it is not located in close proximity to any specific color category, suggesting that its distinct position reflects an overall difference in profile rather than a strong association with a single color level. The Ideal cut is located in the upper-right quadrant, relatively close to D and F, indicating a moderate association with these color levels. This position is primarily influenced by Dimension 2, on which Ideal is well represented and contributes substantially. Finally, the Very Good cut lies in the lower part of the map and is closest to H and E, suggesting an association with these color categories. This relationship is also mainly captured by Dimension 2.

Overall, all categories are located far from the origin, suggesting a strong deviations between the cut and color combinations.

```{r}
tab_cut_color <- table(diamonds$cut, diamonds$color)
tab_cut_color
cut_color <- CA(tab_cut_color)

# Eigenvalues
cut_color$eig
barplot(cut_color$eig[,2], main="Eigenvalues")

# Row profiles
cut_color$row$coord
cut_color$row$contrib
cut_color$row$cos2

# Column profiles
cut_color$col$coord
cut_color$col$contrib
cut_color$col$cos2

plot.CA(cut_color,axes = c(1,2))
plot.CA(cut_color,axes = c(1,3))
plot.CA(cut_color,axes = c(2,3))
```

### Cut x clarity
There's 3 dimensions because dim = min(4−1, 6−1).

With 2 dimensions we obtain 58.09% + 25.95% = 84.04%, meaning that the 2-dimensional CA map captures most of the association between cut and clarity.

The correspondence analysis reveals clear relationship patterns between clarity grade and cut quality as captured by the first two dimensions, which together account for the majority of the total inertia.

VVS2 is positioned in the upper-right quadrant of the map, relatively isolated from the cut quality categories. Its elevated position along Dimension 2 suggests it has a distinct profile, though it shows some proximity to Very Good cut. On the other hand, I1/SI2 is positioned on the left side of the map, showing clear proximity to both Premium and Fair/Good cuts. This location along Dimension 1 suggests that lower clarity grades are distinctly associated with these cut qualities, representing a profile separate from higher clarity grades. Finally, IF/VVS1 occupies the far-right position on Dimension 1, positioned closest to Ideal cut.

It is worth noting that SI1, VS1, and VS2 are clustered very close to the origin of the map. This central positioning indicates that these mid-range clarity grades do not show strong associations with any particular cut quality category. Their location near the origin suggests they have relatively uniform distributions across cut qualities and contribute less to explaining the main patterns of association in the data.

Overall, the map reveals a gradient where the highest clarity grade (IF/VVS1) associates relatively well with Ideal cuts, mid-range clarity grades with Very Good cuts, and lower clarity grades (I1/SI2) with Fair/Good and Premium cuts.

```{r}
tab_cut_clarity <- table(diamonds$cut, diamonds$clarity)
tab_cut_clarity
cut_clarity <- CA(tab_cut_clarity)

# Eigenvalues
cut_clarity$eig
barplot(cut_clarity$eig[,2], main="Eigenvalues")

# Row profiles
cut_clarity$row$coord
cut_clarity$row$contrib
cut_clarity$row$cos2

# Column profiles
cut_clarity$col$coord
cut_clarity$col$contrib
cut_clarity$col$cos2

plot.CA(cut_clarity,axes = c(1,2))
plot.CA(cut_clarity,axes = c(1,3))
plot.CA(cut_clarity,axes = c(2,3))
```

### Color x clarity
There's 5 dimensions because dim = min(6−1, 6−1).

With 2 dimensions we explain 52.15% + 28.18% = 80.33% of the association between color and clarity, meaning that the 2-dimensional map captures most of the relationship between the two variables.

The correspondence analysis shows the spatial distribution of clarity grades and color categories when considering the representation on the first two dimensions, which together account for the majority of the total inertia.

The map does not reveal clear association patterns between clarity and color. Most clarity grades (VS2, SI1, I1/SI2) are clustered near the origin, and similarly, most color categories (D, I/J, F, H) are grouped in the central area of the map. This positioning indicates that these variables do not show strong associations with each other. Additionally, the categories that are positioned away from the origin (VVS2, VS1, IF/VVS1 for clarity; E, G for color) remain relatively isolated and do not appear in close proximity to categories from the other variable.

Furthermore, examination of alternative dimensional combinations did not reveal any clearer association patterns between these two variables. The consistent lack of clear co-location between clarity grades and color categories across multiple dimensional representations, strongly suggests that these two characteristics are largely independent in this dataset, with minimal systematic association between them.

```{r}
tab_color_clarity <- table(diamonds$color, diamonds$clarity)
color_clarity <- CA(tab_color_clarity)
tab_color_clarity
# Eigenvalues
color_clarity$eig
barplot(color_clarity$eig[,2], main="Eigenvalues")

# Row profiles
color_clarity$row$coord
color_clarity$row$contrib
color_clarity$row$cos2

# Column profiles
color_clarity$col$coord
color_clarity$col$contrib
color_clarity$col$cos2

plot.CA(color_clarity,axes = c(1,2))
plot.CA(color_clarity,axes = c(2,3))
plot.CA(color_clarity,axes = c(2,4))
plot.CA(color_clarity,axes = c(1,3))
plot.CA(color_clarity,axes = c(1,4))
plot.CA(color_clarity,axes = c(3,4))
```
## 2.4. Multiple correspondence analysis

POSAR BIBLIOGRAFIA

The MCA analysis has been applied using the the original categorical variables and the categorized numerical variables. We have applied the \emph{MCA()} function from the \emph{FactoMineR} library.

The objective of this method is to identify global patterns, associations, and similarities among the categories of different variables. Using this approach, we perform a more comprehensive analysis of the results obtained from CA. In addition, the numeric variables have been prepared to be included in the method in a categorized form, allowing us to observe patterns between both numeric and categorical variables. It is important to note that several strategies were tested to categorize the numeric variables, and the best results are those presented in this report. Moreover, the nature of each variable has been taken into account when defining the different categories.

Most of the numerical variables have been transformed into binary factors ("above average" / "below average") to reduce dimensionality and enhance interpretability. However, the table and depth variables have not been dichotomized using the median. Instead, both variables have been categorized based on gemological criteria, as their interpretation depends on proportion ranges that determine whether a diamond is well-cut.

The table variable is defined as the width of the top surface expressed as a percentage of the diamond’s diameter. In gemology, there is a clearly defined range considered ideal, typically around $53-58\%$, which produces better light performance:
    - Values within $53-58\%$: Ideal
    - Values outside this interval: NotIdeal
Thus, a two-level factor ("Ideal", "NotIdeal") is more coherent and informative.


The depth variable measures the total depth of the diamond relative to its diameter.
Unlike table, depth has two different ways of being non-ideal:
    - Low depth: diamond too shallow ("flat"), light leaks through the pavilion
    - High depth: diamond too deep, appears small and loses brilliance

Ideal depth lies approximately between 59% and 62.5%

Because both extremes (low and high) represent different types of proportion defects, collapsing the variable into only two categories ("ideal" / "non-ideal") would lose important information. Therefore, depth is categorized into three levels:
    - Low (< 59%)
    - Ideal (59-62.5%)
    - High (> 62.5%)

This three-group division preserves the geometric meaning of the variable and leads to more interpretable patterns in the MCA.

```{r}
dfmca <- diamonds
nums = names(numeric_vars)
vars_to_split <- setdiff(nums, c("table", "depth"))

for (var in vars_to_split){
  medianvar = median(dfmca[[var]])
  dfmca[[var]] <- ifelse(dfmca[[var]] >= medianvar, "o", "b") 
  dfmca[[var]] <- as.factor(dfmca[[var]])
}

# Table
dfmca$table <- factor(
  ifelse(dfmca$table >= 53 & dfmca$table <= 58, "Ideal", "NotIdeal")
)

# Depth
dfmca$depth <- factor(
  cut(dfmca$depth,
      breaks = c(-Inf, 59, 62.5, Inf),
      labels = c("Low", "Ideal", "High"),
      right = TRUE)
)

summary(dfmca)
```

The analysis will be done using Indicator Matrix. That way we will be able to study both the categories of the variables and the individuals.

Regarding the logPrice variable, the approach followed will be the same as in PCA: it will be included as a supplementary variable, to help further the interpretation without being included in the calculations.

```{r}
## Indicator Matrix
library(FactoMineR)
tab.disjonctif(dfmca)

## MCA Application on Data using Indicator Matrix
# logPrice used as a supplementary variable
res.mca <- MCA(dfmca,quali.sup=10)
summary(res.mca)
```

In order to choose which dimensions we will work with, one option is to apply the rule of choosing the dimensions with eigenvalues greater than the average inertia of the indicator matrix per dimension.

In this case, as there are 9 variables, J = 9, and the total number of categories is K=4+6+7+3+(5x2)=30, number of dimensions is K-J=21. Therefore, the average inertia is 1/21 (0.04761905), and the dimensions selected would theoretically be until Dimension 17.

Howerver, using the scree plot, we observe a clear elbow around Dimension 2.

After the second dimension, the eigenvalues decrease smoothly and almost linearly, with no additional major drops. This indicates that most of the structure is captured by the first few dimensions, while the remaining ones mainly represent smaller, less interpretable variations.

Therefore, although the theoretical criterion based on average inertia would suggest retaining many more dimensions, the scree plot indicates that the first 2 dimensions concentrate the meaningful information, and the interpretation will be limited to these.

```{r}
# K=4+6+7+3+(5x2)=30
# no of dimension: K-J=21
# average inertia per dimension: 1/(K-J)=1/21
summary(res.mca, ncp=4)
# Choose the dimensions with eigenvalues > 1/21 --> Until Dim.17

### Eigenvalues (scree plot)
res.mca$eig
barplot(res.mca$eig[,1],main="Eigenvalues", names.arg=1:nrow(res.mca$eig))
lines(res.mca$eig[,1], type = "o", col = "red", pch = 19, add=T)

# Choose the dimensions that explain the most variability --> Until Dim.2
```

As can be seen in the description of Dimensions, the \textbf{most significant variables in each dimension}, which are those represented with the highest value of $R^2$, are the following: 

- For Dimension 1, the variables are `x`, `y`, `z` and `logCarat` all with $R^2$ values above 0.90.
  It is also relevant to note that the supplementary variable `logPrice` is also very significant in this dimension, with a value of $R^2$ of 0.79.
- For Dimension 2, the variables that contribute the most are `cut`, `table`, and `depth`, with $R^2$ values of approximately 0.69, 0.42, and 0.38 respectively. 

Also, it is relevant to observe the link between variable and categories to see which are the *most significant categories in each dimension*, which are those represented with the highest absolute value of Estimate.

- Dimension 1: The categories most strongly associated with this axis are the "below-average" modalities of the most significant variables (`x`, `y`, `z` and `logCarat`) on the positive side, and their corresponding "over-average" modalities on the negative side. This symmetric pattern indicates that Dimension 1 primarily captures a size gradient, separating smaller stones from larger ones.

- Dimension 2: The modalities with the strongest positive association are table_NotIdeal, depth_Low, and cut_Fair/Good.
On the negative side, the most strongly associated categories are table_Ideal, depth_Ideal, and cut_Ideal.
This shows that Dimension 2 reflects differences in geometric proportions, contrasting diamonds with non-ideal table and depth values against those with ideal proportions.


```{r}
## Coordinates of categories
res.mca$var$coord

## Description of Dimensions
dimdesc(res.mca)
```

For the plots, we will interpret them in the 1st and 2nd dimension, since they are the most significative.

Firstly, we will interpret the *cloud of individuals*. The plot shows both the individuals and the categories represented by the 1st and 2nd dimension. In the plot, the following patterns can be observed:

The individuals form two clearly separated groups horizontally. This mainly happens because the variables that dominate Dimension 1 were converted into "over" and "below" categories. Since these binary modalities are positioned on opposite sides of the map, the individuals are naturally pulled toward one side or the other depending on whether they fall above or below the median. As a result, almost no points appear near the center, and the two clouds become very visible.

Vertically, however, the structure is much softer. The variables contributing to Dimension 2 (cut, table, depth, and partly clarity and color) are not binary, so they do not create two opposing groups. Instead, they spread individuals more gradually, generating a continuous cloud rather than distinct clusters along this axis.

As a whole, the plot shows a strong separation driven by the over/under transformations of the size-related variables, while the multi-level categorical variables introduce smoother variation without forming sharply defined groups.

```{r}
### Clouds ###
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none")
```

We now interpret the *cloud of individuals using the most significant categories for each dimension*, by selecting the corresponding variable in the habillage parameter. This allows us to visualise how the individuals are distributed across the map according to the categories that contribute the most to each axis.

-   For *dimension 1*, colouring individuals by `logCarat`. The map clearly shows this distinction: in the negative side of the Dim 1 axis, we can see the individuals with a value of logCarat over average, and in the positive side, those below average.

-   For *dimension 2*, grouping individuals by `cut`. The separation is less sharp than for logCarat but some general tendencies can still be observed. Fair/Good and Very Good appear more often in the upper part of the map, while Ideal is mostly concentrated in the lower area.

However, the distribution does not fully follow the expected pattern: Premium and Very Good are quite mixed instead of forming clearly separated groups, unlike Ideal, which shows a much more consistent positioning. This occurs because cut has several levels and interacts with other variables (such as table and depth), which softens the separation along Dimension 2 and produces a more gradual spread of individuals.

Additional view: table (Ideal vs Not Ideal)

Finally, colouring individuals by the table classification shows another soft structure.
Although the categories are not as strongly separated as logCarat, we can observe that table_NotIdeal tends to appear more in the upper area of the plot, while table_Ideal is more frequent in the lower half.
Again, this gradient contributes to the vertical spread of individuals but does not create clear clusters.

```{r}
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "logCarat")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "cut")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "table")
```

The graph of the active categories is very relevant: the categories of values 'over average' are represented on the positive side of Dim 1, and the ones 'below average' are on the negative side. And dimension 2 shows 'ideal' values on the negative side, and non-ideal ones on the positive.

Considering the total information obtained from the cloud of categories, and also the cloud of variables, the conclusions we draw of the *information obtained by the dimensions* is the following:

-   Dimension $1$: Physical Magnitude (and Price Component) Dimension $1$ separates diamonds primarily according to their physical scale. Variables such as x, y, z, and logCarat contribute strongly and in the same direction, and logPrice aligns closely as a supplementary variable. This indicates that this axis summarises a general size-value gradient: diamonds with higher coordinates are physically larger and tend to be more expensive, while those on the negative side are smaller, lighter, and typically less valuable

-   Dimension $2$: Geometric Balance Dimension $2$ is mainly shaped by the categories of table and cut, and depth is also significant. The plot shows a consistent vertical contrast across these three characteristics: the “Ideal” categories appear on the lower side of the axis, while the non-ideal or extreme categories tend to cluster on the upper side.

For table, the “Ideal” modality is located at the bottom, whereas “Not Ideal” appears clearly higher. For cut, the Ideal cut is also positioned in the lower region, while the remaining levels (Fair/Good, Premium, Very Good) lie in the upper half. A similar pattern occurs with depth: both Low and High depth categories are placed on the positive side of Dimension 2, while Ideal depth appears on the negative side.

Taken together, these patterns show that Dimension 2 separates diamonds with ideal geometric proportions (Ideal table + Ideal cut + Ideal depth) from those with non-ideal or unbalanced shapes, which tend to be wider, flatter, or too tall relative to the ideal standard.


```{r}
## Cloud of Categories
plot(res.mca,invisible=c("ind"),title="Graph of the active categories")

## Cloud of variables
plot(res.mca,choix="var",title="Cloud of variables")
```

Overall, the MCA highlights which characteristics matter most for differentiating diamonds. Size clearly drives the main separation and is closely aligned with price, whereas geometric balance (Ideal vs. Not Ideal proportions) introduces additional distinctions. In other words, a high-quality cut can enhance a diamond’s value, but it does not compensate for being smaller, and well-balanced proportions tend to strengthen the value of diamonds that are already physically larger.

## 2.5. Clustering and profiling

In this section, a clustering analysis is performed with the aim of identifying homogeneous groups of diamonds that share similar characteristics. The objective is to detect natural group structures in the data and to compare the clustering results obtained under different modeling settings.

The analysis is carried out under two complementary settings. First, clustering is performed using only the numerical variables, combining hierarchical and non-hierarchical methods, as well as clustering based on the principal component scores, in order to take advantage of the reduced and uncorrelated representation provided by PCA. Second, clustering is extended to include both numerical and categorical variables, with the aim of obtaining a more comprehensive and informative segmentation of the diamonds dataset.

### 2.5.1. Hierarchical clustering

The hierarchical clustering analysis is performed on the numerical variables after standardisation using the \emph{scale()} function. A Euclidean distance matrix is then computed by applying the \emph{dist()} function to the scaled dataset.

The clustering structure is obtained using the \emph{hclust()} function, and two linkage criteria are considered in order to compare their performance. First, the \textbf{complete linkage method} is applied, which defines the distance between clusters based on the maximum distance between their observations. Second, \textbf{Ward’s method} is used, which is based on minimising the within-cluster sum of squared errors (SSE). Other linkage methods have also been applied, but they have been discarded as they result in highly unbalanced and poorly interpretable cluster solutions.

The resulting dendrograms allow an initial inspection of the potential number of clusters. Using the \emph{rect.hclust()} function, possible partitions into 4, 5, and 6 clusters are highlighted. As observed in the figures, Ward’s method provides a clearer and more compact separation between groups than the complete linkage method, suggesting a more interpretable clustering solution. Based on the Ward dendrogram, a solution with 4 clusters appears to be the most appropriate, as it captures the main structure of the data before a major increase in fusion height.

```{r}
## Scale numerical data
dfscaled <- scale(numeric_vars)
dfscaled <- as.data.frame(dfscaled) # convert the scaled data to a dataframe
d <- dist(dfscaled, method = "euclidean") # apply distance matrix on the scaled dataframe

## Set margins 
par(xpd = TRUE, mar = c(5, 4, 4, 8))

## 1. Complete linkage method
fit1 <- hclust(d, method="complete") 
plot(fit1, main="Dendrogram of complete Linkage") 
# 4 clusters/6 clusters

rect.hclust(fit1, k=3, border="gold") # 3 clusters
rect.hclust(fit1, k=4, border="blue")  # 4 clusters
rect.hclust(fit1, k=5, border="cyan")  # 5 clusters
rect.hclust(fit1, k=6, border="red")   # 6 clusters

legend("topright",
       legend = c("3 clusters", "4 clusters", "5 clusters", "6 clusters"),
       col = c("gold", "blue", "cyan", "red"),
       lwd = 2,
       cex = 0.8,
       bg = "white")

## 2. Ward method
fit2 <- hclust(d, method="ward.D2") 
plot(fit2, main="Dendrogram of Ward Method") 
# 4 clusters/5 clusters

rect.hclust(fit2, k=3, border="brown")  # 3 clusters
rect.hclust(fit2, k=4, border="green")  # 4 clusters
rect.hclust(fit2, k=5, border="purple") # 5 clusters
rect.hclust(fit2, k=6, border="orange") # 6 clusters

legend("topright", 
       legend = c("3 clusters", "4 clusters", "5 clusters", "6 clusters"),
       col = c("brown", "green", "purple", "orange"),
       lwd = 2,
       cex = 0.8,
       bg = "white")

## 3. Single linkage method
fit3 <- hclust(d, method = "single")
plot(fit3, main = "Dendrogram of Single Linkage")

rect.hclust(fit3, k = 3, border = "pink") # 3 clusters
rect.hclust(fit3, k = 4, border = "blue") # 4 clusters
rect.hclust(fit3, k = 5, border = "yellow") # 5 clusters
rect.hclust(fit3, k = 6, border = "purple") # 6 clusters

legend("topright", inset = c(-0.25, 0),
       legend = c("3 clusters", "4 clusters", "5 clusters", "6 clusters"),
       col = c("pink", "blue", "yellow", "purple"),
       lwd = 2, cex = 0.8, bg = "white")

## 4. Average linkage method
fit4 <- hclust(d, method = "average")
plot(fit4, main = "Dendrogram of Average Linkage")

rect.hclust(fit4, k = 3, border = "cyan") # 3 clusters
rect.hclust(fit4, k = 4, border = "red") # 4 clusters
rect.hclust(fit4, k = 5, border = "green") # 5 clusters
rect.hclust(fit4, k = 6, border = "orange") # 6 clusters

legend("topright", inset = c(-0.25, 0),
       legend = c("3 clusters", "4 clusters", "5 clusters", "6 clusters"),
       col = c("cyan", "red", "green", "orange"),
       lwd = 2, cex = 0.8, bg = "white")

## 5. Centroid linkage method
fit5 <- hclust(d, method = "centroid")
plot(fit5, main = "Dendrogram of Centroid Linkage")

rect.hclust(fit5, k = 3, border = "pink") # 3 clusters
rect.hclust(fit5, k = 4, border = "purple") # 4 clusters
rect.hclust(fit5, k = 5, border = "blue") # 5 clusters
rect.hclust(fit5, k = 6, border = "brown") # 6 clusters

legend("topright", inset = c(-0.25, 0),
       legend = c("3 clusters", "4 clusters", "5 clusters", "6 clusters"),
       col = c("pink", "purple", "blue", "brown"),
       lwd = 2, cex = 0.8, bg = "white")

par(xpd = FALSE, mar = c(5, 4, 4, 2))
```

### 2.5.2. Non-hierarchical clustering

A non-hierarchical clustering analysis is performed using several criteria to determine an appropriate number of clusters. Three different methods are considered: the \textbf{TWSS Elbow Graph}, the \textbf{Pseudo F Index}, and the \textbf{Silhouette Index}.

##### TWSS Elbow Graph

The Total Within-Cluster Sum of Squares (TWSS) has been computed for different values of k using the \textbf{k-means algorithm}, with the aim of identifying an appropriate number of clusters through the \textbf{Elbow method}.

As shown in the figure, the TWSS decreases sharply between $k=2$ and $k=4$, and then continues to decrease more gradually. A noticeable bend appears around $k=4$, suggesting this value as a reasonable choice for the number of clusters.

In addition, the ratio between the between-cluster sum of squares and the total sum of squares has been computed. For $k=4$, this ratio is equal to $0.694$, indicating a fair degree of separation between clusters.

```{r}
## Applying k-means algorithm for different number of clusters 
aux <- c()
for (i in 2:10){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- k$tot.withinss 
}
plot(2:10, aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")
points(4, aux[4 - 1], pch = 19, cex = 1.5, col = "red") # Optimum point: k=4

# k=3
k3 <- kmeans(dfscaled, centers = 3, nstart = 25) 

## Sum of Squares 
k3$withinss 
k3$totss 
k3$tot.withinss 
k3$betweenss + k3$tot.withinss
k3$betweenss/k3$totss # 0.6251488 separation between clusters

# k=4
k4 <- kmeans(dfscaled, centers = 4, nstart = 25)  

## Sum of Squares 
k4$withinss 
k4$totss 
k4$tot.withinss 
k4$betweenss + k4$tot.withinss 
k4$betweenss/k4$totss # 0.694 fair separation between clusters

# k=5
k5 <- kmeans(dfscaled, centers = 5, nstart = 25) 

## Sum of Squares 
k5$withinss 
k5$totss 
k5$tot.withinss 
k5$betweenss + k5$tot.withinss 
k5$betweenss/k5$totss # 0.735624 separation between clusters

# k=6
k6 <- kmeans(dfscaled, centers = 6, nstart = 25) 

## Sum of Squares
k6$withinss
k6$totss
k6$tot.withinss
k6$betweenss + k6$tot.withinss
k6$betweenss/k6$totss # 0.768 greater separation between clusters
```

##### Pseudo F Index

The \textbf{Pseudo F Index} has been computed for different values of k in order to evaluate the separation between clusters obtained with the k-means algorithm. This index is based on the ratio between the between-cluster sum of squares (BSS) and the within-cluster sum of squares (WSS).

In this case, the index decreases steadily as k increases, with the highest value observed at $k=2$. However, selecting two clusters does not provide a meaningful segmentation of the dataset and is not consistent with the structure suggested by the dendrogram and the TWSS Elbow method.

```{r}
# Pseudo F index = BSS*(n-k)/WSS*(k-1)
aux <- c()
for (i in 2:20){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- ((k$betweenss)*(nrow(dfscaled)-i))/((k$tot.withinss)*(i-1))
}
plot(2:20, aux, xlab="Number of Clusters", ylab="Pseudo-F", type="l", main="Pseudo F Index")
points(2, aux[2-1], pch = 19, cex = 1.5, col = "red")
# max Pseudo F: 2 clusters
```

##### Silhoutte Index

Finally, the \textbf{Silhouette Index} is computed. For each observation, this measure compares the average distance to points within its own cluster with the average distance to points in the nearest alternative cluster.

The average silhouette widths obtained for $k=3$, $k=4$, $k=5$, and $k=6$ are $0.35$, $0.36$, $0.28$, and $0.29$, respectively. The four-cluster solution exhibits the highest overall cohesion.

The silhouette index computed using the \emph{fastkmed()} function confirms the same pattern: across the range of cluster solutions considered, the highest silhouette values are obtained for three or four clusters, while performance decreases for larger values of k. Overall, the four-cluster solution provides a good balance between cluster separation and stability.

```{r}
## A) Silhouette function from cluster library

# k=3
si <- silhouette(k3$cluster, d)
plot(si) 

# k=4
si <- silhouette(k4$cluster, d)
plot(si) 

# k=5
si <- silhouette(k5$cluster, d)
plot(si)

# k=6
si <- silhouette(k6$cluster, d)
plot(si) 


## B) Silhouette function from kmed library
# k=3
res <- fastkmed(d, 3)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=3 higher silhouette value

# k=4
res <- fastkmed(d, 4)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=3 higher silhouette value

# k=5
res <- fastkmed(d, 5)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=4 higher silhouette value

# k=6
res <- fastkmed(d, 6) 
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=4 higher silhouette value
```

To sum up, the three criteria do not point to a single value of k, but they suggest a consistent overall tendency. The Elbow method shows a clear bend around four clusters, the Pseudo-F index does not provide useful guidance, and the Silhouette index indicates that three or four clusters yield the most coherent structure. Therefore, \textbf{four clusters} appear to be the most balanced and interpretable solution, and this value is used in the subsequent steps of the analysis.

### 2.5.3. k-means clustering using the optimal number of clusters

\textbf{- Cluster 1: Large, high-value diamonds}
This cluster contains the largest diamonds in terms of avergage carat ($\approx 1.65$) and physical dimensions (x, y, z), and it is associated with the highest average price ($\approx 11.200$). It shows a high prevalence of Premium cuts, while color grades are mainly concentrated in the near-colorless to tinted range (H and I/J). Clarity grades are predominantly in the lower categories. This group corresponds to large diamonds whose price is mainly driven by size.

\textbf{- Cluster 2: Small, low-value diamonds}
This cluster groups the smallest diamonds, with the lowest average carat ($\approx 0.39$), dimensions, and prices ($\approx 1.200$). A large proportion of Ideal cuts are in this cluster, with medium color grades and clarity levels.

\textbf{- Cluster 3: Medium-sized, balanced diamonds}
Diamonds in this cluster have intermediate carat ($\approx 0.76$), moderate dimensions and price values ($\approx 3.180$). They are characterized by a strong dominance of Premium and Very Good cuts and colorless diamonds. This cluster represents well-balanced diamonds combining size and quality.

\textbf{- Cluster 4: Upper-mid-range diamonds}
Diamonds in this cluster are larger than average with $\approx 0.95$ average carat and $\approx 6.25$ x, y values. They are the second most expensive diamonds cluster, with an average of $\approx 4.630$. There is a high proportion of Ideal and clearless diamonds. Hence, this cluster represents upper-mid-range diamonds.

```{r}
set.seed(12345) # NO TREURE LA SET SEED D'AQUI QUE SINO CANVIA!!

# Apply k-means with the selected number of clusters
k4 <- kmeans(dfscaled, centers = 4, nstart = 25)
dfClustKmeans <- diamondsOg
# Add cluster label to original unscaled data
dfClustKmeans$Cluster <- as.factor(k4$cluster)

# Descriptive statistics per cluster
cluster_summary <- dfClustKmeans[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(everything(), mean, na.rm = TRUE))
cluster_summary

table(dfClustKmeans$Cluster)
prop.table(table(dfClustKmeans$Cluster))

# Clusters based on factors
table(dfClustKmeans$Cluster, dfClustKmeans$cut)
table(dfClustKmeans$Cluster, dfClustKmeans$color)
table(dfClustKmeans$Cluster, dfClustKmeans$clarity)
```

### 2.5.4. PCA-based clustering

The Hierarchical Clustering on Principal Components (HCPC) method has been applied to the scores obtained from the previously computed PCA. The categorical variables have been set as supplementary in order to allow their interpretation within the resulting clusters.

```{r}
diamonds.pca.sup.clust <- PCA(diamonds, quanti.sup = 10, quali.sup = c(1,2,3), graph = T)
```


#### Automatic cut: 3 clusters
The \emph{HCPC()} function has been used with an automatic selection of the number of clusters, which has resulted in a partition into $3$ clusters.

From the factor map, the three clusters have been projected onto the first two principal components. Cluster $1$ is mainly located on the negative side of the first dimension, which is associated with smaller physical measurements of the diamonds. Cluster $2$ occupies an intermediate region, whereas Cluster $3$ is positioned towards the positive side of the first component, corresponding to larger diamond sizes. It is worth noting that no clear separation is observed along the second dimension, indicating that the clustering criterion mainly distinguishes diamonds according to size and price.

```{r}
# HCPC applied to the previously computed PCA
res.hcpc <- HCPC(diamonds.pca.sup.clust, -1) # automatic cut: 3 clusters

# Dendrogram visualization
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - PCA-based Hierarchical Clustering")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on PCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc, choice = "3D.map", main = "3D Cluster Visualization")
```
```{r}
table(res.hcpc$data.clust$clust)
prop.table(table(res.hcpc$data.clust$clust))

# Cluster description: categorical variables
res.hcpc$desc.var$test.chi2
res.hcpc$desc.var$category

# Cluster description: numerical variables
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti
```

```{r}
dfClustPCA3 <- diamondsOg
# Add cluster label to original unscaled data
dfClustPCA3$Cluster <- as.factor(res.hcpc$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_pca <- dfClustPCA3[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_pca

table(dfClustPCA3$Cluster)
prop.table(table(dfClustPCA3$Cluster))

# Clusters based on factors
table(dfClustPCA3$Cluster, dfClustPCA3$cut)
table(dfClustPCA3$Cluster, dfClustPCA3$color)
table(dfClustPCA3$Cluster, dfClustPCA3$clarity)
```

#### 4 clusters

Although the automatic HCPC cut suggests a three-cluster partition, the factor map and the dendrogram reveal that a four-cluster solution is more balanced and more interpretable. The three-cluster solution produces one dominant group, whereas splitting the structure into four clusters yields groups of more comparable size and a clearer separation in the PCA space. In particular, the four-cluster solution takes into account both the first and the second principal dimensions, separating the diamonds not only according to their overall size and price, but also according to their geometric proportions, as captured by depth and table.

```{r}
# HCPC applied to the previously computed PCA
res.hcpc <- HCPC(diamonds.pca.sup.clust, nb.clust = 4, consol = TRUE) # 4 clusters

pal <- c("#000000", "#E41A1C", "green3", "dodgerblue") 

# Dendrogram visualization
fviz_dend(res.hcpc, k = 4, rect = TRUE,  rect_fill = TRUE, k_colors = pal, main = "Dendrogram - PCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc, choice = "bar", main = "Inertia gain on PCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on PCA Factor Map (Dim 1 vs Dim 2)")

fviz_cluster(res.hcpc, show.clust.cent = TRUE, main = "Cluster Visualization on PCA Factor Map (Dim 1 vs Dim 2)", labelsize = 0)
```

The PCA-based hierarchical clustering applied with four clusters reveals clearly differentiated diamond profiles in terms of quality and physical characteristics. A chi-square test indicates a highly significant association between cluster membership and all categorical variables (clarity, color, and cut) leading to the rejection of the null hypothesis of independence in every case, as evidenced by the corresponding p-values. Furthermore, the ANOVA results indicate highly significant differences across all major numerical variables. In particular, the strongest discriminatory power is observed for the dimensional variables (x,y,z), as well as for carat weight and price, for which the null hypothesis of equal means across clusters is decisively rejected with extremely small p-values. Depth and table also differ significantly across clusters, although with more moderate effect sizes. These results confirm that the clusters reflect fundamentally different diamond size, value, and proportion profiles.

\textbf{- Cluster 1 (black): Small, high-clarity and clear diamonds with low price}

This cluster accounts for $42.8\%$ of the sample. It is characterized by very high clarity grades, with a strong over-representation of IF/VVS1 and VVS2 diamonds. For instance, $77.8\%$ of IF/VVS1 diamonds belong to this cluster, largely exceeding their overall proportion in the dataset. Ideal cuts and color E are also significantly over-represented, whereas more tinted color categories such as I/J and lower clarity levels such as I1/SI2 are strongly under-represented. Overall, this cluster is mainly composed of colorless and high-clarity diamonds.

Despite their superior clarity and cut, diamonds in this cluster exhibit significantly smaller physical dimensions (x, y and z), approximately $18$ units below the overall mean, and lower logCarat values, around $18.7$ units below the overall mean. The logPrice variable is also well below the overall mean, by approximately $18.2$ units. These diamonds tend to present slightly higher depth values and smaller table values, indicating more pointed stones with narrower top surfaces, which is consistent with their smaller overall size.

Therefore, this cluster is composed of small-sized diamonds with excellent cut, color, and clarity characteristics, whose market value remains limited mainly due to their reduced physical size rather than to quality-related attributes.

\textbf{- Cluster 2 (red): Shallow diamonds with low cut quality}

This cluster accounts for $14.3\%$ of the sample and exhibits a distinctly cut-driven profile. Fair/Good cuts are significantly over-represented, whereas Ideal cuts are strongly under-represented. Diamonds with color grade F also appear more frequently than expected in this cluster.

As expected, from a numerical perspective, this cluster is primarily distinguished by proportion-related variables rather than by size or price. Diamonds in this group display significantly higher table values (approximately $10.2$ units above the overall mean) and substantially lower depth values (about $12.5$ units below the overall mean), indicating flatter stones. No strong deviations are observed for carat weight, price, or the main dimensional measures (x, y and z).

Consequently, this cluster corresponds to diamonds with less optimal geometric proportions, typically associated with lower cut quality, but without extreme differences in overall size or price value.

\textbf{- Cluster 3 (green): Medium-sized diamonds with moderate prices}

This cluster accounts for $27.6\%$ of the sample. It contains diamonds that are moderately larger and more expensive than average, as indicated by significantly positive deviations in logCarat, logPrice, and all three physical dimensions (x, y and z). Depth values are also slightly above the overall mean, while table values are marginally lower. It should be highlighted that the numerical deviations are moderate, as they range between approximately $6$ and $8$ units above the overall mean.

From a qualitative perspective, this cluster shows an over-representation of lower clarity grades (I1/SI2) and an under-representation of very high clarity categories (IF/VVS1 and VVS2). Premium cuts are also under-represented, suggesting that value in this cluster is driven more by physical size than by clarity or cut quality.

Accordingly, this cluster represents medium-sized diamonds which, despite lower clarity grades, are associated with above-average prices.

\textbf{- Cluster 4 (blue): Large, lower-clarity and tinted diamonds with high price}

This cluster accounts for $15.3\%$ of the sample and is clearly distinguished by the largest and most valuable diamonds in the dataset. All size-related variables (x, y, z and logCarat), together with logPrice, show very large positive deviations from the overall mean, ranging approximately from $14$ to $15.5$ units. In addition, the table variable also presents a mean value above the overall average.

From a qualitative perspective, this cluster is associated with more tinted diamonds, with a higher frequency of color grades I/J and H, as well as lower clarity levels (I1/SI2). In contrast, high-quality color and clarity categories (D, E, F and IF/VVS1) are strongly under-represented. Premium cuts are significantly over-represented, whereas Ideal cuts are under-represented.

This pattern indicates that diamonds in this cluster achieve their high prices primarily through size rather than quality, emphasising the dominant role of physical magnitude over color, clarity, and cut in determining their market value.

The spatial distribution of the clusters in the PCA factor map further supports the validity of the clustering solution. In the Figure, clusters $1$ (black), $3$ (green) and $4$ (blue) are located at along the first dimension, indicating a strong separation based on physical size and price. On the other hand, cluster $2$ (red) is mainly differentiated along the second dimension, reflecting differences in geometric proportions, clarity, and cut quality.

In conclusion, the PCA-based hierarchical clustering proves to be particularly effective in this context. By performing clustering on the principal components rather than on the original correlated variables, the method focuses on the main sources of variability in the data and reduces redundancy. This leads to well-separated, stable, and interpretable clusters that reflect meaningful differences in diamond size, price and quality.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\textbf{- Cluster 1 (black): Small, low price diamonds}
This cluster includes the smallest diamonds, with a low average carat (≈ 0.39), small dimensions (x, y ≈ 4.65), and the lowest mean price (≈ 1,020). From a categorical perspective, it is dominated by Ideal cut diamonds, with colors mainly in the E–G range. Clarity is centered on SI1 and VS2, although a substantial number of high-clarity stones (IF/VVS1) are also present. Overall, this cluster represents small diamonds with relatively good and balanced quality attributes.

\textbf{- Cluster 2 (red): Medium-low diamonds}
Diamonds in this cluster have intermediate size (carat ≈ 0.78; x, y ≈ 5.92) and moderate prices (≈ 3,300). They are characterized by lower depth and higher table values, indicating flatter proportions. Categorically, this group is mainly composed of Premium-cut diamonds, with F color most frequent and clarity largely concentrated in SI1 and VS2, with very few high-clarity stones. This cluster reflects a compromise where size increases at the expense of quality.

\textbf{- Cluster 3 (green): Medium-high diamonds}
This cluster groups slightly larger diamonds (carat ≈ 0.94; x, y ≈ 6.23) with higher prices than Cluster 2 (≈ 4,630). These diamonds show higher depth and lower table values, suggesting improved geometric balance. In categorical terms, Ideal cut is predominant, with G color most common. However, clarity is mainly SI1 and I1/SI2, indicating lower clarity on average than Cluster 1 despite better proportions.


\textbf{- Cluster 4 (blue): Large, high-value diamonds}
Diamonds in this cluster are the largest and most expensive, with the highest carat (≈ 1.65), the largest dimensions (x, y ≈ 7.56), and the highest mean price (≈ 11,100). This group is dominated by Premium cuts, poorer color grades (H–I/J), and lower clarity levels, mainly I1/SI2 and VS2. These results indicate that, for this cluster, diamond value is driven primarily by size rather than quality attributes.

Overall, the four clusters describe a clear size-quality trade-off: smaller diamonds tend to concentrate better quality grades, whereas larger diamonds are associated with lower color and clarity but substantially higher prices.
```{r}
table(res.hcpc$data.clust$clust)
prop.table(table(res.hcpc$data.clust$clust))

# Cluster description: categorical variables
res.hcpc$desc.var$test.chi2
res.hcpc$desc.var$category

# Cluster description: numerical variables
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti
```

```{r}
dfClustPCA4 <- diamondsOg
# Add cluster label to original unscaled data
dfClustPCA4$Cluster <- as.factor(res.hcpc$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_pca <- dfClustPCA4[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_pca

table(dfClustPCA4$Cluster)
prop.table(table(dfClustPCA4$Cluster))

# Clusters based on factors
table(dfClustPCA4$Cluster, dfClustPCA4$cut)
table(dfClustPCA4$Cluster, dfClustPCA4$color)
table(dfClustPCA4$Cluster, dfClustPCA4$clarity)
```

### 2.5.5. MCA-based clustering

#### Automatic cut: 6 clusters

Hierarchical clustering has also been applied to the MCA scores using the HCPC approach. The automatic cut has resulted in a six-cluster solution. However, inspection of the factor map shows that these clusters are highly overlapping and mainly separated along dimension $1$, which reflects the size gradient induced by the over/under transformations of the numerical variables. Dimension $2$, which is related to geometric proportions and cut quality, provides only limited additional separation. As a consequence, the six-cluster solution does not yield a clear or interpretable partition of the diamonds

```{r}
# HCPC applied to the previously computed MCA
res.hcpc.mca <- HCPC(res.mca, -1) # automatic cut: 6 clusters

# Dendrogram visualization
fviz_dend(res.hcpc.mca, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - MCA-based Hierarchical Clustering")

# Visualize clusters in the MCA factor map
fviz_cluster(res.hcpc.mca, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on MCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.mca, choice = "3D.map", main = "3D Cluster Visualization")
```

#### 4 clusters

Although the automatic HCPC cut on the MCA scores suggests a six-cluster partition, inspection of the factor map and the dendrogram reveals substantial overlap between groups and weak separation. 

Forcing a four-cluster solution does not improve this situation, as the clusters remain largely overlapping and lack clear interpretability. Consequently, MCA-based clustering is not retained as an appropriate solution for this analysis.

```{r}
# HCPC applied to the previously computed MCA
res.hcpc.mca <- HCPC(res.mca, nb.clust = 4, consol = TRUE) # 4 clusters

pal <- c("#000000", "#E41A1C", "green3", "dodgerblue") 

# Dendrogram visualization
fviz_dend(res.hcpc.mca, k = 4, rect = TRUE,  rect_fill = TRUE, k_colors = pal, main = "Dendrogram - PCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.mca, choice = "bar", main = "Inertia gain on MCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc.mca, repel = TRUE, show.clust.cent = TRUE,main = "Cluster Visualization on MCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.mca, choice = "3D.map", main = "3D Cluster Visualization")
```

```{r}
table(res.hcpc.mca$data.clust$clust)
prop.table(table(res.hcpc.mca$data.clust$clust))

# Cluster description: categorical variables
res.hcpc.mca$desc.var$test.chi2
res.hcpc.mca$desc.var$category
```

```{r}
dfClustMCA <- diamondsOg
# Add cluster label to original unscaled data
dfClustMCA$Cluster <- as.factor(res.hcpc.mca$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_mca <- dfClustMCA[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_mca

table(dfClustMCA$Cluster)
prop.table(table(dfClustMCA$Cluster))

# Clusters based on factors
table(dfClustMCA$Cluster, dfClustMCA$cut)
table(dfClustMCA$Cluster, dfClustMCA$color)
table(dfClustMCA$Cluster, dfClustMCA$clarity)
```

### 2.5.6. FAMD-based clustering

The clustering analysis that incorporates both numerical and categorical variables has been conducted using a Factor Analysis of Mixed Data (FAMD).

The interpretation of the continuous variables is fully consistent with the results previously obtained from the PCA. 

Dimension 1, which explains $26.69\%$ of the total variability, is dominated by the physical measurements of the diamonds (x, y, z, logCarat, and logPrice), all of which exhibit very high squared correlations with this axis. As in the PCA, this dimension represents a size-value gradient, separating small, inexpensive diamonds from larger and more valuable ones. The categorical variables follow the same structure. More tinted color grades (especially I/J) and lower clarity categories (I1/SI2) are positioned on the positive side of Dimension 1, indicating an association with larger and more expensive diamonds. In contrast, higher clarity grades (IF/VVS1 and VVS2) and colorless grades (E) are located on the negative side of this dimension, corresponding to smaller but higher-quality diamonds.

Dimension 2, which explains $8.31\%$ of the variance, mainly reflects differences in geometric proportions. Ideal cuts are associated with positive values of this dimension, whereas Premium and Fair/Good cuts are located on the negative side, indicating less optimal geometric balance.

```{r}
# Factor Analysis for Mixed Data
res <- FAMD(diamonds)
summary(res)
```

The FAMD-based hierarchical clustering applied with three clusters reveals clearly differentiated diamond profiles in terms of quality and physical characteristics. A chi-square test indicates a highly significant association between cluster membership and all categorical variables (clarity, color, and cut) leading to the rejection of the null hypothesis of independence in every case, as evidenced by the corresponding p-values. Furthermore, the ANOVA results indicate highly significant differences across all size-related measures, indicating that they are the strongest discriminators across clusters. In other words, the variables x, y, z, logCarat, and logPrice all exhibit extremely large effect sizes, indicating that cluster separation is driven predominantly by diamond size and price. In contrast, table and depth show statistically significant but much smaller effect sizes, suggesting a secondary role in cluster differentiation.

\textbf{- Cluster 1 (black): Small, high-clarity diamonds with low price}
This cluster accounts for $47\%$ of sample the sample. It is strongly associated with very high clarity grades, with IF/VVS1 and VVS2 diamonds being significantly over-represented. Ideal cuts and color E are also over-represented, whereas darker color grades (H and I/J), Premium cuts, and low-clarity categories (I1/SI2) are significantly under-represented.

From a numerical perspective, this cluster is characterized by substantially smaller diamonds. All size-related variables (x, y, z and logCarat), together with logPrice, exhibit very large negative deviations from the overall mean, indicating that these diamonds are considerably smaller and less expensive than average. Table values are also slightly below the overall mean.

Overall, this cluster represents small, low-priced diamonds with excellent clarity and cut quality, for which superior quality does not translate into high prices due to limited physical size.

\textbf{- Cluster 2 (red): Medium-sized diamonds with moderate quality} 
This cluster accounts for $38\%$ of sample the sample and shows a categorical profile dominated by mid clarity grades, with SI1 and I1/SI2 diamonds being significantly over-represented. In contrast, very high clarity categories (IF/VVS1 and VVS2) are clearly under-represented.

From a numerical perspective, this cluster contains moderately large and relatively valuable diamonds. All dimensional variables and logCarat display significant positive deviations from the overall mean, as does logPrice, indicating above-average size and price. Table values are slightly higher than average, while no pronounced deviations are observed for depth.

Thus, this cluster corresponds to medium-sized diamonds whose market value is mainly driven by carat weight, despite only moderate clarity levels.

\textbf{- Cluster 3 (green): Very large, lower-quality diamonds}
This cluster accounts for $15\%$ of sample the sample and is clearly distinguished by its association with lower color and clarity grades. Diamonds with color I/J, Premium cuts, and I1/SI2 clarity are strongly over-represented, whereas high-quality color grades (D, E, F), Ideal cuts, and very high clarity categories (IF/VVS1) are significantly under-represented.

From a numerical point of view, this cluster contains the largest and most expensive diamonds in the dataset. All size-related variables and logCarat exhibit extremely large positive deviations from the overall mean, and logPrice is also far above average. Table values are higher than average, while depth is slightly below average, indicating flatter geometric proportions.

Overall, this pattern indicates that diamonds in this cluster attain high prices primarily due to their size rather than intrinsic quality, highlighting the dominant role of carat weight over color, clarity or cut precision.

```{r}
# Hierarchical Clustering on FAMD scores
res.hcpc.famd <- HCPC(res, -1, consol = TRUE) # automatic cut: 3 clusters

pal <- c("#000000", "#E41A1C", "green3") 

# Dendrogram visualization
fviz_dend(res.hcpc.famd, k = 3, rect = TRUE,  rect_fill = TRUE, k_colors = pal, main = "Dendrogram - FAMD-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.famd, choice = "bar", main = "Inertia gain on FAMD")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc.famd, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on FAMD Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.famd, choice = "3D.map", main = "3D Cluster Visualization")
```

```{r}
table(res.hcpc.famd$data.clust$clust)
prop.table(table(res.hcpc.famd$data.clust$clust))

# Cluster description: categorical variables
res.hcpc.famd$desc.var$test.chi2
res.hcpc.famd$desc.var$category

# Cluster description: numerical variables
res.hcpc.famd$desc.var$quanti.var
res.hcpc.famd$desc.var$quanti
```

```{r}
dfClustFAMD <- diamondsOg
# Add cluster label to original unscaled data
dfClustFAMD$Cluster <- as.factor(res.hcpc.famd$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_famd <- dfClustFAMD[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_famd

table(dfClustFAMD$Cluster)
prop.table(table(dfClustFAMD$Cluster))

# Clusters based on factors
table(dfClustFAMD$Cluster, dfClustFAMD$cut)
table(dfClustFAMD$Cluster, dfClustFAMD$color)
table(dfClustFAMD$Cluster, dfClustFAMD$clarity)
```

## 2.6. Discriminant analysis

In this section, the objective is to develop a model that predicts the cut of each diamond based on the depth and table variables. To apply the subsequent methods, it is necessary for the numerical variables to follow a normal distribution. Based on the exploratory data analysis conducted earlier, normality can only be assumed for the depth and table variables. Furthermore, since a certain association between the diamond cut and its geometric proportions has been observed, it is reasonable to create a model that predicts the cut using these variables.


### 2.6.1. MANOVA

Before developing the model, it is important to assess whether significant differences exist in the depth and table variables across the different diamond cuts. Hence, MANOVA statistical test can be used to assess whether there are overall differences in the mean vectors of depth and table across multiple diamond cuts simultaneously.

First, it is important to assess the homogeneity of variances across the different cut levels for the two normally distributed variables. To this end, Box’s M test has been applied to evaluate the equality of variance-covariance matrices, using the \emph{boxM()} function.

As the resulting p-value is significantly lower than the significance level $\alpha=0.05$, the null hypothesis of homogeneity of variances is rejected. Furthermore, an inspection of the boxplots reveals clear differences in the variance homogeneity across the depth and table variables.

```{r}
# Homogeneity of covariance matrices obtained from multivariate normal data
boxM(
  diamonds[, c("depth", "table")],
  diamonds$cut
) #p-value < 0.05

boxplot(depth ~ cut, data = diamonds, main = "Depth distribution in different cuts")
boxplot(table ~ cut, data = diamonds, main = "Table distribution in different cuts")
```

The MANOVA test rejects the null hypothesis of equal mean vectors for depth and table across the different cut levels, with a very small p-value ($<2.2\times10^{-16}$). This indicates that there is a statistically significant difference in the means of depth and table when comparing the various levels of the cut variable. To further investigate these differences, separate ANOVA tests have been conducted for depth and table individually. 

The results of the ANOVA test for the depth variable reject the null hypothesis of equal means across groups (p-value of $2.5\times10^{-5}$), indicating significant differences in mean depth values across the four diamond cut categories. Specifically, the post-hoc Tukey tests reveal that Premium diamonds differ significantly in depth from Fair/Good, Ideal, and Very Good diamonds. However, no significant differences in depth are observed between the following pairs: Very Good vs Fair/Good, Very Good vs Ideal, and Ideal vs Fair/Good, suggesting that these cut categories exhibit similar depth characteristics.

Similarly, the ANOVA test for the table variable also rejects the null hypothesis of equal means across groups (p-value $<2.2\times10^{-16}$), indicating significant differences in mean table values across the four cut categories. Post-hoc Tukey tests reveal that, with the exception of the Premium vs Fair/Good and Very Good vs Fair/Good comparisons, all other cut categories show significant differences in table values.

```{r}
res.man <- manova(cbind(depth, table) ~ cut, data = diamonds)
summary(res.man)
summary.aov(res.man)

TukeyHSD(aov(depth~cut,data=diamonds),"cut")
TukeyHSD(aov(table~cut,data=diamonds),"cut")
```

### 2.6.2. Discriminant Analysis

Considering the three main types of discriminant models, it is reasonable to use Quadratic Discriminant Analysis (QDA) to create a model that predicts the diamond cut based on the variables depth and table. Since the two variables are slightly correlated, the Naive Bayes method is not appropriate. Additionally, because the variables do not share the same covariance matrix, Linear Discriminant Analysis is also unsuitable.

To obtain an unbiased assessment of the predictive performance, the dataset has been randomly divided into a training set and a test set. The Quadratic Discriminant Analysis model has been estimated using only the training data, while all predictions and performance measures have been computed exclusively on the test set. This separation ensures that the reported classification accuracy reflects the model’s ability to generalize to unseen observations rather than its fit to the data used for estimation.

The prior probabilities reflect the empirical class distribution of the training dataset, with Ideal ($42\%$) and Premium ($25.5\%$) cuts being the most frequent categories and Fair/Good ($9.3\%$) the least frequent. The estimated group means show coherent and gradual patterns across cut categories. Nonetheless, there is no cut category characterized by extremely high table values combined with very low depth values, nor by very low table values combined with high depth values, indicating that extreme geometric configurations are not exclusive to any single cut category.

```{r}
# train 2/3, test 1/3
lltrain <- sample(1:nrow(diamonds), round(0.67*nrow(diamonds), dig=0))
dftrain <- diamonds[lltrain,]
dftest <- diamonds[-lltrain,]
qda.fit <- qda(cut ~ depth + table, data = dftrain)
qda.fit

qda.fit$prior
```

When evaluated on the train set, the QDA achieves an overall correct classification rate (CCR) of $68.8\%$, which represents a substantial improvement over random classification and indicates good predictive performance given the limited number of predictors. In particular, the model performs very well for the most prevalent and well-defined categories, Ideal and Premium, with class-specific accuracies of $92\%$ and $88.8\%$, respectively. This suggests that depth and table are especially informative for identifying higher-quality cuts, where geometric differences are more pronounced. In contrast, classification performance is considerably lower for Fair/Good ($16\%$) and Very Good ($18.6\%$) cuts.

It should be highlighted that the model represents a clear improvement over random guessing. The baseline accuracy, computed from class proportions, is $30.4\%$, whereas the achieved CCR is substantially higher. Furthermore, the Q statistic rejects the null hypothesis of random classification. This confirms that the discriminant function extract meaningful information for cut classification, even with a limited set of predictors.

```{r}
# Predicting Groups
pred <- predict(qda.fit, newdata = dftrain)

# Class Predictions 
qda.fit$class

# Posterior Probabilities 
qda.fit$posterior

# Contingency Table of Observed and Predicted Values
# rows are the real cut categories and columns the predicted categories
tab<-table(dftrain$cut,pred$class); tab

# Correct Classification Rate (CCR)
classrate<-sum(diag(tab))/sum(tab)
classrate

# Total CCR (alternative way)
sum(diag(prop.table(tab)))

# CCR across groups (over rows)
diag(prop.table(tab, 1))

# Prediction Accuracy p1^2+p2^2+p3^2+p4^2
pa <- sum(qda.fit$prior^2)
pa

# Q-statistic
n <- sum(tab) # Total number of observations
k <- nrow(tab) # Number of classes (number of rows in the confusion matrix)
ntilde <- sum(diag(tab)) # Number of correctly classified observations (sum of the diagonal)

Q <- (n - ntilde*k)^2 / (n * (k - 1)); Q

# p-value using chi-squared distribution with (k - 1) degrees of freedom
pvalue <- 1 - pchisq(Q, df = k - 1); pvalue
```

When evaluated on the test set, the QDA achieves an overall correct classification rate (CCR) of $66.5\%$, which represents a substantial improvement over random classification and indicates good predictive performance given the limited number of predictors. In particular, the model performs very well for the most prevalent and well-defined categories, Ideal and Premium, with class-specific accuracies of $89.4\%$ and $84.4\%$, respectively. This suggests that depth and table are especially informative for identifying higher-quality cuts, where geometric differences are more pronounced. In contrast, classification performance is considerably lower for Fair/Good ($27.7\%$) and Very Good ($23.7\%$) cuts.

Misclassifications occur mainly between neighboring cut categories. The amount of Fair/Good diamonds that are classified as Very Good is twice the number of those correctly classified, and the same pattern is seen with Very Good diamonds being classified as Premium. This pattern is consistent with the ordinal nature of cut quality and indicates that the model captures the underlying structure of the data rather than producing arbitrary errors.

It should be highlighted that the model represents a clear improvement over random guessing. The baseline accuracy, computed from class proportions, is $30.4\%$, whereas the achieved CCR is substantially higher. Furthermore, the Q statistic rejects the null hypothesis of random classification. This confirms that the discriminant function extract meaningful information for cut classification, even with a limited set of predictors.

In conclusion, the results show that Quadratic Discriminant Analysis provides a robust and informative classification approach, and that depth and table alone allow for a reasonably accurate and interpretable prediction of diamond cut quality, particularly for distinguishing higher-grade cuts.

```{r}
# Predicting Groups
pred <- predict(qda.fit, newdata = dftest)

# Class Predictions 
qda.fit$class

# Posterior Probabilities 
qda.fit$posterior

# Contingency Table of Observed and Predicted Values
# rows are the real cut categories and columns the predicted categories
tab<-table(dftest$cut,pred$class); tab

# Correct Classification Rate (CCR)
classrate<-sum(diag(tab))/sum(tab)
classrate

# Total CCR (alternative way)
sum(diag(prop.table(tab)))

# CCR across groups (over rows)
diag(prop.table(tab, 1))

# Prediction Accuracy p1^2+p2^2+p3^2+p4^2
pa <- sum(qda.fit$prior^2)
pa

# Q-statistic
n <- sum(tab) # Total number of observations
k <- nrow(tab) # Number of classes (number of rows in the confusion matrix)
ntilde <- sum(diag(tab)) # Number of correctly classified observations (sum of the diagonal)

Q <- (n - ntilde*k)^2 / (n * (k - 1)); Q

# p-value using chi-squared distribution with (k - 1) degrees of freedom
pvalue <- 1 - pchisq(Q, df = k - 1); pvalue
```
