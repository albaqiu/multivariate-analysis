---
title: "Diamonds Data Frame Analysis"
author: "Berta Torrents"
date: "December 2025"
output:
  word_document: default
  pdf_document:
    latex_engine: xelatex
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  results = 'markup',
  message = FALSE,
  warning = FALSE,
  fig.width = 7,
  fig.height = 5
)
```

```{r}
set.seed(12345)

# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Libraries
library(dplyr)
library(FactoMineR)
library(MASS)
library(psych)
library(factoextra)
library(corrplot)
```

## 1. Data preparation

The dataset have been imported using the \emph{read.csv()} function from the \emph{dplyr} library. The variable named X have been removed, as it corresponds to an index column rather than an explanatory variable. Additionally, the categorical variables cut, color, and clarity have been correctly converted into factors.

```{r}
diamonds <- read.csv('diamonds.csv')

diamonds <- diamonds[,-1]

diamonds$cut <- as.factor(diamonds$cut)
diamonds$color <- as.factor(diamonds$color)
diamonds$clarity <- as.factor(diamonds$clarity)

summary(diamonds)
```

Since the dataset contained a large number of observations $(53940)$, a random sample of $500$ observations has been selected to allow for a more manageable and focused analysis. It is worth noting that the means of the numerical variables and the proportions of observations in each category do not vary significantly.

```{r}
n <- nrow(diamonds)
indices <- sample(1:n, 500) # Take 500 random indices
diamonds <- diamonds[indices,]
# rownames(diamonds) <- seq_len(nrow(diamonds))

summary(diamonds)
```

## Exploratory data analysis

For each variable, we check: duplicates, existence of zeros, existence of outliers, existence of missing values and we apply transformations if it is needed.

## Numerical variables

Helper function to evaluate missing values and zeros

```{r}
check_numeric <- function(x) {
  list(na = sum(is.na(x)),
  zeros = sum(x == 0))
}
```

```{r}
num_vars <- diamonds %>% select_if(is.numeric)
results <- lapply(num_vars, check_numeric)
results
```
We observe that no numeric variable has NA or zeros

### Carat variable

After analyzing the carat variable, we observe that it doesn't have severe outliers. The carat variable is right-skewed, with a long tail towards larger values, so is clearly not normally distributed. After using boxcox function, we see that a log transformation is appropriate to improve symmetry and stabilize variance.

```{r}
# Outliers
boxplot(diamonds$carat, horizontal = T)
varout <- summary(diamonds$carat)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr

boxplot(diamonds$carat, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out <- which((diamonds$carat >= usout) | (diamonds$carat <= lsout))
sev_out # 0

# Distribution
hist(diamonds$carat,freq=F)
curve(dnorm(x),add=T, col="red") #It is obviously not normally distributed

# Transformation
boxcox(diamonds$carat~1,lambda=seq(-1,1,by=0.1)) # Box-Cox Transformation
diamonds$logCarat <- log(diamonds$carat) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logCarat, horizontal = T) # No outliers

hist(diamonds$logCarat, freq = F) # Is logCarat normally distributed?
m = mean(diamonds$logCarat)
std = sd(diamonds$logCarat)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)
```

### Depth variable

The depth variable has 2 severe outliers, which we removed. The histogram of the variable shows that the depth variable has an approximately symmetric, bell-shaped distribution, so no transformation is needed. Nonetheless, the normality test shapiro shows a low p-value, rejecting the normality distribution hypothesis of the variable.

```{r}
# Outliers
boxplot(diamonds$depth, horizontal = T)
varout <- summary(diamonds$depth)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$depth >= usout) | (diamonds$depth <= lsout))

boxplot(diamonds$depth, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #[1] 462 467
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Distribution
hist(diamonds$depth,freq=F)
m = mean(diamonds$depth)
std = sd(diamonds$depth)
curve(dnorm(x,m,std),add=T, col="red")

shapiro.test(diamonds$depth)
```


### Table variable

The variable table has 1 severe outlier. After removing it, we see that the distribution of this variable is almost symmetric with mild right skewness. Since its shape is already close to normal and a transformation would not provide meaningful improvement, no transformation is applied. However, the shapiro test returned a low p-value, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$table, horizontal = TRUE)
varout <- summary(diamonds$table)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$table >= usout) | (diamonds$table <= lsout))

boxplot(diamonds$table, horizontal = TRUE)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #206
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Distribution
hist(diamonds$table,freq=F)
m = mean(diamonds$table)
std = sd(diamonds$table)
curve(dnorm(x,m,std),add=T, col="red")

shapiro.test(diamonds$table)
```

### Price variable

The variable price doesn't have severe outliers, but we can observe a long tail at the right of the boxplot, indicating potential mild outliers.

The histogram shows a right skewed distribution, which can be improved by a transformation to reduce its skewness and stabilize variance. Based on the boxcox, it seems like a logarithmic transformation would be suitable.

After applying the log transformation, the distribution becomes more symmetric and closer to a bell-shaped form, without being still normal distributed.

```{r}
# Outliers
boxplot(diamonds$price, horizontal = T)
varout <- summary(diamonds$price)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$price > usout) | (diamonds$price < lsout))

boxplot(diamonds$price, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$price, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$price)
std = sd(diamonds$price)
curve(dnorm(x,m,std),add=T, col="red")

# Logarithmic transformation
boxcox(diamonds$price~1,lambda=seq(-1,1,by=0.1)) # Box-Cox Transformation
diamonds$logPrice <- log(diamonds$price) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logPrice, horizontal = T) # No outliers
hist(diamonds$logPrice, freq = F) # Is logPrice normally distributed?
m = mean(diamonds$logPrice)
std = sd(diamonds$logPrice)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)
```

### x variable
The x variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$x, horizontal = T)
varout <- summary(diamonds$x)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$x > usout) | (diamonds$x < lsout))

boxplot(diamonds$x, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$x, freq = F)
m = mean(diamonds$x)
std = sd(diamonds$x)
curve(dnorm(x,m,std),add=T, col="red")
```

### y variable
The y variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$y, horizontal = T)
varout <- summary(diamonds$y)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$y > usout) | (diamonds$y < lsout))

boxplot(diamonds$y, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$y, freq = F)
m = mean(diamonds$y)
std = sd(diamonds$y)
curve(dnorm(x,m,std),add=T, col="red")
```

### z variable
The y variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$z, horizontal = T)
varout <- summary(diamonds$z)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$z > usout) | (diamonds$z < lsout))

boxplot(diamonds$z, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$z, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$z)
std = sd(diamonds$z)
curve(dnorm(x,m,std),add=T, col="red")
```

## Categorical variables

### Cut variable 

Cut variable shows no missing values. It is clearly unbalanced, with the Ideal level accounting for 37.4% of observations, while the Fair level comprising only 1.8% of the observations. These strange categories could be grouped further depending on the results of the multivariate analysis.

```{r}
# Check NA (we don't need to check 0s)
cat("Missing values in cut:", sum(is.na(diamonds$cut)), "\n") #0

# Distribution
summary(diamonds$cut)
barplot(summary(diamonds$cut), main="Diamond Cut Quality")

prop.table(table(diamonds$cut))
```

### Color variable

Color variable shows no missing values. It is a more balanced variable. Nonetheless, the most frequent grade is G, representing 12.6% of observations, while the least frequent is J, with 3.4%, indicating that diamonds with a noticeable tint are less common in the dataset.

```{r}
# Check NA
cat("Missing values in color:", sum(is.na(diamonds$color)), "\n") #0

# Distribution
summary(diamonds$color)
barplot(summary(diamonds$color), main="Diamond Color")

prop.table(table(diamonds$color))
```

### Clarity variable

Clarity variable shows no missing values. It is clearly unbalanced. The diamond clarity distribution is more frequent for the SI1 and VS2 grades, whereas I1 grade only represents 1.4% of the sample. Hence, the sample is composed mostly of slightly-to-very slightly included stones. The strange categories could be grouped further depending on the results of the multivariate analysis.

```{r}
# Check NA 
cat("Missing values in clarity:", sum(is.na(diamonds$clarity)), "\n") #0

# Distribution
summary(diamonds$clarity)
barplot(summary(diamonds$clarity), main="Diamond Clarity")

prop.table(table(diamonds$clarity))
```

In conclusion, we have removed 3 observations and transformed the variables carat and price with a logarithmic transformation for a better interpretation. Hence, the dataframe we will work with is:

```{r}
diamonds <- diamonds[,c(2:6,8:12)]
```

### VARAIBLE CREATION????

## Correlations between numeric variables

The numeric variables are generally highly correlated. The x, y, z, logPrice, and logCarat variables exhibit nearly perfect positive correlations, and their scatter plots form almost straight lines. In fact, the logarithmic transformations of price and carat further increase these correlations.

In contrast, the depth and table variables show weak correlations with the other variables, with depth being the only variable negatively correlated with all the others. Their scatter plots display a cloud of points without a clear pattern, although table and depth appear slightly negatively correlated.

As expected, the Bartlett test returns a very low p-value, rejecting the null hypothesis that the correlation matrix is equal to the identity matrix.

```{r}
numeric_vars <- diamonds[ , sapply(diamonds, is.numeric)]

cor_matrix <- cor(numeric_vars)

corrplot(cor_matrix, method = "color", addCoef.col = "black", title = "Correlation Plot of Numeric Variables")

pairs(numeric_vars, main="Scatterplot Matrix of Numeric Variables")

cortest.bartlett(cor_matrix, nrow(diamonds))
```
Taking into account the KMO index, since it is higher than 0.5, the data are factorable. In other words, the variables are sufficiently correlated with each other so that meaningful factors can be extracted.

```{r}
# (function written by Prof. Shigenobu Aok.)

kmo <- function(x)
{
  x <- subset(x, complete.cases(x))       # Omit missing values
  r <- cor(x)                             # Correlation matrix
  r2 <- r^2                               # Squared correlation coefficients
  i <- solve(r)                           # Inverse matrix of correlation matrix
  d <- diag(i)                            # Diagonal elements of inverse matrix
  p2 <- (-i/sqrt(outer(d, d)))^2          # Squared partial correlation coefficients
  diag(r2) <- diag(p2) <- 0               # Delete diagonal elements
  KMO <- sum(r2)/(sum(r2)+sum(p2))
  MSA <- colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

# KMO index
kmo(numeric_vars)
```

# Part 2: Multivariant analysis

## Principal components analysis
The PCA analysis has been applied using the numeric variables of the diamonds dataset with the \emph{PCA()} function from the \emph{FactoMineR} library.

In order to determine the number of components to extract, taking into account that the data is standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, the higher the eigenvalue, the greater the proportion of total variability accounted by that dimension. In this case, the first two components (with eigenvalues $4.99$ and $1.2$) are the ones that meet the Kaiser's requirement. Moreover, the cumulative percentage of variance shows that these two components explain $89.46\%$ of the total variance, which indicates that they capture most of the information in the data frame. In addition, the scree plot shows a noticeable elbow after the first component, indicating that retaining two components captures most of the variance while keeping the solution interpretable.

```{r}
diamonds.pca <- PCA(numeric_vars, graph = T)

barplot(diamonds.pca$eig[,1], names.arg = 1:7, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca$eig[,1], type = "o", col = "red", pch = 19, add=T)

diamonds.pca$eig
```

It is important to determine which variables contribute most to each of the relevant components in order to reduce the number of variables. Hence, considering the coordinates of the variables, it is possible to see which variable is associated with each component. On the other hand, examining the $cos^2$ values shows which variables are more strongly related to a given component.

- Dimension $1$: Physical Magnitude and Price Component
Dimension $1$ shows strong positive loadings for variables related to the diamond’s physical magnitude, such as x, y, z, and logCarat, as well as for logPrice. This indicates a clear relationship between a diamond’s overall material magnitude and its price. As expected, larger and heavier diamonds tend to be more expensive. Hence, this axis can be interpreted as an overall physical magnitude-price direction. Diamonds with high coordinates on dimension $1$ are generally larger, heavier, and more valuable, while diamonds with low coordinates tend to be smaller and less expensive.

- Dimension $2$: Geometric Balance
Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Considering that depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. In other words, dimension $2$ is positively associated with taller, more pointed diamonds with a narrower top surface relative to their width, and negatively associated with shallow or flattened diamonds.
```{r}
corrplot(diamonds.pca$var$coord)
diamonds.pca$var$cos2
diamonds.pca$var$contrib

diamonds.pca$var$coord

plot.PCA(diamonds.pca, choix = "var", axes = c(1, 2))
```

## PCA SUPLEMENTARY??
The PCA analysis has been applied using the numeric variables of the diamonds dataset with the \emph{PCA()} function from the \emph{FactoMineR} library.

The logPrice numeric variable has been considered as a supplementary variable, since it is not a descriptive variable but a result of characteristics given by the numeric and categorical variables.

Including price as an active variable could distort the PCA structure, given its high variance and strong correlations with the physical dimensions of the diamond, and it would make the interpretation of the components less meaningful.

By treating price as a supplementary variable, it is excluded from the calculation of the principal components and only projected onto the factorial space afterwards. This makes it possible to study how price aligns with the main axes without altering their structure. This approach results in a clearer interpretation of the extracted components, focused on the physical properties of the diamonds, while still indicating how these characteristics relate to the final price.

In order to determine the number of components to extract, taking into account that the data is standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, the higher the eigenvalue, the greater the proportion of total variability accounted by that dimension. In this case, the first two components (with eigenvalues $4.06$ and $1.26$) are the ones that meet the Kaiser's requirement. Moreover, the cumulative percentage of variance shows that these two components explain $88.65\%$ of the total variance, which indicates that they capture most of the information in the data frame. In addition, the scree plot shows a noticeable elbow after the first component, indicating that retaining two components captures most of the variance while keeping the solution interpretable.

```{r}
diamonds.pca.sup <- PCA(numeric_vars, quanti.sup = 7, graph = T)

barplot(diamonds.pca.sup$eig[,1], names.arg = 1:6, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca.sup$eig[,1], type = "o", col = "red", pch = 19, add=T)

diamonds.pca.sup$eig
```

It is important to determine which variables contribute most to each of the relevant components in order to reduce the number of variables. Hence, considering the coordinates of the variables, it is possible to see which variable is associated with each component. On the other hand, examining the $cos^2$ values shows which variables are more strongly related to a given component.

- Dimension $1$: Physical Magnitude (and Price Component)
Dimension $1$ shows strong positive loadings for variables related to the diamond’s physical magnitude, such as x, y, z, and logCarat. As can be seen by the plot, the supplementary variable logPrice is very correlated with dimension 1. This indicates a clear relationship between a diamond’s overall material magnitude and its price. As expected, larger and heavier diamonds tend to be more expensive. Hence, this axis can be interpreted as an overall physical magnitude-price direction. Diamonds with high coordinates on dimension $1$ are generally larger, heavier, and more valuable, while diamonds with low coordinates tend to be smaller and less expensive.

- Dimension $2$: Geometric Balance
Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Considering that depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. In other words, dimension $2$ is positively associated with taller, more pointed diamonds with a narrower top surface relative to their width, and negatively associated with shallow or flattened diamonds.
```{r}
corrplot(diamonds.pca.sup$var$coord)
diamonds.pca.sup$var$cos2
diamonds.pca.sup$var$contrib

diamonds.pca.sup$var$coord

plot.PCA(diamonds.pca.sup, choix = "var", axes = c(1, 2))
```

## Multidimensional scaling

## Correspondance analysis ??????

## Multiple correspondance analysis

In order to do Multiple Corresponance Analysis using also the numerical variables, these will be converted to binary variables with two categories as “over average” and “below average” for each of them, which will also help with its interpretability. 

Factor level 'over average' will be defined as 'o', and 'below average' as 'b'.

```{r}
dfmca <- diamonds
nums = names(numeric_vars)

for (var in nums){
  meanvar = mean(dfmca[[var]])
  dfmca[[var]] <- ifelse(dfmca[[var]] >= meanvar, "o", "b") 
  dfmca[[var]] <- as.factor(dfmca[[var]])
}
summary(dfmca)
```
The analysis will be done using Indicator Matrix. That way we will be able to study both the categories of the variables and the individuals.

Regarding the logPrice variable, the approach followed will be the same as in PCA: it will be included as a supplementary variable, to help further the interpretation without being included in the calculations.

```{r}
## Indicator Matrix
library(FactoMineR)
tab.disjonctif(dfmca)

## MCA Application on Data using Indicator Matrix
# logPrice used as a supplementary variable
res.mca <- MCA(dfmca,quali.sup=10)
summary(res.mca)
```

In order to choose which dimensions we will work with, we apply the rule of choosing the dimensions with eigenvalues greater than the average inertia of the indicator matrix per dimension.

In this case, as there are 10 variables, J = 10, and the total number of values is K=5+7+8+(7x2)=34, number of dimensions is K-J=24. Therefore, the average inertia is 1/24 (0.04166667), and the dimensions selected are until Dim.20????

The summary shows all of the analysis, but the description will be done separately.
```{r}
# K=15x2=30, J=15
# no of dimension: K-J=15
# average inertia per dimension: 1/(K-J)=1/15

# Choose the dimensions with eigenvalues > 1/15 --> Until Dim.4
summary(res.mca, ncp=4)
res.mca
```

As can be seen in the description of Dimensions, the most significant variables in each dimension, which are those represented with the highest value of R2, are:
  - For *dimension 1*, the Efficiency `EFF`, as it's value of R2 is 0.8414.
  - For *dimension 2*, it is number of 2pt Field Goal `2PT FG`, with a value of R2 of 0.6864.
  - For *dimension 3*, there isn't such a significant variable, but the one with the highest R2 value is 3pt Field Goal `3PT FG`, with a value of R2 of 0.3615.
  
Also, it is relevant to observe the link between variable and categories to see which are the most significant categories in each dimension, which are those represented with the highest absolute value of Estimate.

They follow the same pattern for all three dimensions: the categories most positively correlated are the most significant variables mentioned above, for the 'over average' category, and the most negatively correlated is the 'under average' value of the same variable. 

This can also be corroborated with the coordinate analysis. They are the values that are more extreme, in the negative and positive side of the axis.

That means that observations will be easily separated between those over average and under, and interpreted according to each dimension.

* Note: The results for dimension 4 are not shown by R, as it has an eigenvalue that is technically lower than the threshold (0.066 < 0.067).


```{r}
### Eigenvalues
res.mca$eig
barplot(res.mca$eig[,2],main="Eigenvalues", names.arg=1:nrow(res.mca$eig))

## Coordinates of categories
res.mca$var$coord

## Description of Dimensions
dimdesc(res.mca)
```
For the plots, we will interpret them in the 1st and 2nd dimension, since they are the most significative.

Firstly, we will interpret the *cloud of individuals*. The plot shows both the individuals and the categories represented by the 1st and 2nd dimension.

  - It is worth noting that there are almost no points in the center of the graph, they are all placed away from the origin.
  
  - For the categories, this pattern is due to the transformation of the numerical variables into binary ones, dividing them into “over average” and “below average” categories. Since both categories have similar frequencies, none of them represents a “dominant” or “average” profile. Therefore, both are located in opposite directions.
  
  - For the individuals, since each player belongs to either the “over” or “under” group for each variable, the overall profile of individuals is more polarized. 
  
  - As a conclusion, binarizing continuous variables helps to highlight contrasts between players’ performances, but at the same time reduces the amount of information in the variables.
```{r}
### Clouds ###
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none")
```

Then, we will interpret the *cloud of individuals grouped by most significant categories per dimension*. This is done by selecting the variable in the 'habillage' parameter.

  - For *dimension 1*, the Efficiency `EFF`.
  The map clearly shows this distinction: in the negative side of the Dim 1 axis, we can see the individuals with an efficiency below average, and in the positive side, those over average.
  
  - For *dimension 2*, it is number of 2pt Field Goal `2PT FG`
  Although there are a few exceptions, the map also shows this distinction: in the negative side of the Dim 2 axis, we can see the individuals with a value of `2PT FG` below average, and in the positive side, those over average.

```{r}
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "logCarat")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "cut")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "table")
```

The graph of the active categories is very relevant: the categories of values 'over average' are represented on the positive side of Dim 1, and the ones 'below average' are on the negative side.

Considering the total information obtained from the cloud of categories, and also the cloud of variables, the conclusions we draw of the information obtained by the dimensions is the following:

  - *Dimension 1* mainly represents the overall performance level, separating players with “over average” results  on the positive side, from those “below average” on the negative side. 
  
  - *Dimension 2* is more related to the shooting style of the players. Players with above-average values in 2-point field goals (2PT FG) are positioned on the upper side of the axis, together with offensive rebounds (OREB), which makes sense since players who frequently capture offensive rebounds often score from close range, leading to more 2-point shots.
These players also tend to have below-average 3-point field goals (3PT FG), which fits with the usual profile of inside players.
Conversely, the lower part of the axis shows the opposite pattern — players with higher 3-point shooting and fewer offensive rebounds, typically corresponding to perimeter players.

```{r}
## Cloud of Categories
plot(res.mca,invisible=c("ind"),title="Graph of the active categories")
plot(res.mca,invisible=c("ind"),col.var=rep(c("black","red"),15),title="Graph of the active categories")

## Cloud of variables
plot(res.mca,choix="var",title="Cloud of variables")
```

Additional analysis: Using `position` as a supplementary variable.

This is done in order to check if this variable gives any extra information to the analysis.

As we can see in the analysis, this supplementary variable is mainly focused on the second dimension.
Center and forward are on the positive side, while guard is on the negative. The behaviour of these positions corresponds to the interpretation of dimension 2 done previously: players who score more on 2 PT shots and with more offensive rebounds are Center and Forward, and those with more 3 PT shots are the Guards.

```{r}

# Position used as a supplementary variable
res.mca <- MCA(dfmca,quali.sup=1)

# Result summaries of MCA
res.mca$quali.sup

# Graphs of the variables
plot(res.mca,choix="var",title="Graph of the variables")
plot(res.mca,invisible=c("ind","var"),hab="quali", title="Graph of the supplementary categories")
```


## Clustering and profiling

### Cluster analysis


```{r}
res <- FAMD(diamonds)
summary(res)
```

```{r}
hc_famd <- HCPC(res,-1) 
```
```{r}
# Dendrogram with optimal number of clusters
library(factoextra)
fviz_dend(hc_famd, rect = TRUE, rect_fill = TRUE)

# Visualization of Clusters
fviz_cluster(hc_famd, repel = TRUE, show.clust.cent = TRUE, main = "Factor map")
```



### Profiling

```{r}
hc_famd$desc.var
```

