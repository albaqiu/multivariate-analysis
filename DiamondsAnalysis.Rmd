---
title: "Diamonds Data Frame Analysis"
author: "Berta Torrents"
date: "December 2025"
output:
  word_document: default
  pdf_document:
    latex_engine: xelatex
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  results = 'markup',
  message = FALSE,
  warning = FALSE,
  fig.width = 7,
  fig.height = 5
)
```

```{r}
set.seed(12345)

# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Libraries
library(dplyr)
library(FactoMineR)
library(MASS)
library(psych)
library(factoextra)
library(corrplot)
```

## 1. Data preparation

The dataset have been imported using the \emph{read.csv()} function from the \emph{dplyr} library. The variable named X have been removed, as it corresponds to an index column rather than an explanatory variable. Additionally, the categorical variables cut, color, and clarity have been correctly converted into factors.

```{r}
diamonds <- read.csv('diamonds.csv')

diamonds <- diamonds[,-1]

diamonds$cut <- as.factor(diamonds$cut)
diamonds$color <- as.factor(diamonds$color)
diamonds$clarity <- as.factor(diamonds$clarity)

summary(diamonds)
```

Since the dataset contained a large number of observations $(53940)$, a random sample of $500$ observations has been selected to allow for a more manageable and focused analysis. It is worth noting that the means of the numerical variables and the proportions of observations in each category do not vary significantly.

```{r}
n <- nrow(diamonds)
indices <- sample(1:n, 500) # Take 500 random indices
diamonds <- diamonds[indices,]

summary(diamonds)
```

## Exploratory data analysis

For each variable, we check: duplicates, existence of zeros, existence of outliers, existence of missing values and we apply transformations if it is needed.

## Numerical variables

Helper function to evaluate missing values and zeros

```{r}
check_numeric <- function(x) {
  list(na = sum(is.na(x)),
  zeros = sum(x == 0))
}
```

```{r}
num_vars <- diamonds %>% select_if(is.numeric)
results <- lapply(num_vars, check_numeric)
results
```
We observe that no numeric variable has NA or zeros

### Carat variable

After analyzing the carat variable, we observe that it doesn't have severe outliers. The carat variable is right-skewed, with a long tail towards larger values, so is clearly not normally distributed. After using boxcox function, we see that a log transformation is appropriate to improve symmetry and stabilize variance.

```{r}
# Outliers
boxplot(diamonds$carat, horizontal = T)
varout <- summary(diamonds$carat)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr

boxplot(diamonds$carat, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out <- which((diamonds$carat >= usout) | (diamonds$carat <= lsout))
sev_out # 0

# Distribution
hist(diamonds$carat,freq=F)
curve(dnorm(x),add=T, col="red") #It is obviously not normally distributed

# Transformation
boxcox(diamonds$carat~1,lambda=seq(-1,1,by=0.1)) # Box-Cox Transformation
diamonds$logCarat <- log(diamonds$carat) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logCarat, horizontal = T) # No outliers

hist(diamonds$logCarat, freq = F) # Is logCarat normally distributed?
m = mean(diamonds$logCarat)
std = sd(diamonds$logCarat)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)
```

### Depth variable

The depth variable has 2 severe outliers, which we removed. The histogram of the variable shows that the depth variable has an approximately symmetric, bell-shaped distribution, so no transformation is needed. Nonetheless, the normality test shapiro shows a low p-value, rejecting the normality distribution hypothesis of the variable.

```{r}
# Outliers
boxplot(diamonds$depth, horizontal = T)
varout <- summary(diamonds$depth)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$depth >= usout) | (diamonds$depth <= lsout))

boxplot(diamonds$depth, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #[1] 462 467
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Distribution
hist(diamonds$depth,freq=F)
m = mean(diamonds$depth)
std = sd(diamonds$depth)
curve(dnorm(x,m,std),add=T, col="red")

shapiro.test(diamonds$depth)
```


### Table variable

The variable table has severe 1 outlier. After removing it, we see that the distribution of this variable is almost symmetric with mild right skewness. Since its shape is already close to normal and a transformation would not provide meaningful improvement, no transformation is applied. However, the shapiro test returned a low p-value, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$table, horizontal = TRUE)
varout <- summary(diamonds$table)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$table >= usout) | (diamonds$table <= lsout))

boxplot(diamonds$table, horizontal = TRUE)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #206
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Distribution
hist(diamonds$table,freq=F)
m = mean(diamonds$table)
std = sd(diamonds$table)
curve(dnorm(x,m,std),add=T, col="red")

shapiro.test(diamonds$table)
```

### Price variable

The variable price doesn't have severe outliers, but we can observe a long tail at the right of the boxplot, indicating potential mild outliers.

The histogram shows a right skewed distribution, which can be improved by a transformation to reduce its skewness and stabilize variance. Based on the boxcox, it seems like a logarithmic transformation would be suitable.

After applying the log transformation, the distribution becomes more symmetric and closer to a bell-shaped form, without being still normal distributed.

```{r}
# Outliers
boxplot(diamonds$price, horizontal = T)
varout <- summary(diamonds$price)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$price > usout) | (diamonds$price < lsout))

boxplot(diamonds$price, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$price, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$price)
std = sd(diamonds$price)
curve(dnorm(x,m,std),add=T, col="red")

# Logarithmic transformation
boxcox(diamonds$price~1,lambda=seq(-1,1,by=0.1)) # Box-Cox Transformation
diamonds$logPrice <- log(diamonds$price) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logPrice, horizontal = T) # No outliers
hist(diamonds$logPrice, freq = F) # Is logPrice normally distributed?
m = mean(diamonds$logPrice)
std = sd(diamonds$logPrice)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)
```

### x variable
The x variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$x, horizontal = T)
varout <- summary(diamonds$x)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$x > usout) | (diamonds$x < lsout))

boxplot(diamonds$x, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$x, freq = F)
m = mean(diamonds$x)
std = sd(diamonds$x)
curve(dnorm(x,m,std),add=T, col="red")
```

### y variable
The y variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$y, horizontal = T)
varout <- summary(diamonds$y)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$y > usout) | (diamonds$y < lsout))

boxplot(diamonds$y, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$y, freq = F)
m = mean(diamonds$y)
std = sd(diamonds$y)
curve(dnorm(x,m,std),add=T, col="red")
```

### z variable
The y variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$z, horizontal = T)
varout <- summary(diamonds$z)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$z > usout) | (diamonds$z < lsout))

boxplot(diamonds$z, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$z, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$z)
std = sd(diamonds$z)
curve(dnorm(x,m,std),add=T, col="red")
```

## Categorical variables

### Cut variable 

Cut variable shows no missing values. It is clearly unbalanced, with the Ideal level accounting for 37.4% of observations, while the Fair level comprising only 1.8% of the observations. These strage categories could be grouped further depending on the results of the multivariate analysis.

```{r}
# Check NA (we don't need to check 0s)
cat("Missing values in cut:", sum(is.na(diamonds$cut)), "\n") #0

# Distribution
summary(diamonds$cut)
barplot(summary(diamonds$cut), main="Diamond Cut Quality")

prop.table(table(diamonds$cut))
```

### Color variable

Color variable shows no missing values. It is a more balanced variable. Nonetheless, the most frequent grade is G, representing 12.6% of observations, while the least frequent is J, with 3.4%, indicating that diamonds with a noticeable tint are less common in the dataset.

```{r}
# Check NA
cat("Missing values in color:", sum(is.na(diamonds$color)), "\n") #0

# Distribution
summary(diamonds$color)
barplot(summary(diamonds$color), main="Diamond Color")

prop.table(table(diamonds$color))
```

### Clarity variable

Clarity variable shows no missing values. It is clearly unbalanced. The diamond clarity distribution is more frequent for the SI1 and VS2 grades, whereas I1 grade only represents 1.4% of the sample. Hence, the sample is composed mostly of slightly-to-very slightly included stones. The strange categories could be grouped further depending on the results of the multivariate analysis.

```{r}
# Check NA 
cat("Missing values in clarity:", sum(is.na(diamonds$clarity)), "\n") #0

# Distribution
summary(diamonds$clarity)
barplot(summary(diamonds$clarity), main="Diamond Clarity")

prop.table(table(diamonds$clarity))
```

In conclusion, we have removed 3 observations and transformed the variables carat and price with a logarithmic transformation for a better interpretation. Hence, the dataframe we will work with is:

```{r}
diamonds <- diamonds[,c(2:6,8:12)]
```

### VARAIBLE CREATION????

## Correlations between numeric variables

The numeric variables are generally highly correlated. The x, y, z, logPrice, and logCarat variables exhibit nearly perfect positive correlations, and their scatter plots form almost straight lines. In fact, the logarithmic transformations of price and carat further increase these correlations.

In contrast, the depth and table variables show weak correlations with the other variables, with depth being the only variable negatively correlated with all the others. Their scatter plots display a cloud of points without a clear pattern, although table and depth appear slightly negatively correlated.

As expected, the Bartlett test returns a very low p-value, rejecting the null hypothesis that the correlation matrix is equal to the identity matrix.

```{r}
numeric_vars <- diamonds[ , sapply(diamonds, is.numeric)]

cor_matrix <- cor(numeric_vars)

corrplot(cor_matrix, method = "color", addCoef.col = "black", title = "Correlation Plot of Numeric Variables")

pairs(numeric_vars, main="Scatterplot Matrix of Numeric Variables")

cortest.bartlett(cor_matrix, nrow(diamonds))
```
Taking into account the KMO index, since it is higher than 0.5, the data are factorable. In other words, the variables are sufficiently correlated with each other so that meaningful factors can be extracted.

```{r}
# (function written by Prof. Shigenobu Aok.)

kmo <- function(x)
{
  x <- subset(x, complete.cases(x))       # Omit missing values
  r <- cor(x)                             # Correlation matrix
  r2 <- r^2                               # Squared correlation coefficients
  i <- solve(r)                           # Inverse matrix of correlation matrix
  d <- diag(i)                            # Diagonal elements of inverse matrix
  p2 <- (-i/sqrt(outer(d, d)))^2          # Squared partial correlation coefficients
  diag(r2) <- diag(p2) <- 0               # Delete diagonal elements
  KMO <- sum(r2)/(sum(r2)+sum(p2))
  MSA <- colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

# KMO index
kmo(numeric_vars)
```

# Part 2: Multivariant analysis

## Principal components analysis
The PCA analysis has been applied using the numeric variables of the diamonds dataset with the \emph{PCA()} function from the \emph{FactoMineR} library.

In order to determine the number of components to extract, taking into account that the data is standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, the higher the eigenvalue, the greater the proportion of total variability accounted by that dimension. In this case, the first two components (with eigenvalues $4.99$ and $1.2$) are the ones that meet the Kaiser's requirement. Moreover, the cumulative percentage of variance shows that these two components explain $89.46\%$ of the total variance, which indicates that they capture most of the information in the data frame. In addition, the scree plot shows a noticeable elbow after the first component, indicating that retaining two components captures most of the variance while keeping the solution interpretable.

```{r}
diamonds.pca <- PCA(numeric_vars, graph = T)

barplot(diamonds.pca$eig[,1], names.arg = 1:7, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca$eig[,1], type = "o", col = "red", pch = 19, add=T)
```

It is important to determine which variables contribute most to each of the relevant components in order to reduce the number of variables. Hence, considering the coordinates of the variables, it is possible to see which variable is associated with each component. On the other hand, examining the $cos^2$ values shows which variables are more strongly related to a given component.

- Dimension $1$: Physical Magnitude and Price Component
Dimension $1$ shows strong positive loadings for variables related to the diamond’s physical magnitude, such as x, y, z, and logCarat, as well as for logPrice. This indicates a clear relationship between a diamond’s overall material magnitude and its price. As expected, larger and heavier diamonds tend to be more expensive. Hence, this axis can be interpreted as an overall physical magnitude-price direction. Diamonds with high coordinates on dimension $1$ are generally larger, heavier, and more valuable, while diamonds with low coordinates tend to be smaller and less expensive.

- Dimension $2$: Geometric Balance
Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Considering that depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. In other words, dimension $2$ is positively associated with taller, more pointed diamonds with a narrower top surface relative to their width, and negatively associated with shallow or flattened diamonds.
```{r}
corrplot(diamonds.pca$var$coord)
diamonds.pca$var$cos2
diamonds.pca$var$contrib

diamonds.pca$var$coord

plot.PCA(diamonds.pca, choix = "var", axes = c(1, 2))
```

## PCA SUPLEMENTARY??

## Multidimensional scaling

## Correspondance analysis ??????

## Multiple correspondance analysis

## Cluster analysis

### Profiling
