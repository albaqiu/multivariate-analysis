---
title: "Diamonds Data Frame Analysis"
author: "Berta Torrents"
date: "December 2025"
output:
  word_document: default
  pdf_document:
    latex_engine: xelatex
geometry: margin=1in
---

-- MIRAR NO OUTLIER PERO SI QUE NO ESTA MASSA BEN POSADA OBSERVATION EN LA LOGPRICE, X,Y,Z I MIRAR SI ALESHORES PODRIEM APLICAR UN LOG

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  results = 'markup',
  message = FALSE,
  warning = FALSE,
  fig.width = 7,
  fig.height = 5
)
```

```{r}
set.seed(12345)

# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Libraries
library(dplyr)
library(FactoMineR)
library(MASS)
library(psych)
library(factoextra)
library(corrplot)
```

## 1. Data preparation

The dataset have been imported using the \emph{read.csv()} function from the \emph{dplyr} library. The variable named X have been removed, as it corresponds to an index column rather than an explanatory variable. Additionally, the categorical variables cut, color, and clarity have been correctly converted into factors.

```{r}
diamonds <- read.csv('diamonds.csv')

diamonds <- diamonds[,-1]

diamonds$cut <- as.factor(diamonds$cut)
diamonds$color <- as.factor(diamonds$color)
diamonds$clarity <- as.factor(diamonds$clarity)

summary(diamonds)
```

Since the dataset contained a large number of observations $(53940)$, a random sample of $500$ observations has been selected to allow for a more manageable and focused analysis. It is worth noting that the means of the numerical variables and the proportions of observations in each category do not vary significantly.

```{r}
n <- nrow(diamonds)
indices <- sample(1:n, 500) # Take 500 random indices
diamonds <- diamonds[indices,]
# rownames(diamonds) <- seq_len(nrow(diamonds))

summary(diamonds)
```

## Exploratory data analysis

For each variable, we check: duplicates, existence of zeros, existence of outliers, existence of missing values and we apply transformations if it is needed.

## Numerical variables

Helper function to evaluate missing values and zeros

```{r}
check_numeric <- function(x) {
  list(na = sum(is.na(x)),
  zeros = sum(x == 0))
}
```

```{r}
num_vars <- diamonds %>% select_if(is.numeric)
results <- lapply(num_vars, check_numeric)
results
```

We observe that no numeric variable has NA or zeros.

### Carat variable

After analyzing the carat variable, we observe that it doesn't have severe outliers. The carat variable is right-skewed, with a long tail towards larger values, so is clearly not normally distributed. After using boxcox function, we see that a log transformation is appropriate to improve symmetry and stabilize variance.

Notice that the log transformation does not follow a normal distribution neither. The histogram is clearly not bell-shaped and the QQ-plot has a lot of deviations. As it is expected both the shapiro and kolmogorov test have rejected the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$carat, horizontal = T)
varout <- summary(diamonds$carat)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr

boxplot(diamonds$carat, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out <- which((diamonds$carat >= usout) | (diamonds$carat <= lsout))
sev_out # 0

# Distribution
hist(diamonds$carat,freq=F)
curve(dnorm(x),add=T, col="red") #It is obviously not normally distributed

# Transformation
boxcox(diamonds$carat~1,lambda=seq(-1,1,by=0.1)) # Box-Cox Transformation
diamonds$logCarat <- log(diamonds$carat) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logCarat, horizontal = T) # No outliers

# Is logCarat normally distributed?
hist(diamonds$logCarat, freq = F)
m = mean(diamonds$logCarat)
std = sd(diamonds$logCarat)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)

qqnorm(diamonds$logCarat, main = "logCarat Q-plot")
qqline(diamonds$logCarat)

shapiro.test(diamonds$logCarat)
ks.test(diamonds$logCarat, "pnorm", mean=mean(diamonds$logCarat), sd=sd(diamonds$logCarat))
```

### Depth variable

The depth variable has 2 severe outliers, which we removed. The histogram of the variable shows that the depth variable. Moreover, the QQ-plot almost shows a straight line without major deviations, which suggests that this variable could be considered approximately normal. Nonetheless, the shapiro and the kolmogorov test show a low p-value, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$depth, horizontal = T)
varout <- summary(diamonds$depth)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$depth >= usout) | (diamonds$depth <= lsout))

boxplot(diamonds$depth, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #[1] 462 467
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Is depth normally distributed?
hist(diamonds$depth,freq=F)
m = mean(diamonds$depth)
std = sd(diamonds$depth)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$depth, main = "depth Q-plot")
qqline(diamonds$depth)

shapiro.test(diamonds$depth)
ks.test(diamonds$depth, "pnorm", mean=mean(diamonds$depth), sd=sd(diamonds$depth))
```

### Table variable

The variable table contains one severe outlier. After removing it, the distribution becomes almost symmetric with a mild right skewness. Since its shape is already close to normal and a transformation would not provide meaningful improvement, no transformation is applied. The QQ-plot also shows points lying close to a straight line, without major deviations, which further suggests approximate normality. Although the variable is discrete, its wide range of values and its bell-shaped histogram support the assumption that normality may reasonably hold. However, the shapiro kolmogorov test show a low p-value, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$table, horizontal = TRUE)
varout <- summary(diamonds$table)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$table >= usout) | (diamonds$table <= lsout))

boxplot(diamonds$table, horizontal = TRUE)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #206
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Is table normally distributed?
hist(diamonds$table,freq=F)
m = mean(diamonds$table)
std = sd(diamonds$table)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$table, main = "table Q-plot")
qqline(diamonds$table)

shapiro.test(diamonds$table)
ks.test(diamonds$table, "pnorm", mean=mean(diamonds$table), sd=sd(diamonds$table))
```

### Price variable

The variable price doesn't have severe outliers, but we can observe a long tail at the right of the boxplot, indicating potential mild outliers. The histogram shows a right skewed distribution, which can be improved by a transformation to reduce its skewness and stabilize variance. Based on the boxcox, it seems like a logarithmic transformation would be suitable. After applying the log transformation, the distribution becomes more symmetric and closer to a bell-shaped form, without being still normal distributed. The QQ-plot suggests that the logarithmic variable is still not normally distributed since many deviations are shown. Morover, the shapiro kolmogorov test show a low p-value, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$price, horizontal = T)
varout <- summary(diamonds$price)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$price > usout) | (diamonds$price < lsout))

boxplot(diamonds$price, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$price, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$price)
std = sd(diamonds$price)
curve(dnorm(x,m,std),add=T, col="red")

boxcox(diamonds$price~1,lambda=seq(-1,1,by=0.1)) # Box-Cox Transformation
diamonds$logPrice <- log(diamonds$price) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logPrice, horizontal = T) # No outliers

# Is logPrice normally distributed?
hist(diamonds$logPrice, freq = F)
m = mean(diamonds$logPrice)
std = sd(diamonds$logPrice)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)

qqnorm(diamonds$logPrice, main = "logPrice Q-plot")
qqline(diamonds$logPrice)

shapiro.test(diamonds$logPrice)
ks.test(diamonds$logPrice, "pnorm", mean=mean(diamonds$logPrice), sd=sd(diamonds$logPrice))
```

### x variable

The x variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$x, horizontal = T)
varout <- summary(diamonds$x)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$x > usout) | (diamonds$x < lsout))

boxplot(diamonds$x, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$x, freq = F)
m = mean(diamonds$x)
std = sd(diamonds$x)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$x, main = "x Q-plot")
qqline(diamonds$x)

shapiro.test(diamonds$x)
ks.test(diamonds$x, "pnorm", mean=mean(diamonds$x), sd=sd(diamonds$x))
```

### y variable

The y variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.

```{r}
# Outliers
boxplot(diamonds$y, horizontal = T)
varout <- summary(diamonds$y)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$y > usout) | (diamonds$y < lsout))

boxplot(diamonds$y, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$y, freq = F)
m = mean(diamonds$y)
std = sd(diamonds$y)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$y, main = "y Q-plot")
qqline(diamonds$y)

shapiro.test(diamonds$y)
ks.test(diamonds$y, "pnorm", mean=mean(diamonds$y), sd=sd(diamonds$y))
```

### z variable

The y variables does not show any mild nor severe outliers. It's histogram clearly reveals that is not normally distributed.


violations of normality at the tails!!!!

```{r}
# Outliers
boxplot(diamonds$z, horizontal = T)
varout <- summary(diamonds$z)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
sev_out <- which((diamonds$z > usout) | (diamonds$z < lsout))

boxplot(diamonds$z, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
sev_out #0

# Distribution
hist(diamonds$z, freq = F)
m = mean(diamonds$z)
std = sd(diamonds$z)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$z, main = "z Q-plot")
qqline(diamonds$z)

shapiro.test(diamonds$z)
ks.test(diamonds$z, "pnorm", mean=mean(diamonds$z), sd=sd(diamonds$z))
```

## Categorical variables

### Cut variable

Cut variable shows no missing values. It is clearly unbalanced, with the Ideal level accounting for 37.4% of observations, while the Fair level comprising only 1.8% of the observations. These strange categories could be grouped further depending on the results of the multivariate analysis.

```{r}
# Check NA (we don't need to check 0s)
cat("Missing values in cut:", sum(is.na(diamonds$cut)), "\n") #0

# Distribution
summary(diamonds$cut)
barplot(summary(diamonds$cut), main="Diamond Cut Quality")

prop.table(table(diamonds$cut))

# We can group Fair and Good in one group because Fair has little observations and the variable is unbalanced

#diamonds$cut <- ifelse(diamonds$cut %in% c("Fair", "Good"), "Fair/Good", as.character(diamonds$cut))

#diamonds$cut <- factor(
#  diamonds$cut,
#  levels = c("Fair/Good", "Ideal", "Premium", "Very Good"))

#barplot(table(diamonds$cut), main="Diamond Cut Quality")
```

### Color variable

Color variable shows no missing values. It is a more balanced variable. Nonetheless, the most frequent grade is G, representing 12.6% of observations, while the least frequent is J, with 3.4%, indicating that diamonds with a noticeable tint are less common in the dataset.

```{r}
# Check NA
cat("Missing values in color:", sum(is.na(diamonds$color)), "\n") #0

# Distribution
summary(diamonds$color)
barplot(summary(diamonds$color), main="Diamond Color")

prop.table(table(diamonds$color))

# We can group I and J in one group because J has little observations and the variable is unbalanced

# diamonds$color <- ifelse(diamonds$color %in% c("I", "J"), "I/J", as.character(diamonds$color))
# summary(diamonds$color)
# 
# diamonds$color <- factor(
#   diamonds$color,
#   levels = c("D", "E", "F", "G","H","I/J")
# )
# 
# barplot(table(diamonds$color), main="Diamond Color")
```

### Clarity variable

Clarity variable shows no missing values. It is clearly unbalanced. The diamond clarity distribution is more frequent for the SI1 and VS2 grades, whereas I1 grade only represents 1.4% of the sample. Hence, the sample is composed mostly of slightly-to-very slightly included stones. The strange categories could be grouped further depending on the results of the multivariate analysis.

```{r}
# Check NA 
cat("Missing values in clarity:", sum(is.na(diamonds$clarity)), "\n") #0

# Distribution
summary(diamonds$clarity)
barplot(summary(diamonds$clarity), main="Diamond Clarity")

prop.table(table(diamonds$clarity))

# We can group I1 and IF in one group because I1 has little observations and the variable is unbalanced

# diamonds$clarity <- ifelse(diamonds$clarity %in% c("I1", "IF"), "I1/IF", as.character(diamonds$clarity))
# summary(diamonds$clarity)
# 
# diamonds$clarity <- factor(
#   diamonds$clarity,
#   levels = c("I1/IF", "SI1", "SI2", "VS1","VS2","VVS1","VVS2")
# )
# 
# barplot(table(diamonds$clarity), main="Diamond Clarity")
```

In conclusion, we have removed 3 observations and transformed the variables carat and price with a logarithmic transformation for a better interpretation. Hence, the dataframe we will work with is:

```{r}
diamonds <- diamonds[,c(2:6,8:12)]
```

### VARAIBLE CREATION????

## Correlations between numeric variables

The numeric variables are generally highly correlated. The x, y, z, logPrice, and logCarat variables exhibit nearly perfect positive correlations, and their scatter plots form almost straight lines. In fact, the logarithmic transformations of price and carat further increase these correlations.

In contrast, the depth and table variables show weak correlations with the other variables, with depth being the only variable negatively correlated with all the others. Their scatter plots display a cloud of points without a clear pattern, although table and depth appear slightly negatively correlated.

As expected, the Bartlett test returns a very low p-value, rejecting the null hypothesis that the correlation matrix is equal to the identity matrix.

```{r}
numeric_vars <- diamonds[ , sapply(diamonds, is.numeric)]

cor_matrix <- cor(numeric_vars)

corrplot(cor_matrix, method = "color", addCoef.col = "black", title = "Correlation Plot of Numeric Variables")

pairs(numeric_vars, main="Scatterplot Matrix of Numeric Variables")

cortest.bartlett(cor_matrix, nrow(diamonds))
```

Taking into account the KMO index, since it is higher than 0.5, the data are factorable. In other words, the variables are sufficiently correlated with each other so that meaningful factors can be extracted.

```{r}
# (function written by Prof. Shigenobu Aok.)

kmo <- function(x)
{
  x <- subset(x, complete.cases(x))       # Omit missing values
  r <- cor(x)                             # Correlation matrix
  r2 <- r^2                               # Squared correlation coefficients
  i <- solve(r)                           # Inverse matrix of correlation matrix
  d <- diag(i)                            # Diagonal elements of inverse matrix
  p2 <- (-i/sqrt(outer(d, d)))^2          # Squared partial correlation coefficients
  diag(r2) <- diag(p2) <- 0               # Delete diagonal elements
  KMO <- sum(r2)/(sum(r2)+sum(p2))
  MSA <- colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

# KMO index
kmo(numeric_vars)
```

# Part 2: Multivariant analysis

## 2.1. Principal components analysis

The PCA analysis has been applied using the numeric variables of the diamonds dataset with the \emph{PCA()} function from the \emph{FactoMineR} library.

In order to determine the number of components to extract, taking into account that the data is standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, the higher the eigenvalue, the greater the proportion of total variability accounted by that dimension. In this case, the first two components (with eigenvalues $4.99$ and $1.2$) are the ones that meet the Kaiser's requirement. Moreover, the cumulative percentage of variance shows that these two components explain $89.46\%$ of the total variance, which indicates that they capture most of the information in the data frame. In addition, the scree plot shows a noticeable elbow after the first component, indicating that retaining two components captures most of the variance while keeping the solution interpretable.

```{r}
diamonds.pca <- PCA(numeric_vars, graph = T)

barplot(diamonds.pca$eig[,1], names.arg = 1:7, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca$eig[,1], type = "o", col = "red", pch = 19, add=T)

diamonds.pca$eig
```

It is important to determine which variables contribute most to each of the relevant components in order to reduce the number of variables. Hence, considering the coordinates of the variables, it is possible to see which variable is associated with each component. On the other hand, examining the $cos^2$ values shows which variables are more strongly related to a given component.

-   Dimension $1$: Physical Magnitude and Price Component Dimension $1$ shows strong positive loadings for variables related to the diamond’s physical magnitude, such as x, y, z, and logCarat, as well as for logPrice. This indicates a clear relationship between a diamond’s overall material magnitude and its price. As expected, larger and heavier diamonds tend to be more expensive. Hence, this axis can be interpreted as an overall physical magnitude-price direction. Diamonds with high coordinates on dimension $1$ are generally larger, heavier, and more valuable, while diamonds with low coordinates tend to be smaller and less expensive.

-   Dimension $2$: Geometric Balance Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Considering that depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. In other words, dimension $2$ is positively associated with taller, more pointed diamonds with a narrower top surface relative to their width, and negatively associated with shallow or flattened diamonds.

```{r}
corrplot(diamonds.pca$var$coord)
diamonds.pca$var$cos2
diamonds.pca$var$contrib

diamonds.pca$var$coord

plot.PCA(diamonds.pca, choix = "var", axes = c(1, 2))
```

## PCA SUPLEMENTARY??

The PCA analysis has been applied using the numeric variables of the diamonds dataset with the \emph{PCA()} function from the \emph{FactoMineR} library.

The logPrice numeric variable has been considered as a supplementary variable, since it is not a descriptive variable but a result of characteristics given by the numeric and categorical variables.

Including price as an active variable could distort the PCA structure, given its high variance and strong correlations with the physical dimensions of the diamond, and it would make the interpretation of the components less meaningful.

By treating price as a supplementary variable, it is excluded from the calculation of the principal components and only projected onto the factorial space afterwards. This makes it possible to study how price aligns with the main axes without altering their structure. This approach results in a clearer interpretation of the extracted components, focused on the physical properties of the diamonds, while still indicating how these characteristics relate to the final price.

In order to determine the number of components to extract, taking into account that the data is standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, the higher the eigenvalue, the greater the proportion of total variability accounted by that dimension. In this case, the first two components (with eigenvalues $4.06$ and $1.26$) are the ones that meet the Kaiser's requirement. Moreover, the cumulative percentage of variance shows that these two components explain $88.65\%$ of the total variance, which indicates that they capture most of the information in the data frame. In addition, the scree plot shows a noticeable elbow after the first component, indicating that retaining two components captures most of the variance while keeping the solution interpretable.

```{r}
diamonds.pca.sup <- PCA(numeric_vars, quanti.sup = 7, graph = T)

barplot(diamonds.pca.sup$eig[,1], names.arg = 1:6, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca.sup$eig[,1], type = "o", col = "red", pch = 19, add=T)

diamonds.pca.sup$eig
```

It is important to determine which variables contribute most to each of the relevant components in order to reduce the number of variables. Hence, considering the coordinates of the variables, it is possible to see which variable is associated with each component. On the other hand, examining the $cos^2$ values shows which variables are more strongly related to a given component.

-   Dimension $1$: Physical Magnitude (and Price Component) Dimension $1$ shows strong positive loadings for variables related to the diamond’s physical magnitude, such as x, y, z, and logCarat. As can be seen by the plot, the supplementary variable logPrice is very correlated with dimension 1. This indicates a clear relationship between a diamond’s overall material magnitude and its price. As expected, larger and heavier diamonds tend to be more expensive. Hence, this axis can be interpreted as an overall physical magnitude-price direction. Diamonds with high coordinates on dimension $1$ are generally larger, heavier, and more valuable, while diamonds with low coordinates tend to be smaller and less expensive.

-   Dimension $2$: Geometric Balance Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Considering that depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. In other words, dimension $2$ is positively associated with taller, more pointed diamonds with a narrower top surface relative to their width, and negatively associated with shallow or flattened diamonds.

```{r}
corrplot(diamonds.pca.sup$var$coord)
diamonds.pca.sup$var$cos2
diamonds.pca.sup$var$contrib

diamonds.pca.sup$var$coord

plot.PCA(diamonds.pca.sup, choix = "var", axes = c(1, 2))
```

## 2.2. Multidimensional scaling

## 2.3. Correspondance analysis ??????

## 2.4. Multiple correspondance analysis

In order to do Multiple Corresponance Analysis using also the numerical variables, these will be converted to binary variables with two categories as “over average” and “below average” for each of them, which will also help with its interpretability.

Factor level 'over average' will be defined as 'o', and 'below average' as 'b'.

```{r}
dfmca <- diamonds
nums = names(numeric_vars)

for (var in nums){
  meanvar = mean(dfmca[[var]])
  dfmca[[var]] <- ifelse(dfmca[[var]] >= meanvar, "o", "b") 
  dfmca[[var]] <- as.factor(dfmca[[var]])
}
summary(dfmca)
```

The analysis will be done using Indicator Matrix. That way we will be able to study both the categories of the variables and the individuals.

Regarding the logPrice variable, the approach followed will be the same as in PCA: it will be included as a supplementary variable, to help further the interpretation without being included in the calculations.

```{r}
## Indicator Matrix
library(FactoMineR)
tab.disjonctif(dfmca)

## MCA Application on Data using Indicator Matrix
# logPrice used as a supplementary variable
res.mca <- MCA(dfmca,quali.sup=10)
summary(res.mca)
```

In order to choose which dimensions we will work with, we apply the rule of choosing the dimensions with eigenvalues greater than the average inertia of the indicator matrix per dimension.

In this case, as there are 10 variables, J = 10, and the total number of values is K=5+7+8+(7x2)=34, number of dimensions is K-J=24. Therefore, the average inertia is 1/24 (0.04166667), and the dimensions selected are until Dim.20????

The summary shows all of the analysis, but the description will be done separately.

```{r}
# K=15x2=30, J=15
# no of dimension: K-J=15
# average inertia per dimension: 1/(K-J)=1/15

# Choose the dimensions with eigenvalues > 1/15 --> Until Dim.4
summary(res.mca, ncp=4)
res.mca
```

As can be seen in the description of Dimensions, the most significant variables in each dimension, which are those represented with the highest value of R2, are: - For *dimension 1*, the Efficiency `EFF`, as it's value of R2 is 0.8414. - For *dimension 2*, it is number of 2pt Field Goal `2PT FG`, with a value of R2 of 0.6864. - For *dimension 3*, there isn't such a significant variable, but the one with the highest R2 value is 3pt Field Goal `3PT FG`, with a value of R2 of 0.3615.

Also, it is relevant to observe the link between variable and categories to see which are the most significant categories in each dimension, which are those represented with the highest absolute value of Estimate.

They follow the same pattern for all three dimensions: the categories most positively correlated are the most significant variables mentioned above, for the 'over average' category, and the most negatively correlated is the 'under average' value of the same variable.

This can also be corroborated with the coordinate analysis. They are the values that are more extreme, in the negative and positive side of the axis.

That means that observations will be easily separated between those over average and under, and interpreted according to each dimension.

-   Note: The results for dimension 4 are not shown by R, as it has an eigenvalue that is technically lower than the threshold (0.066 \< 0.067).

```{r}
### Eigenvalues
res.mca$eig
barplot(res.mca$eig[,2],main="Eigenvalues", names.arg=1:nrow(res.mca$eig))

## Coordinates of categories
res.mca$var$coord

## Description of Dimensions
dimdesc(res.mca)
```

For the plots, we will interpret them in the 1st and 2nd dimension, since they are the most significative.

Firstly, we will interpret the *cloud of individuals*. The plot shows both the individuals and the categories represented by the 1st and 2nd dimension.

-   It is worth noting that there are almost no points in the center of the graph, they are all placed away from the origin.

-   For the categories, this pattern is due to the transformation of the numerical variables into binary ones, dividing them into “over average” and “below average” categories. Since both categories have similar frequencies, none of them represents a “dominant” or “average” profile. Therefore, both are located in opposite directions.

-   For the individuals, since each player belongs to either the “over” or “under” group for each variable, the overall profile of individuals is more polarized.

-   As a conclusion, binarizing continuous variables helps to highlight contrasts between players’ performances, but at the same time reduces the amount of information in the variables.

```{r}
### Clouds ###
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none")
```

Then, we will interpret the *cloud of individuals grouped by most significant categories per dimension*. This is done by selecting the variable in the 'habillage' parameter.

-   For *dimension 1*, the Efficiency `EFF`. The map clearly shows this distinction: in the negative side of the Dim 1 axis, we can see the individuals with an efficiency below average, and in the positive side, those over average.

-   For *dimension 2*, it is number of 2pt Field Goal `2PT FG` Although there are a few exceptions, the map also shows this distinction: in the negative side of the Dim 2 axis, we can see the individuals with a value of `2PT FG` below average, and in the positive side, those over average.

```{r}
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "logCarat")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "cut")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "table")
```

The graph of the active categories is very relevant: the categories of values 'over average' are represented on the positive side of Dim 1, and the ones 'below average' are on the negative side.

Considering the total information obtained from the cloud of categories, and also the cloud of variables, the conclusions we draw of the information obtained by the dimensions is the following:

-   *Dimension 1* mainly represents the overall performance level, separating players with “over average” results on the positive side, from those “below average” on the negative side.

-   *Dimension 2* is more related to the shooting style of the players. Players with above-average values in 2-point field goals (2PT FG) are positioned on the upper side of the axis, together with offensive rebounds (OREB), which makes sense since players who frequently capture offensive rebounds often score from close range, leading to more 2-point shots. These players also tend to have below-average 3-point field goals (3PT FG), which fits with the usual profile of inside players. Conversely, the lower part of the axis shows the opposite pattern — players with higher 3-point shooting and fewer offensive rebounds, typically corresponding to perimeter players.

```{r}
## Cloud of Categories
plot(res.mca,invisible=c("ind"),title="Graph of the active categories")
plot(res.mca,invisible=c("ind"),col.var=rep(c("black","red"),15),title="Graph of the active categories")

## Cloud of variables
plot(res.mca,choix="var",title="Cloud of variables")
```

Additional analysis: Using `position` as a supplementary variable.

This is done in order to check if this variable gives any extra information to the analysis.

As we can see in the analysis, this supplementary variable is mainly focused on the second dimension. Center and forward are on the positive side, while guard is on the negative. The behaviour of these positions corresponds to the interpretation of dimension 2 done previously: players who score more on 2 PT shots and with more offensive rebounds are Center and Forward, and those with more 3 PT shots are the Guards.

```{r}
# Position used as a supplementary variable
res.mca <- MCA(dfmca,quali.sup=1)

# Result summaries of MCA
res.mca$quali.sup

# Graphs of the variables
plot(res.mca,choix="var",title="Graph of the variables")
plot(res.mca,invisible=c("ind","var"),hab="quali", title="Graph of the supplementary categories")
```


## 2.5. Clustering and profiling

In this section, a clustering analysis is performed with the objective of identifying meaningful groups within the dataset. The analysis is carried out under two different settings.

First, clustering is applied using only the numerical variables, combining hierarchical and non-hierarchical methods, as well as clustering based on the principal component scores. Second, clustering of numerical and categorical variables is conducted to obtain a more comprehensive segmentation of the data.

### A) Cluster analysis: numerical variables

#### Hierarchical clustering

The hierarchical clustering analysis is performed on the numerical variables after standardising them with the \emph{scale()} function. This ensures that all variables contribute equally to the computation of distances. An Euclidean distance matrix is then obtained using the \emph{dist()} function applied to the scaled dataset.

The clustering structure is produced with the \emph{hclust()} function, and two linkage criteria are considered in order to compare their performance. First, the **complete linkage method** is computed, which defines the distance between clusters based on the maximum distance between their observations. Second, the **Ward’s method** is used, which is based on minimising the within-cluster sum of squared errors (SSE).

The resulting dendrograms allow a first inspection of the potential number of clusters. Using the \emph{rect.hclust()} function, possible partitions into 4, 5 and 6 clusters are highlighted. As observed in the figures, the Ward method provides a clearer and more compact separation between groups than the complete linkage method, suggesting that it offers a more interpretable clustering solution.

```{r}
## Scale numerical data
dfscaled <- scale(numeric_vars)
dfscaled <- as.data.frame(dfscaled) # convert the scaled data to a dataframe
d <- dist(dfscaled, method = "euclidean") # apply distance matrix on the scaled dataframe

## Complete linkage method
fit1 <- hclust(d, method="complete") 
plot(fit1, main="Dendrogram of complete Linkage") 
# 4 clusters/6 clusters

rect.hclust(fit1, k=4, border="blue") # 4 clusters
rect.hclust(fit1, k=5, border="cyan") # 5 clusters
rect.hclust(fit1, k=6, border="red")  # 6 clusters

legend("topright",
       legend = c("4 clusters", "5 clusters", "6 clusters"),
       col = c("blue", "cyan", "red"),
       lwd = 2,
       cex = 0.8,
       bg = "white")

## Ward method
fit2 <- hclust(d, method="ward.D2") 
plot(fit2, main="Dendrogram of Ward Method") 
# 4 clusters/5 clusters

rect.hclust(fit2, k=4, border="green")  # 4 clusters
rect.hclust(fit2, k=5, border="purple") # 5 clusters
rect.hclust(fit2, k=6, border="orange") # 6 clusters

legend("topright",
       legend = c("4 clusters", "5 clusters", "6 clusters"),
       col = c("green", "purple", "orange"),
       lwd = 2,
       cex = 0.8,
       bg = "white")
```

#### Non-hierarchical clustering

A non-hierarchical clustering analysis is performed using several criteria to determine an appropriate number of clusters. Three different methods are considered: the **TWSS Elbow Graph**, the **Pseudo F Index**, and the **Silhouette Index**.

##### 1. TWSS Elbow Graph

The Total Within-Cluster Sum of Squares (TWSS) has been computed for different values of k using the **k-means algorithm**, in order to identify an appropriate number of clusters through the **Elbow method**.

As shown in the figure, the TWSS decreases sharply between $k=2$ and $k=4$, and then continues to decrease more gradually. A noticeable bend appears around $k=4$.

The ratio between the between-cluster sum of squares and the total sum of squares is computed. For $k=4$, this ratio is equal to $0.694$, indicating a fair degree of separation between clusters.

```{r}
## Applying k-means algorithm for different number of clusters 
aux <- c()
for (i in 2:7){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- k$tot.withinss 
}
plot(2:7, aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")
# Optimum point: k=4

# k=4
k4 <- kmeans(dfscaled, centers = 4, nstart = 25) # 4 number of clusters 

## Sum of Squares 
k4$withinss 
k4$totss 
k4$tot.withinss # total deviation
k4$betweenss + k4$tot.withinss # total variation of the dataset
k4$betweenss/k4$totss 
# 0.694 fair separation between clusters

# k=6
k6 <- kmeans(dfscaled, centers = 6, nstart = 25) # 6 number of clusters

## Sum of Squares
k6$withinss
k6$totss
k6$tot.withinss # total deviation
k6$betweenss + k6$tot.withinss # total variation of the dataset
k6$betweenss/k6$totss
# 0.768 greater separation between clusters
```

##### 2. Pseudo F Index

The **Pseudo-F Index** has been computed for different values of k in order to evaluate the separation between clusters obtained with k-means. This index is based on the ratio between the between-cluster sum of squares (BSS) and the within-cluster sum of squares (WSS).

In this case the index decreases steadily as k increases, with the highest value appearing at $k=2$. Nevertheless, selecting two clusters does not provide a meaningful segmentation of the dataset, and it contradicts the structure suggested by the dendrogram and the TWSS Elbow method.

```{r}
# Pseudo F index = BSS*(n-k)/WSS*(k-1)
aux <- c()
for (i in 2:20){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- ((k$betweenss)*(nrow(dfscaled)-i))/((k$tot.withinss)*(i-1))
}
plot(2:20, aux, xlab="Number of Clusters", ylab="Pseudo-F", type="l", main="Pseudo F Index")
# max Pseudo F --> 2 clusters
```

##### 3. Silhoutte Index

Finally, the **Silhouette Index** is computed. For each observation, this value compares the average distance to points within its own cluster with the average distance to points in the nearest alternative cluster.

Using the \emph{silhouette()} function from the \emph{cluster} package, the average silhouette widths obtained for $k=4$ and $k=6$ are $0.36$ and $0.28$, respectively. The four-cluster solution shows higher overall cohesion, with two clusters (notably cluster 2) presenting silhouette values above $0.40$, whereas the six-cluster solution contains multiple clusters with silhouette widths below $0.25$, indicating weaker separation.

The silhouette index is also computed using the \emph{fastkmed()} function from the \emph{kmed} package. This approach confirms the same trend: for a range of cluster solutions, the highest silhouette values are obtained for three or four clusters, whereas the performance decreases for higher values of k. The four-cluster solution achieves a good balance between separation and stability.

```{r}
## A) Silhouette function from cluster library
library(cluster)

# k=4
si <- silhouette(k4$cluster, d)
plot(si)
# 497 obs, we divide them to 4 clusters, we look for the max silhouette index: 2nd cluster

# k=6
si <- silhouette(k6$cluster, d)
plot(si)
# 497 obs, we divide them to 6 clusters, we look for the max silhouette index: 5th cluster


## B) Silhouette function from kmed library
library(kmed)

# k=4
res <- fastkmed(d, 4) # iterative algorithm, 4 clusters
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot 
# k=3 higher silhouette value

# k=6
res <- fastkmed(d, 6) # iterative algorithm, 6 clusters
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot
# k=4 higher silhouette value
```

**Conclusion:** Overall, the three criteria do not point to the same value of k, but they do suggest a consistent tendency. The Elbow method shows a clear bend around four clusters, the Pseudo-F index does not provide useful guidance, and the Silhouette index indicates that three or four clusters offer the best structure. Thus, **four clusters** appear to be the most balanced and interpretable solution, and this value will be used for the next steps of the analysis.

#### Clustering based on Principal Components

The Hierarchical Clustering on Principal Components (HCPC) method is applied to the scores obtained from the previously computed PCA. This approach combines dimensionality reduction and clustering by performing Ward’s hierarchical clustering on the principal component scores, which reduces the influence of noise and correlation between variables.

The \emph{HCPC()} function is used with an automatic selection of the number of clusters, which results in a partition into 3 clusters.

From the factor map, the 3 clusters are projected onto the first two principal components, which together explain more than 80% of the total variance. Cluster 1 is mainly located on the left side of the first dimension, which is associated with smaller physical measurements of the diamonds. Cluster 2 occupies an intermediate region, while Cluster 3 is positioned towards the positive side of the first component, corresponding to larger diamond sizes.

```{r}
# HCPC applied to the previously computed PCA
res.hcpc <- HCPC(diamonds.pca, -1) # automatic cut: 3 clusters

# Dendrogram visualization
library(factoextra)
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE)

# Cluster visualization on the PCA factor map
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,
             main = "Clusters on PCA Factor Map")
```

Although the automatic HCPC cut suggests a 3-cluster partition, the factor map and the dendrogram reveal that a 4-cluster solution is more balanced and more interpretable. The 3-cluster solution produces one dominant group, whereas splitting the structure into 4 clusters yields groups of comparable size and a clearer separation in the PCA space.

Based on the cluster distribution on the PCA factor map and the interpretation of the principal dimensions, the characteristics of each group can be summarised as follows:

-   **Cluster 1** (black) is situated on the negative side of Dimension 1, indicating that it consists of the smallest and least expensive diamonds in the dataset. Its spread along Dimension 2 suggests a variety of geometric profiles without a dominant shape.
-   **Cluster 2** (red) is positioned around moderately positive values of Dimension 1 and clearly positive values of Dimension 2, suggesting medium-sized diamonds that tend to be deeper and more vertically proportioned.
-   **Cluster 3** (green) occupies the region of moderately positive Dimension 1 but negative Dimension 2, indicating diamonds that are also medium-to-large in size but display flatter geometric shapes.
-   **Cluster 4** (blue) appears on the positive side of Dimension 1, representing the largest and most valuable diamonds, with shapes that are generally balanced or slightly deep.

```{r}
# HCPC applied to the previously computed PCA
res.hcpc <- HCPC(diamonds.pca, nb.clust = 4) # 4 clusters

# Dendrogram visualization
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE)

# Cluster visualization on the PCA factor map
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,
             main = "Clusters on PCA Factor Map")
```


### B) Cluster analysis: numerical and categorical variables

Before performing the clustering analysis that incorporates both numerical and categorical variables, it is important to consider the distribution of the categorical levels. Initially, the clustering is applied using the original categories; however, several levels contain very few observations, which may introduce instability and lead to clusters driven by marginal or unrepresentative cases. For this reason, grouping levels with similar qualitative meaning and low frequency can be a useful strategy to obtain more balanced and consistent clusters.

```{r}
dfclust <- diamonds

# Cut variable: group Fair and Good in one group 
dfclust$cut <- ifelse(dfclust$cut %in% c("Fair", "Good"), "Fair/Good", as.character(dfclust$cut))

dfclust$cut <- factor(
  dfclust$cut,
  levels = c("Fair/Good", "Ideal", "Premium", "Very Good"))

barplot(table(dfclust$cut), main="Diamond Cut Quality")

# Color variable: group I and J in one group 
dfclust$color <- ifelse(dfclust$color %in% c("I", "J"), "I/J", as.character(dfclust$color))

dfclust$color <- factor(
  dfclust$color,
  levels = c("D", "E", "F", "G", "H", "I/J"))

barplot(table(dfclust$color), main="Diamond Color")

# Clarity variable: group I1 and IF in one group
dfclust$clarity <- ifelse(dfclust$clarity %in% c("I1", "IF"), "I1/IF", as.character(dfclust$clarity))

dfclust$clarity <- factor(
  dfclust$clarity,
  levels = c("I1/IF", "SI1", "SI2", "VS1", "VS2", "VVS1", "VVS2"))

barplot(table(dfclust$clarity), main="Diamond Clarity")
```
#### Factor Analysis for Mixed Data
The clustering analysis that includes both numerical and categorical variables is performed through a Factor Analysis of Mixed Data (FAMD), which provides a joint low-dimensional space where continuous and categorical variables contribute to the definition of the principal axes.

!!!!!! Relacionar amb PCA i MCA (quan s'acabi mirar bé)

The interpretation of the continuous variables in the FAMD closely mirrors the results previously obtained with the PCA. Dimension 1, which explains 25.29% of the total variability, is dominated by the physical measurements of the diamonds (x, y, z, logCarat, logPrice), all of which show extremely high squared correlations with this axis. As in the PCA, this dimension represents a size–value gradient, separating small and inexpensive diamonds from those that are larger and more valuable.

The categorical variables reinforce this interpretation. Premium cut diamonds and colour categories such as H and especially I/J have positive coordinates on Dimension 1, confirming their association with larger stones. Conversely, better colour grades (D, E) and the Ideal cut are positioned at the negative side of this axis, in line with smaller diamonds.

Dimension 2 (7.91% of explained variance) reflects geometric proportions, reproducing the same contrast identified in the PCA between diamonds with greater depth (positive coordinates) and those with larger table (negative coordinates). The categorical variable cut aligns with this structure: Ideal cut appears at the positive end, whereas Premium is strongly negative.

Dimension 3 (6.15% of variance) is mainly driven by categorical variables, especially cut categories (Very Good, Fair/Good) and colour categories such as E and H, which show strong positive contributions. Other categories play a smaller role in shaping this axis.

```{r}
# Factor Analysis for Mixed Data
res <- FAMD(dfclust)
summary(res)
```

#### Hierarchical Clustering on FAMD scores

The hierarchical clustering is applied to the individual scores obtained from the FAMD. Using the \emph{HCPC()} function with the automatic cut, the algorithm has selected a three-cluster solution. The separation observed in the factor map is primarily driven by Dimension 1:
-   **Cluster 1** (black) groups observations with negative coordinates on this dimension, corresponding to smaller diamonds and categories associated with higher quality grades (Ideal cut, better colour). 
-   **Cluster 2** (red) occupies an intermediate position and presents internal heterogeneity, both in terms of size and categorical composition, which makes the partition less balanced.
-   **Cluster 3** (green) lies on the positive side of Dimension 1 and gathers larger diamonds, consistently aligned with levels such as Premium cut and lower-quality colours (H, I/J). 

```{r}
# Hierarchical Clustering on FAMD scores
hc_famd <- HCPC(res, -1) # automatic cut: 3 clusters
```
To obtain a more homogeneous segmentation, the HCPC has been recomputed by imposing a **four-cluster solution**. In the factor map, the two central clusters separate mainly along Dimension 2, which reflects contrasting geometric proportions and the behaviour of the cut variable: the cluster scoring positively is associated with deeper diamonds and categories such as Ideal, whereas the negatively scoring cluster corresponds to flatter diamonds and Premium cuts. Together with the cluster of small diamonds and the cluster of large diamonds, the four groups provide a clearer interpretation that integrates both numerical and categorical patterns. For this reason, the four-cluster solution is retained for the profiling stage.

```{r}
# Hierarchical Clustering on FAMD scores
hc_famd <- HCPC(res, nb.clust = 4) # 4 clusters 
```

```{r}
# Dendrogram with 4 number of clusters
library(factoextra)
fviz_dend(hc_famd, rect = TRUE, rect_fill = TRUE)

# Visualization of Clusters
fviz_cluster(hc_famd, repel = TRUE, show.clust.cent = TRUE, main = "Factor map")
```

### Profiling

#### Profiling of the HCPC clysters on PCA scores

The HCPC performed on the PCA scores allows us to characterise the clusters according to the structure captured by the two principal components. Since Dimension 1 reflects the overall physical magnitude–price axis, and Dimension 2 captures the geometric balance between deep and flat diamonds, the clusters can be interpreted in terms of size, price and shape.

- **Cluster 1:** Smallest and least expensive diamonds
Cluster 1 presents the lowest means for x, y, z, logCarat and logPrice, with extremely negative v-tests. These diamonds are small, light and low-priced, fully aligned with the negative side of Dimension 1. They also show slightly smaller table values.

- **Cluster 2:** Diamonds with higher depth and smaller tables
Cluster 2 is defined by geometric proportions, as shown by the strong positive v-test for depth and negative v-test for table. These diamonds tend to be deeper and more vertically elongated, even though their physical magnitude remains below the global average. They represent diamonds with pronounced depth and flat top surfaces, matching the positive side of Dimension 2.

- **Cluster 3:** Moderately large diamonds with high table and low depth
Cluster 3 shows significantly higher table values and lower depth, the opposite pattern of Cluster 2. Its size-related variables are slightly above the mean, especially x, y, z and logPrice, indicating medium-to-large diamonds. Therefore, these stones correspond to flatter, larger diamonds.

- **Cluster 4:** Largest and most expensive diamonds
Cluster 4 is characterised by extremely high values of x, y, z, logCarat and logPrice, with very large positive v-tests. This cluster clearly gathers the largest, heaviest and most expensive diamonds in the dataset, representing the upper end of Dimension 1.

```{r}
res.hcpc$desc.var
```

#### Profiling based on FAMD scores

The profiling of the clusters obtained from the FAMD scores can be interpreted as follows:

- **Cluster 1:** Small, high-quality diamonds
Shows strong overrepresentation of the Ideal cut, as well as premium clarity levels such as VVS1, VVS2 and I1/IF, together with an excess of colour D diamonds. Conversely, low-quality colours (I/J) and intermediate clarity levels (VS2, SI2) are significantly underrepresented.

This behaviour is consistent with the quantitative profile: Cluster 1 contains the smallest diamonds in the dataset, with the lowest means for x, y, z and logCarat, and with the lowest logPrice. The diamonds in this group also tend to have smaller tables and slightly higher depth.

- **Cluster 2:** Medium-sized diamonds with premium/proportioned cuts
This cluster is dominated by the Premium cut, which accounts for almost two-thirds of its observations. It also shows overrepresentation of clarity levels VS1, VS2 and colours E and G, indicating that this group contains well-proportioned diamonds of medium quality. The Ideal cut, however, is strongly underrepresented.

Quantitatively, the diamonds in this cluster remain smaller than the overall mean but are larger than those in Cluster 1. They present the highest table values, confirming that this cluster groups diamonds that are relatively flat in shape, whereas their depth values are slightly lower than the global mean. Price and size metrics are moderately below the overall mean.

- **Cluster 3:** Medium-to-large diamonds with high prices and deeper shapes
It combines a strong presence of the Ideal cut with an overrepresentation of clarity SI1. The categorical composition indicates a group of medium-quality but well-cut diamonds.

The quantitative variables show that this cluster consists of larger diamonds: the means for x, y, z and logCarat are above the overall mean. This group also exhibits the highest average logPrice, consistent with the size increase. In terms of geometry, these diamonds tend to be deeper.

- **Cluster 4:** Largest, most expensive diamonds with lower colour/clarity grades
Cluster 4 gathers the largest diamonds in the dataset, with extremely high values for x, y, z, logCarat and logPrice, all showing the highest positive v-tests

Categorically, it shows strong overrepresentation of colour I/J and colour H, and of clarity SI2. Premium cut is also heavily overrepresented, while the Ideal cut appears far below expectations. In geometric terms, these diamonds have large table values and slightly lower depth than average, suggesting flatter shapes.


```{r}
hc_famd$desc.var
```
