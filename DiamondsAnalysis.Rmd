---
title: "Diamonds Data Frame Analysis"
author: "Berta Torrents"
date: "December 2025"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
geometry: margin=1in
---

- podem usar totes les variables correlacionades en el mca, clustering... o es millor usar per exemple un index ue sorti a partir de la PCA i englobi les variables??


- hem d'interpretar tots el clusters al profiling o només el resultant? el problema és que el que millor resultat ens dona es el del pca i aleshores no podriem fer profiling de les categoriques


- unificar text + $4$ + $\%$ + ** + treure '' en les variables + "" i - del changpt

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  results = 'markup',
  message = FALSE,
  warning = FALSE,
  fig.width = 7,
  fig.height = 5
)
```

```{r}
set.seed(12345)

# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Libraries
library(dplyr)
library(FactoMineR)
library(MASS)
library(psych)
library(factoextra)
library(corrplot)
library(cluster)
library(ggplot2)
library(ggrepel)
```

## 1. Data preparation

The dataset have been imported using the \emph{read.csv()} function from the \emph{dplyr} library. The variable named X have been removed, as it corresponds to an index column rather than an explanatory variable. Additionally, the categorical variables cut, color, and clarity have been correctly converted into factors.

```{r}
diamonds <- read.csv('diamonds.csv')

diamonds <- diamonds[,-1]

diamonds$cut <- as.factor(diamonds$cut)
diamonds$color <- as.factor(diamonds$color)
diamonds$clarity <- as.factor(diamonds$clarity)

summary(diamonds)
```

Since the dataset contained a large number of observations $(53940)$, a random sample of $500$ observations has been selected to allow for a more manageable and focused analysis. It is worth noting that the means of the numerical variables and the proportions of observations in each category do not vary significantly.

```{r}
n <- nrow(diamonds)
indices <- sample(1:n, 500) # Take 500 random indices
diamonds <- diamonds[indices,]
rownames(diamonds) <- seq_len(nrow(diamonds))

summary(diamonds)
```

## Exploratory data analysis

For each variable, we check: duplicates, existence of zeros, existence of outliers, existence of missing values and we apply transformations if it is needed.

## Numerical variables

Helper function to evaluate missing values and zeros

```{r}
check_na <- function(x) {
  list(na = sum(is.na(x)),
  zeros = sum(x == 0))
}
```

```{r}
results <- lapply(diamonds, check_na)
results
```

We observe that no numeric variable has NA or zeros.

### Carat variable

After analyzing the carat variable, we observe that it doesn't have severe outliers. The carat variable is right-skewed, with a long tail towards larger values, so is clearly not normally distributed. After using boxcox function, we see that a log transformation is appropriate to improve symmetry and stabilize variance.

Notice that the log transformation does not follow a normal distribution neither. The histogram is clearly not bell-shaped and the QQ-plot has a lot of deviations. As it is expected both the Shapiro and the Kolmogorov test have rejected the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$carat, horizontal = T)
varout <- summary(diamonds$carat)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$carat, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$carat >= usout) | (diamonds$carat <= lsout))
mild_out <- which((diamonds$carat >= umout) | (diamonds$carat <= lmout))
sev_out # 0
mild_out #22

# Distribution
hist(diamonds$carat,freq=F)
curve(dnorm(x),add=T, col="red") #It is obviously not normally distributed

# Transformation
b <- boxcox(lm(diamonds$carat~1)) # Box-Cox Transformation
# lambda <- b$x[which.max(b$y)]
# diamonds$new_carat <- (diamonds$carat^1-1)/lambda

diamonds$logCarat <- log(diamonds$carat) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logCarat, horizontal = T) # No outliers

# Is logCarat normally distributed?
hist(diamonds$logCarat, freq = F)
m = mean(diamonds$logCarat)
std = sd(diamonds$logCarat)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)

qqnorm(diamonds$logCarat, main = "logCarat Q-plot")
qqline(diamonds$logCarat)

shapiro.test(diamonds$logCarat)
ks.test(diamonds$logCarat, "pnorm", mean=mean(diamonds$logCarat), sd=sd(diamonds$logCarat))
```

### Depth variable

The depth variable has $2$ severe outliers, which we removed to make the distribution closer to normality. The histogram of the variable shows that the depth variable appears to be bell-shaped. Moreover, the QQ-plot almost shows a straight line without major deviations, which suggests that this variable could be considered approximately normal.

Despite the fact that the Shapiro and the Kolmogorov test show a low p-value, leading to the rejection of the normality hypothesis, we have decided to consider the variable to be normally distributed based on the graphical assessment.

```{r}
# Outliers
boxplot(diamonds$depth, horizontal = T)
varout <- summary(diamonds$depth)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$depth, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$depth >= usout) | (diamonds$depth <= lsout))
mild_out <- which((diamonds$depth >= umout) | (diamonds$depth <= lmout))
sev_out #[1] 462 467
mild_out #21
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Is depth normally distributed?
hist(diamonds$depth,freq=F)
m = mean(diamonds$depth)
std = sd(diamonds$depth)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$depth, main = "depth Q-plot")
qqline(diamonds$depth)

shapiro.test(diamonds$depth)
ks.test(diamonds$depth, "pnorm", mean=mean(diamonds$depth), sd=sd(diamonds$depth))
```

### Table variable

The variable table contains $1$ severe outlier. After removing it, the distribution becomes almost symmetric with a mild right skewness. Since its shape is already close to normal and a transformation would not provide meaningful improvement, no transformation is applied. The QQ-plot also shows points lying close to a straight line, without major deviations, which further suggests approximate normality. Although the variable is discrete, its wide range of values and its bell-shaped histogram support the assumption that normality may reasonably hold.

Despite the fact that the Shapiro and the Kolmogorov test show a low p-value, leading to the rejection of the normality hypothesis, we have decided to consider the variable to be normally distributed based on the graphical assessment.

```{r}
# Outliers
boxplot(diamonds$table, horizontal = TRUE)
varout <- summary(diamonds$table)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$table, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$table >= usout) | (diamonds$table <= lsout))
mild_out <- which((diamonds$table >= umout) | (diamonds$table <= lmout))
sev_out #[1] 206
mild_out #[1] 206 445
diamonds <- diamonds[-sev_out,] # Remove extreme outliers

# Is table normally distributed?
hist(diamonds$table,freq=F)
m = mean(diamonds$table)
std = sd(diamonds$table)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$table, main = "table Q-plot")
qqline(diamonds$table)

shapiro.test(diamonds$table)
ks.test(diamonds$table, "pnorm", mean=mean(diamonds$table), sd=sd(diamonds$table))
```

### Price variable

The variable price doesn't have severe outliers, but we can observe a long tail at the right of the boxplot, indicating potential mild outliers. The histogram shows a right skewed distribution, which can be improved by a transformation to reduce its skewness and stabilize variance. Based on the boxcox, it seems like a logarithmic transformation would be suitable. After applying the log transformation, the distribution becomes more symmetric and closer to a bell-shaped form, without being still normal distributed. The QQ-plot suggests that the logarithmic variable is still not normally distributed since many deviations are shown. Moreover, the Shapiro and the Kolmogorov test show a low p-value, leading to the rejection of the normality hypothesis.

```{r}
# Outliers
boxplot(diamonds$price, horizontal = T)
varout <- summary(diamonds$price)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$price, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$price >= usout) | (diamonds$price <= lsout))
mild_out <- which((diamonds$price >= umout) | (diamonds$price <= lmout))
sev_out #0
mild_out #28

# Distribution
hist(diamonds$price, freq = F) # A log transformation could be applied to reduce skewness and stabilize variance
m = mean(diamonds$price)
std = sd(diamonds$price)
curve(dnorm(x,m,std),add=T, col="red")

b <- boxcox(lm(diamonds$price~1)) # Box-Cox Transformation
diamonds$logPrice <- log(diamonds$price) # Since lambda ~ 0 -> logarithmic transformation
boxplot(diamonds$logPrice, horizontal = T) # No outliers

# Is logPrice normally distributed?
hist(diamonds$logPrice, freq = F)
m = mean(diamonds$logPrice)
std = sd(diamonds$logPrice)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)

qqnorm(diamonds$logPrice, main = "logPrice Q-plot")
qqline(diamonds$logPrice)

shapiro.test(diamonds$logPrice)
ks.test(diamonds$logPrice, "pnorm", mean=mean(diamonds$logPrice), sd=sd(diamonds$logPrice))
```

### x variable

The x variable does not show any mild nor severe outliers. Nevertheless, the histogram clearly shows that there is one observation with a very low x value compared with the rest. We have attempted to remove this observation, but even after applying a transformation afterward, we have not succeeded in normalizing the variable. For this reason, we have decided to treat the variable in its original form although it clearly does not follow a normal distribution.

```{r}
# Outliers
boxplot(diamonds$x, horizontal = T)
varout <- summary(diamonds$x)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$x, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$x >= usout) | (diamonds$x <= lsout))
mild_out <- which((diamonds$x >= umout) | (diamonds$x <= lmout))
sev_out #0
mild_out #0

# Distribution
hist(diamonds$x, freq = F)
m = mean(diamonds$x)
std = sd(diamonds$x)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$x, main = "x Q-plot")
qqline(diamonds$x)

shapiro.test(diamonds$x)
ks.test(diamonds$x, "pnorm", mean=mean(diamonds$x), sd=sd(diamonds$x))
```

### y variable

The y variable does not show any mild nor severe outliers. Nevertheless, the histogram clearly shows that there is one observation with a very low y value compared with the rest. We have attempted to remove this observation, but even after applying a transformation afterward, we have not succeeded in normalizing the variable. For this reason, we have decided to treat the variable in its original form although it clearly does not follow a normal distribution.

```{r}
# Outliers
boxplot(diamonds$y, horizontal = T)
varout <- summary(diamonds$y)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$y, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$y >= usout) | (diamonds$y <= lsout))
mild_out <- which((diamonds$y >= umout) | (diamonds$y <= lmout))
sev_out #0
mild_out #0

# Distribution
hist(diamonds$y, freq = F)
m = mean(diamonds$y)
std = sd(diamonds$y)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$y, main = "y Q-plot")
qqline(diamonds$y)

shapiro.test(diamonds$y)
ks.test(diamonds$y, "pnorm", mean=mean(diamonds$y), sd=sd(diamonds$y))
```

### z variable

The z variable does not show any mild nor severe outliers. Nevertheless, the histogram clearly shows that there is one observation with a very low z value compared with the rest. We have attempted to remove this observation, but even after applying a transformation afterward, we have not succeeded in normalizing the variable. For this reason, we have decided to treat the variable in its original form although it clearly does not follow a normal distribution.

```{r}
# Outliers
boxplot(diamonds$z, horizontal = T)
varout <- summary(diamonds$z)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

boxplot(diamonds$z, horizontal = T)
abline(v=usout,col='red', add = T)
abline(v=lsout,col='red', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out <- which((diamonds$z >= usout) | (diamonds$z <= lsout))
mild_out <- which((diamonds$z >= umout) | (diamonds$z <= lmout))
sev_out #0
mild_out #0

# Distribution
hist(diamonds$z, freq = F)
m = mean(diamonds$z)
std = sd(diamonds$z)
curve(dnorm(x,m,std),add=T, col="red")

qqnorm(diamonds$z, main = "z Q-plot")
qqline(diamonds$z)

shapiro.test(diamonds$z)
ks.test(diamonds$z, "pnorm", mean=mean(diamonds$z), sd=sd(diamonds$z))
```

## Categorical variables

### Cut variable

Cut variable shows no missing values. It is clearly unbalanced, with the Ideal level accounting for $37.4\%$ of observations, while the Fair level comprising only $1.8\%$ of the observations. These strange categories could be grouped further depending on the results of the multivariate analysis.

Since Fair and Good are consecutive levels on the standard cut-quality scale and represent similar performance in terms of light return and symmetry, they can be combined into a single category (Fair/Good). This grouping preserves the ordinal logic of the cut grading system while ensuring more balanced frequencies across categories, leading to more robust and interpretable results.

```{r}
# Distribution
summary(diamonds$cut)
barplot(summary(diamonds$cut), main="Diamond Cut Quality")

prop.table(table(diamonds$cut))

# We can group Fair and Good in one group because Fair has little observations and the variable is unbalanced

diamonds$cut <- ifelse(diamonds$cut %in% c("Fair", "Good"), "Fair/Good", as.character(diamonds$cut))

diamonds$cut <- factor(
 diamonds$cut,
 levels = c("Fair/Good", "Ideal", "Premium", "Very Good"))

barplot(table(diamonds$cut), main="Diamond Cut Quality")
```

### Color variable

Color variable shows no missing values. It is a more balanced variable. Nonetheless, the most frequent grade is G, representing $12.6\%$ of observations, while the least frequent is J, with $3.4\%$, indicating that diamonds with a noticeable tint are less common in the dataset.

To avoid categories with very low frequency and to improve stability in the multivariate analysis, the light-tinted grades I and J can be grouped together, as these two grades are consecutive in the GIA color scale and represent similar coloration.

```{r}
# Distribution
summary(diamonds$color)
barplot(summary(diamonds$color), main="Diamond Color")

prop.table(table(diamonds$color))

# We can group I and J in one group because J has little observations and the variable is unbalanced

diamonds$color <- ifelse(diamonds$color %in% c("I", "J"), "I/J", as.character(diamonds$color))
summary(diamonds$color)

diamonds$color <- factor(
  diamonds$color,
  levels = c("D", "E", "F", "G","H","I/J")
)

barplot(table(diamonds$color), main="Diamond Color")
```

### Clarity variable

Clarity variable shows no missing values. It is clearly unbalanced. The diamond clarity distribution is more frequent for the SI1 and VS2 grades. Hence, the sample is composed mostly of slightly-to-very slightly included stones. 

Since some clarity levels (particularly I1 and IF) have very low frequencies, the strange categories can be grouped to avoid having categories with extremely few observations.

The grouping should be done by pairing them with their closest neighbouring grades in the GIA clarity scale. Specifically, I1 should be grouped with SI2, and IF with VVS1. This transformation reduces sparsity, preserves the ordinal structure of the variable and results in more stable and interpretable results.

```{r}
# Distribution
summary(diamonds$clarity)
barplot(summary(diamonds$clarity), main="Diamond Clarity")

prop.table(table(diamonds$clarity))

# We can group I1 and IF in one group because I1 has little observations and the variable is unbalanced

diamonds$clarity <- ifelse(diamonds$clarity %in% c("IF", "VVS1"), "IF/VVS1", as.character(diamonds$clarity))
summary(diamonds$clarity)
diamonds$clarity <- ifelse(diamonds$clarity %in% c("I1", "SI2"), "I1/SI2", as.character(diamonds$clarity))
summary(diamonds$clarity)


diamonds$clarity <- factor(
  diamonds$clarity,
  levels = c("I1/SI2", "SI1", "VS1", "VS2", "VVS2", "IF/VVS1")
)


barplot(table(diamonds$clarity), main="Diamond Clarity")
```

In conclusion, we have removed 3 observations and transformed the variables carat and price with a logarithmic transformation for a better interpretation. Hence, the dataframe we will work with is:

```{r}
diamondsOg <- diamonds[,c(1:10)]
diamonds <- diamonds[,c(2:6,8:12)]
```

## Correlations between numeric variables

The numeric variables are generally highly correlated. The x, y, z, logPrice, and logCarat variables exhibit nearly perfect positive correlations, and their scatter plots form almost straight lines. In fact, the logarithmic transformations of price and carat further increase these correlations.

In contrast, the depth and table variables show weak correlations with the other variables, with depth being the only variable negatively correlated with all the others. Their scatter plots display a cloud of points without a clear pattern, although table and depth appear slightly negatively correlated.

As expected, the Bartlett test returns a very low p-value, rejecting the null hypothesis that the correlation matrix is equal to the identity matrix.

It is important to note that the correlations of the whole population and of the randomly generated sample are very similar, so no misleading conclusions are drawn from using the sample instead of the full dataset.

```{r}
numeric_vars <- diamonds[ , sapply(diamonds, is.numeric)]

cor_matrix <- cor(numeric_vars)

corrplot(cor_matrix, method = "color", addCoef.col = "black", title = "Correlation Plot of Numeric Variables")

pairs(numeric_vars, main="Scatterplot Matrix of Numeric Variables")

cortest.bartlett(cor_matrix, nrow(diamonds))
```

Taking into account the KMO index, since it is higher than 0.5, the data are factorable. In other words, the variables are sufficiently correlated with each other so that meaningful factors can be extracted.

```{r}
# (function written by Prof. Shigenobu Aok.)

kmo <- function(x)
{
  x <- subset(x, complete.cases(x))       # Omit missing values
  r <- cor(x)                             # Correlation matrix
  r2 <- r^2                               # Squared correlation coefficients
  i <- solve(r)                           # Inverse matrix of correlation matrix
  d <- diag(i)                            # Diagonal elements of inverse matrix
  p2 <- (-i/sqrt(outer(d, d)))^2          # Squared partial correlation coefficients
  diag(r2) <- diag(p2) <- 0               # Delete diagonal elements
  KMO <- sum(r2)/(sum(r2)+sum(p2))
  MSA <- colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

# KMO index
kmo(numeric_vars)
```

# Part 2: Multivariant analysis

## 2.1. Principal components analysis

The PCA analysis has been applied using the numeric variables of the diamonds dataset with the \emph{PCA()} function from the \emph{FactoMineR} library.

The objective of this method is to construct an index that summarizes the numerical characteristics of the diamonds (logCarat, x, y, z, depth, and table), with the aim of reducing the dimensionality of the dataset and generating an indicator that can be related to the diamond’s price. Therefore, the logPrice numeric variable has been considered as a supplementary variable. Moreover, the PCA scores can be used to interpret the MDS and clustering results.

By treating price as a supplementary variable, it is excluded from the calculation of the principal components and only projected onto the factorial space afterwards. This makes it possible to study how price aligns with the main axes without altering their structure.

In order to determine the number of components to extract, taking into account that the data is standardized, the Kaiser criterion has been applied. Each eigenvalue represents the amount of variance explained by its corresponding principal component. Therefore, the higher the eigenvalue, the greater the proportion of total variability accounted by that dimension. In this case, the first two components (with eigenvalues $4.06$ and $1.26$) are the ones that meet the Kaiser's requirement. Moreover, the cumulative percentage of variance shows that these two components explain $88.65\%$ of the total variance, which indicates that they capture most of the information in the data frame. In addition, the scree plot shows a noticeable elbow after the first component, indicating that retaining two components captures most of the variance while keeping the solution interpretable.

```{r}
diamonds.pca.sup <- PCA(numeric_vars, quanti.sup = 7, graph = T)

barplot(diamonds.pca.sup$eig[,1], names.arg = 1:6, main = "Scree Plot", xlab = "Dimension", ylab = "Eigenvalue", col = "blue")
lines(diamonds.pca.sup$eig[,1], type = "o", col = "red", pch = 19, add=T)

diamonds.pca.sup$eig
```

It is important to determine which variables contribute most to each of the relevant components in order to reduce the number of variables. Hence, considering the coordinates of the variables, it is possible to see which variable is associated with each component. On the other hand, examining the $cos^2$ values shows which variables are more strongly related to a given component.

-   Dimension $1$: Physical Magnitude (and Price Component): Dimension $1$ shows strong positive loadings for variables related to the diamond’s physical magnitude, such as x, y, z, and logCarat. As can be seen by the plot, the supplementary variable logPrice is very correlated with dimension 1. This indicates a clear relationship between a diamond’s overall material magnitude and its price. As expected, larger and heavier diamonds tend to be more expensive. Hence, this axis can be interpreted as an overall physical magnitude-price direction. Diamonds with high coordinates on dimension $1$ are generally larger, heavier, and more valuable, while diamonds with low coordinates tend to be smaller and less expensive.

-   Dimension $2$: Geometric Balance Dimension $2$ shows a strong positive loading for the depth variable and a strong negative loading for the table variable. Considering that depth measures the vertical height of the diamond from the culet (bottom point) to the table (top flat surface), this dimension can be interpreted as a geometric balance between tall and wide diamonds. In other words, dimension $2$ is positively associated with taller, more pointed diamonds with a narrower top surface relative to their width, and negatively associated with shallow or flattened diamonds.

It is important to note that these results are expected when considering the correlation analysis. The physical dimensions and price are highly and positively correlated, whereas table and depth are only mildly and negatively correlated.

```{r}
corrplot(diamonds.pca.sup$var$coord)
diamonds.pca.sup$var$cos2
diamonds.pca.sup$var$contrib

diamonds.pca.sup$var$coord

plot.PCA(diamonds.pca.sup, choix = "var", axes = c(1, 2))
```

Although an individual assessment of the dataset has not provided much additional information, it is important to note that some observations fall far from the main cloud of points. For instance, the diamonds labeled $270$ and $380$ are clear outliers in dimension two, corresponding to a shallow or flattened diamond and a pointed diamond, respectively. Conversely, the diamond labeled 365 is a clear outlier in dimension one, suggesting that it is larger, heavier, and more valuable.

```{r}
plot.PCA(diamonds.pca.sup, choix = "ind", axes = c(1, 2))
```

TO DO CONCLUSIONS


## 2.2. Multidimensional scaling

The Euclidean distance matrix, based on the scaled numerical variables, has been computed using the \emph{dist()} function. Using this matrix, the \emph{cmdscale()} function has been applied. 

The number of dimensions selected has been $k=2$. Taking into account the scree plot, it shows a noticeable elbow after the first component, indicating that retaining two dimensions captures most of the variance while keeping the solution interpretable. Moreover, the GOF obtained for the two-dimensional solution is $0.89\%$, indicating that the map preserves a substantial proportion of the original distance information and provides a reasonably good representation of the relationships among the observations.

The MDS individual plot shows relationships similar to those observed in the PCA results. The diamonds labeled $270$, $365$, and $308$ are clearly outstanding observations, standing apart from the main cloud. In fact, since they occupy the same positions as in the PCA, it can be concluded that the first and second MDS coordinates correspond closely to the first and second PCA components. One notable difference between the two methods is that MDS distributes the individuals more evenly across the space, rather than concentrating them as in the PCA plot.

```{r}
ds_eu <- dist(scale(numeric_vars), method = "euclidean")

dimaonds.mds.eu <- cmdscale(ds_eu, eig = TRUE, k = 2)

dimaonds.mds.eu$GOF

plot(1:length(dimaonds.mds.eu$eig), dimaonds.mds.eu$eig, 
     type = "b",
     xlab = "Dimension", 
     ylab = "Eigenvalue",
     main = "Scree Plot")

x <- dimaonds.mds.eu$points[,1]
y <- dimaonds.mds.eu$points[,2]

plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
     main="Individual plot using the first two coordinates with Euclidian distance", type="n")

text(x, y, labels = row.names(diamonds), cex=.7)
```

On the other hand, we have also performed the MDS analysis using the Gower distance, computed with the \emph{daisy} function from the \emph{cluster} library, allowing us to include all categorical variables. 

The scree plot also suggests that using two dimensions captures most of the variance while keeping the solution interpretable. In the case of Euclidean distance, both GOF values are identical. However, when using the Gower distance, the two GOF components differ slightly. The first value $(0.26)$ measures the proportion of total variance explained by the two dimensions, considering both positive and negative eigenvalues, whereas the second value $(0.36)$ only accounts for the positive eigenvalues. This difference arises because the Gower distance combines mixed variable types.

Although the GOF measure is lower when including the categorical variables, it is important to note that the individual plot clearly separates the diamonds into four distinct groups, which should be analyzed further. Nonetheless, observations $365$ is still an outstanding observation.

```{r}
ds_gw <- daisy(diamonds, metric = "gower")

diamonds.mds.gw <- cmdscale(ds_gw, eig = TRUE, k = 2)

diamonds.mds.gw$GOF

plot(1:length(diamonds.mds.gw$eig), diamonds.mds.gw$eig, 
     type = "b",
     xlab = "Dimension", 
     ylab = "Eigenvalue",
     main = "Scree Plot")

x <- diamonds.mds.gw$points[,1]
y <- diamonds.mds.gw$points[,2]


plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
     main="Individual plot using the first two coordinates with Gower distance", 
     type="n")

draw.ellipse <- function(x0, y0, a, b, col, lwd=2, alpha=0.3, angle=0) {
  theta <- seq(0, 2*pi, length.out=200)
  x_ellipse <- a*cos(theta)
  y_ellipse <- b*sin(theta)
  
  angle_rad <- angle * pi/180
  x_rot <- x_ellipse * cos(angle_rad) - y_ellipse * sin(angle_rad)
  y_rot <- x_ellipse * sin(angle_rad) + y_ellipse * cos(angle_rad)
  
  x_ellipse <- x0 + x_rot
  y_ellipse <- y0 + y_rot
  
  polygon(x_ellipse, y_ellipse, 
          col=adjustcolor(col, alpha.f=alpha), 
          border=NA)
  lines(x_ellipse, y_ellipse, col=col, lwd=lwd)
}

draw.ellipse(x0=-0.15, y0=0.18, a=0.2, b=0.05, angle=-20, col="red", lwd=2.5)
draw.ellipse(x0=-0.2, y0=-0.05, a=0.23, b=0.15, col="blue", lwd=2.5)
draw.ellipse(x0=0.2, y0=0.12, a=0.12, b=0.08, col="green", lwd=2.5)
draw.ellipse(x0=0.20, y0=-0.12, a=0.18, b=0.12, col="orange", lwd=2.5)

text(x, y, labels = row.names(diamonds), cex=.7)

legend("bottomleft", 
       legend=c("Grupo 1", "Grupo 2", "Grupo 3", "Grupo 4"),
       col=c("red", "blue", "green", "orange"),
       lwd=2.5, 
       bty="n",
       cex=0.8)
```

## 2.3. Correspondence analysis

The CA analysis has been applied using the the original categorical variables (cut, color, clarity) by analyzing them pairwisely. We have applied the \emph{CA()} function from the \emph{FactoMineR} library.

The objective of this method is to identify associations between the different categorical variables in order to determine which levels tend to appear together in the observations. Due to the nature of the categorical variables in this dataset, it is possible to apply CA. Although we will later apply MCA to examine the common relationships among all categorical variables, we considered it useful to first perform a simpler pairwise analysis. This initial step allows us to use the results to facilitate the interpretation of the subsequent MCA.

### Cut x color

to do individual conclusions !!!!!

There's 4 dimensions because dim = min(5−1, 7−1).

The first two dimensions explain 58.70% + 38.02% = 96.72% of the association between cut and color. This means that the two-dimensional map provides a really good summary of the relationship between the two variables.

Dimension 1 separates two main groups of cut-color combinations. On the left, Premium cut appears close to G and I/J, suggesting that these colors occur slightly more often with Premium cut than expected from the overall distribution. On the right, Fair/Good cut appears near D, indicating a stronger than average association between these categories.

Dimension 2 distinguishes Ideal from Very Good cuts. Ideal lies in the upper part of the map, near D, F, and I/J, indicating slight deviations from the average pattern in this direction. Very Good is in the lower part, close to E and H.
These categories contribute more strongly to Dimension 2, while other groups that lie closer to the center follow the average distribution more closely.

```{r}
tab_cut_color <- table(diamonds$cut, diamonds$color)
cut_color <- CA(tab_cut_color)

# Eigenvalues
cut_color$eig
barplot(cut_color$eig[,2], main="Eigenvalues")

# Row profiles
cut_color$row$coord
cut_color$row$contrib
cut_color$row$cos2

# Column profiles
cut_color$col$coord
cut_color$col$contrib
cut_color$col$cos2
```

### Cut x clarity
There's 4 dimensions because dim = min(5−1, 8−1).

With 2 dimensions we obtain 58.09% + 25.95% = 84.04%, meaning that the 2-dimensional CA map captures most of the association between cut and clarity.

Dimension 1 represents the strongest difference from the average diamond profile. Categories on the right, such as the Ideal cut and very high clarity (IF/VVS1), show similar behaviour and tend to appear together more often than expected. On the left, Fair/Good cut tends to appear with low clarity diamonds more often than the average diamond in the dataset. This dimension separates diamonds that tend to be associated with higher clarity from those linked to lower clarity, based on how their frequencies differ from the overall average.

Dimension 2 shows a second way in which some categories differ from the average diamond. Categories at the top, like IF/VVS1 clarity, behave in a more unusual way compared with the rest of the data. Categories lower on the axis, such as the Very Good cut or VVS2 clarity, also differ from the average but in a different direction. This dimension helps separate categories that do not follow the main pattern shown on Dimension 1.

```{r}
tab_cut_clarity <- table(diamonds$cut, diamonds$clarity)
cut_clarity <- CA(tab_cut_clarity)

# Eigenvalues
cut_clarity$eig
barplot(cut_clarity$eig[,2], main="Eigenvalues")

# Row profiles
cut_clarity$row$coord
cut_clarity$row$contrib
cut_clarity$row$cos2

# Column profiles
cut_clarity$col$coord
cut_clarity$col$contrib
cut_clarity$col$cos2
```

### Color x clarity
There's 6 dimensions because dim = min(7−1, 8−1).

With 2 dimensions we explain 52.15% + 28.18% = 80.33% of the association between color and clarity, meaning that the 2-dimensional map captures most of the relationship between the two variables.

In this map, clarity groups such as SI1 and I1/SI2, as well as color groups D and I/J, appear close to the origin. This indicates that these categories follow the general pattern in the data and do not show particularly distinctive color-clarity combinations.

In contrast, categories positioned farther from the center have more pronounced deviations from the average profile. For example, IF/VVS1 and VS1 lie on the right-hand side near color G, suggesting that high-clarity stones appear with G more often than expected from the overall distribution. Similarly, VVS2 is located toward the upper left near color E, indicating that this clarity group is unusually associated with E relative to the mean pattern.

```{r}
tab_color_clarity <- table(diamonds$color, diamonds$clarity)
color_clarity <- CA(tab_color_clarity)

# Eigenvalues
color_clarity$eig
barplot(color_clarity$eig[,2], main="Eigenvalues")

# Row profiles
color_clarity$row$coord
color_clarity$row$contrib
color_clarity$row$cos2

# Column profiles
color_clarity$col$coord
color_clarity$col$contrib
color_clarity$col$cos2
```
## 2.4. Multiple correspondence analysis

POSAR BIBLIOGRAFIA

The MCA analysis has been applied using the the original categorical variables and the categorized numerical variables. We have applied the \emph{MCA()} function from the \emph{FactoMineR} library.

The objective of this method is to identify global patterns, associations, and similarities among the categories of different variables. Using this approach, we perform a more comprehensive analysis of the results obtained from CA. In addition, the numeric variables have been prepared to be included in the method in a categorized form, allowing us to observe patterns between both numeric and categorical variables. It is important to note that several strategies were tested to categorize the numeric variables, and the best results are those presented in this report. Moreover, the nature of each variable has been taken into account when defining the different categories.

Most of the numerical variables have been transformed into binary factors ("above average" / "below average") to reduce dimensionality and enhance interpretability. However, the table and depth variables have not been dichotomized using the median. Instead, both variables have been categorized based on gemological criteria, as their interpretation depends on proportion ranges that determine whether a diamond is well-cut.

The table variable is defined as the width of the top surface expressed as a percentage of the diamond’s diameter. In gemology, there is a clearly defined range considered ideal, typically around $53-58\%$, which produces better light performance:
    - Values within $53-58\%$: Ideal
    - Values outside this interval: NotIdeal
Thus, a two-level factor ("Ideal", "NotIdeal") is more coherent and informative.


The depth variable measures the total depth of the diamond relative to its diameter.
Unlike table, depth has two different ways of being non-ideal:
    - Low depth: diamond too shallow ("flat"), light leaks through the pavilion
    - High depth: diamond too deep, appears small and loses brilliance

Ideal depth lies approximately between 59% and 62.5%

Because both extremes (low and high) represent different types of proportion defects, collapsing the variable into only two categories ("ideal" / "non-ideal") would lose important information. Therefore, depth is categorized into three levels:
    - Low (< 59%)
    - Ideal (59-62.5%)
    - High (> 62.5%)

This three-group division preserves the geometric meaning of the variable and leads to more interpretable patterns in the MCA.

```{r}
dfmca <- diamonds
nums = names(numeric_vars)
vars_to_split <- setdiff(nums, c("table", "depth"))

for (var in vars_to_split){
  medianvar = median(dfmca[[var]])
  dfmca[[var]] <- ifelse(dfmca[[var]] >= medianvar, "o", "b") 
  dfmca[[var]] <- as.factor(dfmca[[var]])
}

# Table
dfmca$table <- factor(
  ifelse(dfmca$table >= 53 & dfmca$table <= 58, "Ideal", "NotIdeal")
)

# Depth
dfmca$depth <- factor(
  cut(dfmca$depth,
      breaks = c(-Inf, 59, 62.5, Inf),
      labels = c("Low", "Ideal", "High"),
      right = TRUE)
)

summary(dfmca)
```

The analysis will be done using Indicator Matrix. That way we will be able to study both the categories of the variables and the individuals.

Regarding the logPrice variable, the approach followed will be the same as in PCA: it will be included as a supplementary variable, to help further the interpretation without being included in the calculations.

```{r}
## Indicator Matrix
library(FactoMineR)
tab.disjonctif(dfmca)

## MCA Application on Data using Indicator Matrix
# logPrice used as a supplementary variable
res.mca <- MCA(dfmca,quali.sup=10)
summary(res.mca)
```

In order to choose which dimensions we will work with, one option is to apply the rule of choosing the dimensions with eigenvalues greater than the average inertia of the indicator matrix per dimension.

In this case, as there are 9 variables, J = 9, and the total number of categories is K=4+6+7+3+(5x2)=30, number of dimensions is K-J=21. Therefore, the average inertia is 1/21 (0.04761905), and the dimensions selected would theoretically be until Dimension 17.

Howerver, using the scree plot, we observe a clear elbow around Dimension 2.

After the second dimension, the eigenvalues decrease smoothly and almost linearly, with no additional major drops. This indicates that most of the structure is captured by the first few dimensions, while the remaining ones mainly represent smaller, less interpretable variations.

Therefore, although the theoretical criterion based on average inertia would suggest retaining many more dimensions, the scree plot indicates that the first 2 dimensions concentrate the meaningful information, and the interpretation will be limited to these.

```{r}
# K=4+6+7+3+(5x2)=30
# no of dimension: K-J=21
# average inertia per dimension: 1/(K-J)=1/21
summary(res.mca, ncp=4)
# Choose the dimensions with eigenvalues > 1/21 --> Until Dim.17

### Eigenvalues (scree plot)
res.mca$eig
barplot(res.mca$eig[,1],main="Eigenvalues", names.arg=1:nrow(res.mca$eig))
lines(res.mca$eig[,1], type = "o", col = "red", pch = 19, add=T)

# Choose the dimensions that explain the most variability --> Until Dim.2
```

As can be seen in the description of Dimensions, the \textbf{most significant variables in each dimension}, which are those represented with the highest value of $R^2$, are the following: 

- For Dimension 1, the variables are `x`, `y`, `z` and `logCarat` all with $R^2$ values above 0.90.
  It is also relevant to note that the supplementary variable `logPrice` is also very significant in this dimension, with a value of $R^2$ of 0.79.
- For Dimension 2, the variables that contribute the most are `cut`, `table`, and `depth`, with $R^2$ values of approximately 0.69, 0.42, and 0.38 respectively. 

Also, it is relevant to observe the link between variable and categories to see which are the *most significant categories in each dimension*, which are those represented with the highest absolute value of Estimate.

- Dimension 1: The categories most strongly associated with this axis are the "below-average" modalities of the most significant variables (`x`, `y`, `z` and `logCarat`) on the positive side, and their corresponding "over-average" modalities on the negative side. This symmetric pattern indicates that Dimension 1 primarily captures a size gradient, separating smaller stones from larger ones.

- Dimension 2: The modalities with the strongest positive association are table_NotIdeal, depth_Low, and cut_Fair/Good.
On the negative side, the most strongly associated categories are table_Ideal, depth_Ideal, and cut_Ideal.
This shows that Dimension 2 reflects differences in geometric proportions, contrasting diamonds with non-ideal table and depth values against those with ideal proportions.


```{r}
## Coordinates of categories
res.mca$var$coord

## Description of Dimensions
dimdesc(res.mca)
```

For the plots, we will interpret them in the 1st and 2nd dimension, since they are the most significative.

Firstly, we will interpret the *cloud of individuals*. The plot shows both the individuals and the categories represented by the 1st and 2nd dimension. In the plot, the following patterns can be observed:

The individuals form two clearly separated groups horizontally. This mainly happens because the variables that dominate Dimension 1 were converted into "over" and "below" categories. Since these binary modalities are positioned on opposite sides of the map, the individuals are naturally pulled toward one side or the other depending on whether they fall above or below the median. As a result, almost no points appear near the center, and the two clouds become very visible.

Vertically, however, the structure is much softer. The variables contributing to Dimension 2 (cut, table, depth, and partly clarity and color) are not binary, so they do not create two opposing groups. Instead, they spread individuals more gradually, generating a continuous cloud rather than distinct clusters along this axis.

As a whole, the plot shows a strong separation driven by the over/under transformations of the size-related variables, while the multi-level categorical variables introduce smoother variation without forming sharply defined groups.

```{r}
### Clouds ###
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none")
```

We now interpret the *cloud of individuals using the most significant categories for each dimension*, by selecting the corresponding variable in the habillage parameter. This allows us to visualise how the individuals are distributed across the map according to the categories that contribute the most to each axis.

-   For *dimension 1*, colouring individuals by `logCarat`. The map clearly shows this distinction: in the negative side of the Dim 1 axis, we can see the individuals with a value of logCarat over average, and in the positive side, those below average.

-   For *dimension 2*, grouping individuals by `cut`. The separation is less sharp than for logCarat but some general tendencies can still be observed. Fair/Good and Very Good appear more often in the upper part of the map, while Ideal is mostly concentrated in the lower area.

However, the distribution does not fully follow the expected pattern: Premium and Very Good are quite mixed instead of forming clearly separated groups, unlike Ideal, which shows a much more consistent positioning. This occurs because cut has several levels and interacts with other variables (such as table and depth), which softens the separation along Dimension 2 and produces a more gradual spread of individuals.

Additional view: table (Ideal vs Not Ideal)

Finally, colouring individuals by the table classification shows another soft structure.
Although the categories are not as strongly separated as logCarat, we can observe that table_NotIdeal tends to appear more in the upper area of the plot, while table_Ideal is more frequent in the lower half.
Again, this gradient contributes to the vertical spread of individuals but does not create clear clusters.

```{r}
## Cloud of Individuals
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "logCarat")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "cut")
plot.MCA(res.mca,choix="ind",label="none",invisible="var",habillage = "table")
```

The graph of the active categories is very relevant: the categories of values 'over average' are represented on the positive side of Dim 1, and the ones 'below average' are on the negative side. And dimension 2 shows 'ideal' values on the negative side, and non-ideal ones on the positive.

Considering the total information obtained from the cloud of categories, and also the cloud of variables, the conclusions we draw of the *information obtained by the dimensions* is the following:

-   Dimension $1$: Physical Magnitude (and Price Component) Dimension $1$ separates diamonds primarily according to their physical scale. Variables such as x, y, z, and logCarat contribute strongly and in the same direction, and logPrice aligns closely as a supplementary variable. This indicates that this axis summarises a general size-value gradient: diamonds with higher coordinates are physically larger and tend to be more expensive, while those on the negative side are smaller, lighter, and typically less valuable

-   Dimension $2$: Geometric Balance Dimension $2$ is mainly shaped by the categories of table and cut, and depth is also significant. The plot shows a consistent vertical contrast across these three characteristics: the “Ideal” categories appear on the lower side of the axis, while the non-ideal or extreme categories tend to cluster on the upper side.

For table, the “Ideal” modality is located at the bottom, whereas “Not Ideal” appears clearly higher. For cut, the Ideal cut is also positioned in the lower region, while the remaining levels (Fair/Good, Premium, Very Good) lie in the upper half. A similar pattern occurs with depth: both Low and High depth categories are placed on the positive side of Dimension 2, while Ideal depth appears on the negative side.

Taken together, these patterns show that Dimension 2 separates diamonds with ideal geometric proportions (Ideal table + Ideal cut + Ideal depth) from those with non-ideal or unbalanced shapes, which tend to be wider, flatter, or too tall relative to the ideal standard.


```{r}
## Cloud of Categories
plot(res.mca,invisible=c("ind"),title="Graph of the active categories")

## Cloud of variables
plot(res.mca,choix="var",title="Cloud of variables")
```

Overall, the MCA highlights which characteristics matter most for differentiating diamonds. Size clearly drives the main separation and is closely aligned with price, whereas geometric balance (Ideal vs. Not Ideal proportions) introduces additional distinctions. In other words, a high-quality cut can enhance a diamond’s value, but it does not compensate for being smaller, and well-balanced proportions tend to strengthen the value of diamonds that are already physically larger.

## 2.5. Clustering and profiling

In this section, a clustering analysis is performed with the objective of identifying meaningful groups within the dataset. The analysis is carried out under two different settings.

First, clustering is applied using only the numerical variables, combining hierarchical and non-hierarchical methods, as well as clustering based on the principal component scores. Second, clustering of numerical and categorical variables is conducted to obtain a more comprehensive segmentation of the data.

### 2.5.1. Hierarchical clustering

The hierarchical clustering analysis is performed on the numerical variables after standardising them with the \emph{scale()} function. An Euclidean distance matrix is then obtained using the \emph{dist()} function applied to the scaled dataset.

The clustering structure is produced with the \emph{hclust()} function, and two linkage criteria are considered in order to compare their performance. First, the **complete linkage method** is computed, which defines the distance between clusters based on the maximum distance between their observations. Second, the **Ward’s method** is used, which is based on minimising the within-cluster sum of squared errors (SSE).

The resulting dendrograms allow a first inspection of the potential number of clusters. Using the \emph{rect.hclust()} function, possible partitions into 4, 5 and 6 clusters are highlighted. As observed in the figures, the Ward method provides a clearer and more compact separation between groups than the complete linkage method, suggesting that it offers a more interpretable clustering solution. Based on the Ward dendrogram, a solution with $4$ clusters appears to be the most appropriate, as it captures the main structure of the data before a major increase in fusion height.

```{r}
## Scale numerical data
dfscaled <- scale(numeric_vars)
dfscaled <- as.data.frame(dfscaled) # convert the scaled data to a dataframe
d <- dist(dfscaled, method = "euclidean") # apply distance matrix on the scaled dataframe

## Complete linkage method
fit1 <- hclust(d, method="complete") 
plot(fit1, main="Dendrogram of complete Linkage") 
# 4 clusters/6 clusters

rect.hclust(fit1, k=4, border="blue") # 4 clusters
rect.hclust(fit1, k=5, border="cyan") # 5 clusters
rect.hclust(fit1, k=6, border="red")  # 6 clusters

legend("topright",
       legend = c("4 clusters", "5 clusters", "6 clusters"),
       col = c("blue", "cyan", "red"),
       lwd = 2,
       cex = 0.8,
       bg = "white")

## Ward method
fit2 <- hclust(d, method="ward.D2") 
plot(fit2, main="Dendrogram of Ward Method") 
# 4 clusters/5 clusters

rect.hclust(fit2, k=3, border="brown")  # 3 clusters
rect.hclust(fit2, k=4, border="green")  # 4 clusters
rect.hclust(fit2, k=5, border="purple") # 5 clusters
rect.hclust(fit2, k=6, border="orange") # 6 clusters

legend("topright", 
       legend = c("3 clusters", "4 clusters", "5 clusters", "6 clusters"),
       col = c("brown", "green", "purple", "orange"),
       lwd = 2,
       cex = 0.8,
       bg = "white")
```

### 2.5.2. Non-hierarchical clustering

A non-hierarchical clustering analysis is performed using several criteria to determine an appropriate number of clusters. Three different methods are considered: the **TWSS Elbow Graph**, the **Pseudo F Index**, and the **Silhouette Index**.

##### TWSS Elbow Graph

The Total Within-Cluster Sum of Squares (TWSS) has been computed for different values of k using the **k-means algorithm**, in order to identify an appropriate number of clusters through the **Elbow method**.

As shown in the figure, the TWSS decreases sharply between $k=2$ and $k=4$, and then continues to decrease more gradually. A noticeable bend appears around $k=4$.

The ratio between the between-cluster sum of squares and the total sum of squares is computed. For $k=4$, this ratio is equal to $0.694$, indicating a fair degree of separation between clusters.

```{r}
## Applying k-means algorithm for different number of clusters 
aux <- c()
for (i in 2:10){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- k$tot.withinss 
}
plot(2:10, aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")
points(4, aux[4 - 1], pch = 19, cex = 1.5, col = "red") # Optimum point: k=4

# k=3
k3 <- kmeans(dfscaled, centers = 3, nstart = 25) 

## Sum of Squares 
k3$withinss 
k3$totss 
k3$tot.withinss 
k3$betweenss + k3$tot.withinss
k3$betweenss/k3$totss # 0.6251488 separation between clusters

# k=4
k4 <- kmeans(dfscaled, centers = 4, nstart = 25)  

## Sum of Squares 
k4$withinss 
k4$totss 
k4$tot.withinss 
k4$betweenss + k4$tot.withinss 
k4$betweenss/k4$totss # 0.694 fair separation between clusters

# k=5
k5 <- kmeans(dfscaled, centers = 5, nstart = 25) 

## Sum of Squares 
k5$withinss 
k5$totss 
k5$tot.withinss 
k5$betweenss + k5$tot.withinss 
k5$betweenss/k5$totss # 0.735624 separation between clusters

# k=6
k6 <- kmeans(dfscaled, centers = 6, nstart = 25) 

## Sum of Squares
k6$withinss
k6$totss
k6$tot.withinss
k6$betweenss + k6$tot.withinss
k6$betweenss/k6$totss # 0.768 greater separation between clusters
```

##### Pseudo F Index

The **Pseudo-F Index** has been computed for different values of k in order to evaluate the separation between clusters obtained with k-means. This index is based on the ratio between the between-cluster sum of squares (BSS) and the within-cluster sum of squares (WSS).

In this case the index decreases steadily as k increases, with the highest value appearing at $k=2$. Nevertheless, selecting two clusters does not provide a meaningful segmentation of the dataset and it contradicts the structure suggested by the dendrogram and the TWSS Elbow method.

```{r}
# Pseudo F index = BSS*(n-k)/WSS*(k-1)
aux <- c()
for (i in 2:20){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- ((k$betweenss)*(nrow(dfscaled)-i))/((k$tot.withinss)*(i-1))
}
plot(2:20, aux, xlab="Number of Clusters", ylab="Pseudo-F", type="l", main="Pseudo F Index")
points(2, aux[2-1], pch = 19, cex = 1.5, col = "red")
# max Pseudo F: 2 clusters
```

##### Silhoutte Index

Finally, the **Silhouette Index** is computed. For each observation, this value compares the average distance to points within its own cluster with the average distance to points in the nearest alternative cluster.

The average silhouette widths obtained for $k=3$, $k=4$, $k=5$ and $k=6$ are $0.35$, $0.36$, $0.28$ and $0.29$, respectively. The four-cluster solution shows higher overall cohesion.

The silhouette index computed using the \emph{fastkmed()} function confirms the same trend: for a range of cluster solutions, the highest silhouette values are obtained for three or four clusters, whereas the performance decreases for higher values of k. The four-cluster solution achieves a good balance between separation and stability.

```{r}
## A) Silhouette function from cluster library
set.seed(12345)
library(cluster)

# k=3
si <- silhouette(k3$cluster, d)
plot(si) 

# k=4
si <- silhouette(k4$cluster, d)
plot(si) 

# k=5
si <- silhouette(k5$cluster, d)
plot(si)

# k=6
si <- silhouette(k6$cluster, d)
plot(si) 


## B) Silhouette function from kmed library
library(kmed)

# k=3
res <- fastkmed(d, 3)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=3 higher silhouette value

# k=4
res <- fastkmed(d, 4)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=3 higher silhouette value

# k=5
res <- fastkmed(d, 5)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot # k=4 higher silhouette value

# k=6
res <- fastkmed(d, 6) 
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot
# k=4 higher silhouette value
```

To sum up, the three criteria do not point to the same value of k, but they do suggest a consistent tendency. The Elbow method shows a clear bend around four clusters, the Pseudo-F index does not provide useful guidance, and the Silhouette index indicates that three or four clusters offer the best structure. Thus, \textbf{four clusters} appear to be the most balanced and interpretable solution, and this value will be used for the next steps of the analysis.

### 2.5.3. k-means clustering using the optimal number of clusters

\textbf{- Cluster 1: Large, high-value diamonds}
This cluster contains the largest diamonds, with a high average carat (≈ 1.65) and large dimensions (x, y ≈ 7.55). These stones also show the highest mean price (≈ 11,200), clearly representing high-value diamonds driven by size.

\textbf{- Cluster 2: Small, low-value diamonds}
Diamonds in this cluster are very small, with a low average carat (≈ 0.39) and compact dimensions (x, y ≈ 4.65). This is reflected in a much lower average price (≈ 1,020), corresponding to entry-level, low-value stones.

\textbf{- Cluster 3: Medium-sized, balanced diamonds}
This cluster represents diamonds of intermediate size, with an average carat of ≈ 0.76 and moderate dimensions (x, y ≈ 5.88). Prices are also intermediate (≈ 3,180), indicating balanced stones in terms of size and value.

\textbf{- Cluster 4: Upper-mid-range diamonds}
Diamonds in this cluster are larger than average (carat ≈ 0.95; x, y ≈ 6.25) and more expensive than Cluster 3 (≈ 4,630), but clearly smaller and less costly than Cluster 1. This cluster represents upper-mid-range diamonds.

```{r}
set.seed(12345) # NO TREURE LA SET SEED D'AQUI QUE SINO CANVIA!!
# Apply k-means with the selected number of clusters
k4 <- kmeans(dfscaled, centers = 4, nstart = 25)
dfClustKmeans <- diamondsOg
# Add cluster label to original unscaled data
dfClustKmeans$Cluster <- as.factor(k4$cluster)

# Descriptive statistics per cluster
cluster_summary <- dfClustKmeans[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(everything(), mean, na.rm = TRUE))
cluster_summary

table(dfClustKmeans$Cluster)
prop.table(table(dfClustKmeans$Cluster))

# Clusters based on factors
table(dfClustKmeans$Cluster, dfClustKmeans$cut)
table(dfClustKmeans$Cluster, dfClustKmeans$color)
table(dfClustKmeans$Cluster, dfClustKmeans$clarity)
```


### 2.5.4. PCA-based clustering

The Hierarchical Clustering on Principal Components (HCPC) method is applied to the scores obtained from the previously computed PCA. This approach combines dimensionality reduction and clustering by performing Ward’s hierarchical clustering on the principal component scores, which reduces the influence of noise and correlation between variables.

#### Automatic cut: 3 clusters
The \emph{HCPC()} function is used with an automatic selection of the number of clusters, which results in a partition into $3$ clusters.

From the factor map, the 3 clusters are projected onto the first two principal components, which together explain more than $80\%$ of the total variance. Cluster 1 is mainly located on the left side of the first dimension, which is associated with smaller physical measurements of the diamonds. Cluster 2 occupies an intermediate region, while Cluster 3 is positioned towards the positive side of the first component, corresponding to larger diamond sizes.

```{r}
# HCPC applied to the previously computed PCA
res.hcpc <- HCPC(diamonds.pca.sup, -1) # automatic cut: 3 clusters

# Dendrogram visualization
library(factoextra)
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - PCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc, choice = "bar", main = "Inertia gain on PCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on PCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc, choice = "3D.map", main = "3D Cluster Visualization")
```
```{r}
table(res.hcpc$data.clust$clust)
prop.table(table(res.hcpc$data.clust$clust))

# Cluster description: categorical variables
res.hcpc$desc.var$test.chi2
res.hcpc$desc.var$category

# Cluster description: numerical variables
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti
```

```{r}
dfClustPCA3 <- diamondsOg
# Add cluster label to original unscaled data
dfClustPCA3$Cluster <- as.factor(res.hcpc$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_pca <- dfClustPCA3[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_pca

table(dfClustPCA3$Cluster)
prop.table(table(dfClustPCA3$Cluster))

# Clusters based on factors
table(dfClustPCA3$Cluster, dfClustPCA3$cut)
table(dfClustPCA3$Cluster, dfClustPCA3$color)
table(dfClustPCA3$Cluster, dfClustPCA3$clarity)
```

#### 4 clusters
!!!(JO CREC QUE ES MILLOR K=4 MIRANT EL FACTOR MAP, PERO EL AUTOMATIC CUT DIU K=3, DECIDIR QUIN ES MILLOR PER AFEGIR EL PROFILING, DE MOMENT HE FET EL DE K=4)

Although the automatic HCPC cut suggests a 3-cluster partition, the factor map and the dendrogram reveal that a 4-cluster solution is more balanced and more interpretable. The 3-cluster solution produces one dominant group, whereas splitting the structure into 4 clusters yields groups of comparable size and a clearer separation in the PCA space.

The PCA-based hierarchical clustering with four clusters reveals a clear grouping structure primarily driven by diamond size and price. Highly significant differences are observed across all main numerical variables, particularly carat, price, and physical dimensions (x, y, z), while depth and table provide additional discrimination among medium-sized diamonds.

\textbf{- Cluster 1 (black): Small, low-value diamonds}
This cluster includes the smallest diamonds, with a low average carat (≈ 0.39), small dimensions (x, y ≈ 4.65), and the lowest mean price (≈ 1,020). These stones form a clearly distinct group of low-value diamonds.

\textbf{- Cluster 2 (red): Medium-low diamonds}
Diamonds in this cluster have intermediate size (carat ≈ 0.78; x, y ≈ 5.92) and moderate prices (≈ 3,300). Compared to the overall average, they present lower depth and higher table values, indicating flatter proportions.

\textbf{- Cluster 3 (green): Medium-high diamonds}
This cluster groups slightly larger diamonds (carat ≈ 0.94; x, y ≈ 6.23) with higher prices than Cluster 2 (≈ 4,630). These diamonds show higher depth and lower table values, reflecting a different geometric balance within the medium-size range.

\textbf{- Cluster 4 (blue): Large, high-value diamonds}
Diamonds in this cluster are the largest and most expensive, with the highest carat (≈ 1.65), the largest dimensions (x, y ≈ 7.56), and the highest mean price (≈ 11,100). This group is clearly separated from the rest.

```{r}
# HCPC applied to the previously computed PCA
res.hcpc <- HCPC(diamonds.pca.sup, nb.clust = 4, consol = TRUE) # 4 clusters

# Dendrogram visualization
library(factoextra)
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - PCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc, choice = "bar", main = "Inertia gain on PCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on PCA Factor Map (Dim 1 vs Dim 2)")
```

```{r}
table(res.hcpc$data.clust$clust)
prop.table(table(res.hcpc$data.clust$clust))

# Cluster description: categorical variables
res.hcpc$desc.var$test.chi2
res.hcpc$desc.var$category

# Cluster description: numerical variables
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti
```

```{r}
dfClustPCA4 <- diamondsOg
# Add cluster label to original unscaled data
dfClustPCA4$Cluster <- as.factor(res.hcpc$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_pca <- dfClustPCA4[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_pca

table(dfClustPCA4$Cluster)
prop.table(table(dfClustPCA4$Cluster))

# Clusters based on factors
table(dfClustPCA4$Cluster, dfClustPCA4$cut)
table(dfClustPCA4$Cluster, dfClustPCA4$color)
table(dfClustPCA4$Cluster, dfClustPCA4$clarity)
```

### 2.5.5. MCA-based clustering

#### Automatic cut: 6 clusters
Hierarchical clustering was also applied to the MCA individual scores using the HCPC approach. The automatic cut yields a six-cluster solution. However, inspection of the factor map shows that these clusters are highly overlapping and primarily separated along Dimension 1, which reflects the size gradient induced by the over/under transformations of the numerical variables. Dimension 2, related to geometric proportions and cut quality, introduces limited additional separation. As a result, the six-cluster solution does not provide a clear or interpretable partition of the diamonds.

```{r}
# HCPC applied to the previously computed MCA
res.hcpc.mca <- HCPC(res.mca, -1) # automatic cut: 6 clusters

# Dendrogram visualization
fviz_dend(res.hcpc.mca, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - MCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.mca, choice = "bar", main = "Inertia gain on MCA")

# Visualize clusters in the MCA factor map
fviz_cluster(res.hcpc.mca, repel = TRUE, show.clust.cent = TRUE, main = "Cluster Visualization on MCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.mca, choice = "3D.map", main = "3D Cluster Visualization")
```

#### 4 clusters
Although the automatic HCPC cut on the MCA scores suggests a six-cluster partition, inspection of the factor map and the dendrogram reveals substantial overlap between groups and weak separation. Forcing a four-cluster solution does not resolve this issue, as the clusters remain largely overlapping and lack clear interpretability. Consequently, MCA-based clustering is not retained as a suitable solution for the analysis.

```{r}
# HCPC applied to the previously computed MCA
res.hcpc.mca <- HCPC(res.mca, nb.clust = 4, consol = TRUE, graph = FALSE) # 4 clusters

# Visualize the dendrogram
fviz_dend(res.hcpc.mca, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - MCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.mca, choice = "bar", main = "Inertia gain on MCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc.mca, repel = TRUE, show.clust.cent = TRUE,main = "Cluster Visualization on MCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.mca, choice = "3D.map", main = "3D Cluster Visualization")
```

```{r}
table(res.hcpc.mca$data.clust$clust)
prop.table(table(res.hcpc.mca$data.clust$clust))

# Cluster description: categorical variables
res.hcpc.mca$desc.var$test.chi2
res.hcpc.mca$desc.var$category
```

```{r}
dfClustMCA <- diamondsOg
# Add cluster label to original unscaled data
dfClustMCA$Cluster <- as.factor(res.hcpc.mca$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_mca <- dfClustMCA[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_mca

table(dfClustMCA$Cluster)
prop.table(table(dfClustMCA$Cluster))

# Clusters based on factors
table(dfClustMCA$Cluster, dfClustMCA$cut)
table(dfClustMCA$Cluster, dfClustMCA$color)
table(dfClustMCA$Cluster, dfClustMCA$clarity)
```

### 2.5.6. FAMD-based clustering 
The clustering analysis that includes both numerical and categorical variables is performed through a Factor Analysis of Mixed Data (FAMD).

The interpretation of the continuous variables in the FAMD closely mirrors the results previously obtained with the PCA. Dimension 1, which explains 26.69% of the total variability, is dominated by the physical measurements of the diamonds (x, y, z, logCarat, logPrice), all of which show extremely high squared correlations with this axis. As in the PCA, this dimension represents a size-value gradient, separating small and inexpensive diamonds from those that are larger and more valuable.

The categorical variables align with this structure. Lower color grades (especially I/J) and lower clarity categories (such as I1/SI2) are located on the positive side of Dimension 1, indicating an association with larger and more expensive diamonds. In contrast, higher clarity grades (IF/VVS1, VVS2) and better color grades (E) appear on the negative side of this axis, corresponding to smaller but higher-quality diamonds.

Dimension 2 (8.31% of explained variance) mainly reflects differences in geometric proportions. Ideal cut diamonds are associated with positive values of this dimension, whereas Premium and Fair/Good cuts are positioned at negative values, indicating less favorable geometric balance.

```{r}
# Factor Analysis for Mixed Data
res <- FAMD(diamonds)
summary(res)
```
The hierarchical clustering is applied to the individual scores obtained from the FAMD. The algorithm has selected a three-cluster solution. The separation observed in the factor map is driven by Dimension 1:
-   **Cluster 1** (black) groups observations with negative coordinates on this dimension, corresponding to smaller diamonds and categories associated with higher quality grades (Ideal cut, better colour). 
-   **Cluster 2** (red) occupies an intermediate position and presents internal heterogeneity, both in terms of size and categorical composition, which makes the partition less balanced.
-   **Cluster 3** (green) lies on the positive side of Dimension 1 and gathers larger diamonds, consistently aligned with levels such as Premium cut and lower-quality colours (H, I/J). 

\textbf{- Cluster 1 (red): Small, high-quality diamonds}
This cluster contains the smallest diamonds (carat ≈ 0.40; x, y ≈ 4.69) and the lowest mean price (≈ 1,072.68). It is significantly associated with high clarity grades (IF/VVS1 and VVS2) and shows a higher presence of Ideal cut and better color grades (notably E).

\textbf{- Cluster 2 (green): Medium-sized diamonds with lower clarity}
Diamonds in this cluster have intermediate size (carat ≈ 0.93; x, y ≈ 6.23) and a moderate mean price (≈ 4,487.52). The main categorical signal is an over-representation of SI1 and I1/SI2 clarity, suggesting mid-range diamonds with relatively lower clarity compared to Cluster 1.

\textbf{- Cluster 3 (blue): Large diamonds with lower quality indicators}
This cluster groups the largest and most expensive diamonds (carat ≈ 1.64; x, y ≈ 7.54; price ≈ 10,942.12). It is strongly associated with poorer color grades (especially I/J) and Premium cut, together with lower clarity categories (notably I1/SI2), indicating that size dominates while quality tends to be lower.

```{r}
# Hierarchical Clustering on FAMD scores
res.hcpc.famd <- HCPC(res, -1, consol = TRUE) # automatic cut: 3 clusters

# Visualize the dendrogram
fviz_dend(res.hcpc.famd, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - FAMD-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.famd, choice = "bar", main = "Inertia gain on FAMD")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc.famd, repel = TRUE, show.clust.cent = TRUE,main = "Cluster Visualization on FAMD Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.famd, choice = "3D.map", main = "3D Cluster Visualization")
```

```{r}
table(res.hcpc.famd$data.clust$clust)
prop.table(table(res.hcpc.famd$data.clust$clust))

# Cluster description: categorical variables
res.hcpc.famd$desc.var$test.chi2
res.hcpc.famd$desc.var$category

# Cluster description: numerical variables
res.hcpc.famd$desc.var$quanti.var
res.hcpc.famd$desc.var$quanti
```

```{r}
dfClustFAMD <- diamondsOg
# Add cluster label to original unscaled data
dfClustFAMD$Cluster <- as.factor(res.hcpc.famd$data.clust$clust)

# Descriptive statistics per cluster
cluster_summary_famd <- dfClustFAMD[, -c(2:4)] %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

cluster_summary_famd

table(dfClustFAMD$Cluster)
prop.table(table(dfClustFAMD$Cluster))

# Clusters based on factors
table(dfClustFAMD$Cluster, dfClustFAMD$cut)
table(dfClustFAMD$Cluster, dfClustFAMD$color)
table(dfClustFAMD$Cluster, dfClustFAMD$clarity)
```

### Hotelling or MANOVA --> MANOVA (cal explicar per que)
After identifying 4 clusters through PCA-based hierarchical clustering, we decided that this was the best clustering method. To assess whether the clusters are really meaningful, Multivariate Analysis of Variance (MANOVA) has been conducted. 

Two complementary MANOVA strategies have been applied.
- *MANOVA on original numerical variables*: logCarat, x, y, z, depth, table, and logPrice, using cluster membership as the grouping factor.
- *MANOVA on PCA scores*, specifically, applying it to the first two principal component scores obtained from the PCA, which summarize the main latent dimensions of the data. The first component represents a size–value gradient, while the second captures geometric balance related to table and depth proportions. Using PCA scores as response variables avoids multicollinearity and directly tests whether clusters differ along the same dimensions used to construct them.

For the first analysis, before applying MANOVA, the normality assumption must be considered. Normality was assessed using both statistical tests and graphical inspection. The results can be summarized as follows:
- logCarat does not satisfy normality even after the logarithmic transformation, according to formal normality tests.
- Depth fails the normality tests, however, its histogram and Q-Q plot show an approximately symmetric, bell-shaped distribution, so it can be considered approximately normal from a graphical perspective.
- Table also fails the normality tests, although its distribution is fairly symmetric with only mild skewness.
- logPrice, after logarithmic transformation, becomes much more symmetric; despite still failing formal normality tests, its graphical shape is reasonably close to normal.
- x, y, and z do not strictly follow a normal distribution according to the tests, although they present unimodal distributions without extreme skewness or heavy tails.

Overall, while strict univariate normality is not met for all variables, the deviations are generally mild. Since logCarat is not normal even with graphical assessment, we won't use it in the MANOVA analysis.

But for this approach, we see that the homogeneity assumption is not fullfilled. SO CANNOT PROCEED, WE CHECK MANOVA FOR THE PCA SCORES TO SEE IF WE CAN DO IT?????????????????????

```{r}
# df using dfClustPCA4 (has price, not logPrice)
df_manova <- dfClustPCA4[, c("x", "y", "z", "depth", "table", "price", "Cluster")]
df_manova$logPrice <- log(df_manova$price)
df_manova <- df_manova[, c("x", "y", "z", "depth", "table", "logPrice", "Cluster")]

#check homogeneity
library(biotools)

# Homogeneity of covariance matrices obtained from multivariate normal data
boxM(
  df_manova[, c("x", "y", "z", "depth", "table", "logPrice")],
  df_manova$Cluster
) #pvalue < 0.05

boxplot(x ~ Cluster, data = df_manova, main = "x by Cluster")
boxplot(y ~ Cluster, data = df_manova, main = "y by Cluster")
boxplot(z ~ Cluster, data = df_manova, main = "z by Cluster")
boxplot(depth ~ Cluster, data = df_manova, main = "depth by Cluster")
boxplot(table ~ Cluster, data = df_manova, main = "table by Cluster")
boxplot(logPrice ~ Cluster, data = df_manova, main = "logPrice by Cluster")

```

For the second approach, we use PCA scores to perform MANOVA. For this, we also need to check the normality and homogeneity of the PCA scores:
None of them are normal and there's no homogeneity!!!!!!
```{r}
# Extract the first two PCA individual coordinates
df_manova_pca <- as.data.frame(diamonds.pca.sup$ind$coord[, 1:2])
colnames(df_manova_pca) <- c("PC1", "PC2")

# Add cluster membership
df_manova_pca$Cluster <- dfClustPCA4$Cluster

# normality
for (i in 1:2) {
  hist(df_manova_pca[, i], main = colnames(df_manova_pca)[i])
  print(colnames(df_manova_pca)[i])
  print(shapiro.test(df_manova_pca[, i]))
  print(ks.test(df_manova_pca[, i], "pnorm",
                mean = mean(df_manova_pca[, i]),
                sd = sd(df_manova_pca[, i])))
}
# transformation??????? idk if we can transform PCA scores


# homogeneity
library(biotools)
boxM(df_manova_pca[, c("PC1", "PC2")], df_manova_pca$Cluster)

boxplot(PC1 ~ Cluster, data = df_manova_pca, main = "PC1 by Cluster")
boxplot(PC2 ~ Cluster, data = df_manova_pca, main = "PC2 by Cluster")

```

```{r}
# One-way MANOVA
man_pca <- manova(cbind(PC1, PC2) ~ Cluster, data = df_manova_pca)
summary(man_pca)
summary.aov(man_pca)

# Post-hoc comparisons
TukeyHSD(aov(PC1 ~ Cluster, data = df_manova_pca))
TukeyHSD(aov(PC2 ~ Cluster, data = df_manova_pca))
```
## 2.6. Discriminant analysis
In DA, we will work with the same normal variables than in MANOVA.
As data is not normal and not independent, we use QDA.

```{r}
library(MASS)
df_da <- df_manova
qda.fit <- qda(Cluster ~ x + y + z + depth + table + logPrice, data = df_da)
qda.fit

qda.fit$prior
names(qda.fit)

#Predicting Groups
pred <- predict(qda.fit)
names(pred)

#Class Predictions 
qda.fit$class

#Posterior Probabilities 
qda.fit$posterior

#Contingency Table of Observed and Predicted Values
tab<-table(df_da$Cluster,pred$class); tab

#Correct Classification Rate (CCR)
classrate<-sum(diag(tab))/sum(tab)
classrate

# Total CCR (alternative way)
sum(diag(prop.table(tab)))

#CCR across groups (over rows)
diag(prop.table(tab, 1))

#Prediction Accuracy p1^2+p^2
pa<-qda.fit$prior[1]^2 + qda.fit$prior[2]^2;pa

#Comparison of Original and Predicted Groups
comp<-cbind(qda.fit$type,pred$class); comp


## Stepwise Classification ##

library(klaR)
?stepclass

qdastep <- stepclass(df_da[,1:6],df_da[,7],method="qda",direction="backward", criterion="CR") 
# numerical variables, grouping, method, direction, criterion
qdastep
summary(qdastep)
names(qdastep)

qdastep$process
qdastep$model
qdastep$result.pm
```
Stepclass function indicates that best model is with y, depth, table, we assess if it's better than our full model:
```{r}
qda.fit.step <- qda(Cluster ~ y + depth + table, data = df_da)
qda.fit.step


qda.fit.step$prior
names(qda.fit.step)

#Predicting Groups
pred_step <- predict(qda.fit.step)
names(pred_step)

#Class Predictions 
qda.fit.step$class

#Posterior Probabilities 
qda.fit.step$posterior

#Contingency Table of Observed and Predicted Values
tab2<-table(df_da$Cluster,pred_step$class); tab2

#Correct Classification Rate (CCR)
classrate<-sum(diag(tab2))/sum(tab2)
classrate

# Total CCR (alternative way)
sum(diag(prop.table(tab2)))

#CCR across groups (over rows)
diag(prop.table(tab2, 1))

#Prediction Accuracy p1^2+p^2
pa2<-qda.fit.step$prior[1]^2 + qda.fit.step$prior[2]^2;pa2
```

